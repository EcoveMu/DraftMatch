import os
import re
import json
import tempfile
import shutil
import numpy as np
import pandas as pd
import difflib
import docx
import streamlit as st
import fitz  # PyMuPDF
import pdfplumber
from io import BytesIO
import base64

# æª¢æŸ¥sentence-transformersæ˜¯å¦å¯ç”¨
SENTENCE_TRANSFORMERS_AVAILABLE = False
try:
    from sentence_transformers import SentenceTransformer
    SENTENCE_TRANSFORMERS_AVAILABLE = True
except ImportError:
    pass

# å°å…¥ä¸Šä¸‹æ–‡æ„ŸçŸ¥åŒ¹é…æ¨¡å¡Š
try:
    from context_aware_matching import context_aware_matching, semantic_structure_matching
    CONTEXT_MATCHING_AVAILABLE = True
except ImportError:
    CONTEXT_MATCHING_AVAILABLE = False

# è¨­ç½®é é¢é…ç½®
st.set_page_config(
    page_title="æœŸåˆŠæ¯”å°ç³»çµ±",
    page_icon="ğŸ“Š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# è‡ªå®šç¾©CSS
st.markdown("""
<style>
    .main-header {
        color: #1E88E5;
        font-size: 2.5rem;
        margin-bottom: 20px;
    }
    .sub-header {
        color: #0D47A1;
        font-size: 1.8rem;
        margin-top: 30px;
        margin-bottom: 15px;
    }
    .diff-char-removed {
        background-color: #FFCDD2;
        text-decoration: line-through;
    }
    .diff-char-added {
        background-color: #C8E6C9;
    }
    .diff-word-removed {
        background-color: #EF9A9A;
        text-decoration: line-through;
        padding: 2px;
        margin: 2px;
    }
    .diff-word-added {
        background-color: #A5D6A7;
        padding: 2px;
        margin: 2px;
    }
    .table-warning {
        background-color: #FFF9C4;
        padding: 10px;
        border-radius: 5px;
        margin-bottom: 10px;
    }
    .api-key-input {
        margin-top: 10px;
        margin-bottom: 10px;
    }
    .ai-model-section {
        background-color: #E3F2FD;
        padding: 10px;
        border-radius: 5px;
        margin-top: 10px;
        margin-bottom: 10px;
    }
</style>
""", unsafe_allow_html=True)

# é¡¯ç¤ºæ¨™é¡Œ
st.markdown('<h1 class="main-header">æœŸåˆŠæ¯”å°ç³»çµ±</h1>', unsafe_allow_html=True)
st.markdown('æœ¬ç³»çµ±ç”¨æ–¼æ¯”å°åŸå§‹Wordæ–‡ä»¶èˆ‡ç¾ç·¨å¾ŒPDFæ–‡ä»¶çš„å…§å®¹å·®ç•°ï¼Œå¹«åŠ©æ ¡å°äººå“¡å¿«é€Ÿæ‰¾å‡ºä¸ä¸€è‡´ä¹‹è™•ã€‚')

# æª¢æŸ¥Javaæ˜¯å¦å®‰è£
def is_java_installed():
    try:
        result = os.system("java -version > /dev/null 2>&1")
        return result == 0
    except:
        return False

# æª¢æŸ¥EasyOCRæ˜¯å¦å¯ç”¨
def is_easyocr_available():
    try:
        import easyocr
        return True
    except ImportError:
        return False

# æª¢æŸ¥tabula-pyæ˜¯å¦å¯ç”¨
def is_tabula_available():
    if not is_java_installed():
        return False
    try:
        import tabula
        return True
    except ImportError:
        return False

# æª¢æŸ¥sentence-transformersæ˜¯å¦å¯ç”¨
def is_sentence_transformers_available():
    return SENTENCE_TRANSFORMERS_AVAILABLE

# åŠ è¼‰èªç¾©æ¨¡å‹
@st.cache_resource
def load_semantic_model():
    if is_sentence_transformers_available():
        try:
            model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
            return model
        except Exception as e:
            st.error(f"åŠ è¼‰èªç¾©æ¨¡å‹å¤±æ•—: {e}")
            return None
    return None

# ç”Ÿæˆå¼AIæ¨¡å‹é¡
class GenerativeAI:
    def __init__(self, model_name, api_key=None):
        self.model_name = model_name
        self.api_key = api_key
        self.is_available = self._check_availability()
    
    def _check_availability(self):
        """æª¢æŸ¥æ¨¡å‹æ˜¯å¦å¯ç”¨"""
        if self.model_name in ["BERTå¤šèªè¨€æ¨¡å‹", "MPNetä¸­æ–‡æ¨¡å‹", "RoBERTaä¸­æ–‡æ¨¡å‹"]:
            # æª¢æŸ¥æœ¬åœ°æ¨¡å‹
            try:
                from transformers import AutoModel, AutoTokenizer
                return True
            except ImportError:
                st.warning(f"æœªå®‰è£transformersåº«ï¼Œ{self.model_name}ä¸å¯ç”¨")
                return False
        elif self.model_name in ["OpenAI API", "Anthropic API", "Gemini API", "Qwen API"]:
            # æª¢æŸ¥APIæ¨¡å‹
            if self.api_key is not None and len(self.api_key) > 0:
                return True
            else:
                st.warning(f"{self.model_name}éœ€è¦API Keyæ‰èƒ½ä½¿ç”¨")
                return False
        return False
    
    def match_paragraphs(self, source_paragraphs, target_paragraphs):
        """ä½¿ç”¨ç”Ÿæˆå¼AIåŒ¹é…æ®µè½"""
        if not self.is_available:
            st.warning(f"{self.model_name}ä¸å¯ç”¨ï¼Œå°‡ä½¿ç”¨åŸºæœ¬åŒ¹é…ç®—æ³•")
            return None
        
        try:
            if self.model_name == "BERTå¤šèªè¨€æ¨¡å‹":
                return self._match_with_bert(source_paragraphs, target_paragraphs)
            elif self.model_name == "MPNetä¸­æ–‡æ¨¡å‹":
                return self._match_with_mpnet(source_paragraphs, target_paragraphs)
            elif self.model_name == "RoBERTaä¸­æ–‡æ¨¡å‹":
                return self._match_with_roberta(source_paragraphs, target_paragraphs)
            elif self.model_name == "OpenAI API":
                return self._match_with_openai(source_paragraphs, target_paragraphs)
            elif self.model_name == "Anthropic API":
                return self._match_with_anthropic(source_paragraphs, target_paragraphs)
            elif self.model_name == "Gemini API":
                return self._match_with_gemini(source_paragraphs, target_paragraphs)
            elif self.model_name == "Qwen API":
                return self._match_with_qwen(source_paragraphs, target_paragraphs)
        except Exception as e:
            st.error(f"{self.model_name}åŒ¹é…å¤±æ•—: {str(e)}")
            st.info("å°‡ä½¿ç”¨åŸºæœ¬åŒ¹é…ç®—æ³•ä½œç‚ºæ›¿ä»£")
            return None
        
        return None
    
    def _match_with_bert(self, source_paragraphs, target_paragraphs):
        """ä½¿ç”¨BERTå¤šèªè¨€æ¨¡å‹åŒ¹é…æ®µè½"""
        try:
            from transformers import BertModel, BertTokenizer
            import torch
            
            # åŠ è¼‰æ¨¡å‹
            tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')
            model = BertModel.from_pretrained('bert-base-multilingual-cased')
            
            # è¨ˆç®—åµŒå…¥
            source_embeddings = []
            for para in source_paragraphs:
                inputs = tokenizer(para['content'], return_tensors="pt", truncation=True, max_length=512)
                with torch.no_grad():
                    outputs = model(**inputs)
                embeddings = outputs.last_hidden_state.mean(dim=1)
                source_embeddings.append(embeddings.squeeze().numpy())
            
            target_embeddings = []
            for para in target_paragraphs:
                inputs = tokenizer(para['content'], return_tensors="pt", truncation=True, max_length=512)
                with torch.no_grad():
                    outputs = model(**inputs)
                embeddings = outputs.last_hidden_state.mean(dim=1)
                target_embeddings.append(embeddings.squeeze().numpy())
            
            # è¨ˆç®—ç›¸ä¼¼åº¦çŸ©é™£
            similarity_matrix = np.zeros((len(source_paragraphs), len(target_paragraphs)))
            for i, source_emb in enumerate(source_embeddings):
                for j, target_emb in enumerate(target_embeddings):
                    similarity = np.dot(source_emb, target_emb) / (np.linalg.norm(source_emb) * np.linalg.norm(target_emb))
                    similarity_matrix[i, j] = similarity
            
            # ä½¿ç”¨åŒˆç‰™åˆ©ç®—æ³•æ‰¾åˆ°æœ€ä½³åŒ¹é…
            try:
                from scipy.optimize import linear_sum_assignment
                row_ind, col_ind = linear_sum_assignment(-similarity_matrix)
                
                # æ§‹å»ºåŒ¹é…çµæœ
                matches = []
                for i, j in zip(row_ind, col_ind):
                    matches.append({
                        "doc1_index": i,
                        "doc2_index": j,
                        "similarity": similarity_matrix[i, j]
                    })
                
                return matches
            except ImportError:
                # ä½¿ç”¨è²ªå©ªç®—æ³•ä½œç‚ºæ›¿ä»£
                matches = []
                used_target_indices = set()
                
                # ç‚ºæ¯å€‹sourceæ®µè½æ‰¾åˆ°æœ€ä½³åŒ¹é…çš„targetæ®µè½
                for i in range(len(source_paragraphs)):
                    best_similarity = -1
                    best_j = -1
                    
                    # æ‰¾åˆ°æœ€ç›¸ä¼¼çš„æœªä½¿ç”¨çš„targetæ®µè½
                    for j in range(len(target_paragraphs)):
                        if j not in used_target_indices and similarity_matrix[i, j] > best_similarity:
                            best_similarity = similarity_matrix[i, j]
                            best_j = j
                    
                    # å¦‚æœæ‰¾åˆ°åŒ¹é…ï¼Œå‰‡æ·»åŠ åˆ°çµæœä¸­
                    if best_j != -1:
                        matches.append({
                            "doc1_index": i,
                            "doc2_index": best_j,
                            "similarity": best_similarity
                        })
                        used_target_indices.add(best_j)
                    else:
                        # å¦‚æœæ²’æœ‰æœªä½¿ç”¨çš„targetæ®µè½ï¼Œå‰‡é¸æ“‡æœ€ç›¸ä¼¼çš„æ®µè½
                        best_j = np.argmax(similarity_matrix[i])
                        matches.append({
                            "doc1_index": i,
                            "doc2_index": best_j,
                            "similarity": similarity_matrix[i, best_j]
                        })
                
                return matches
            
        except Exception as e:
            st.error(f"BERTåŒ¹é…å¤±æ•—: {e}")
            return None
    
    # å…¶ä»–æ¨¡å‹çš„åŒ¹é…æ–¹æ³•å¯¦ç¾...
    
    def _match_with_qwen(self, source_paragraphs, target_paragraphs):
        """ä½¿ç”¨Qwen APIåŒ¹é…æ®µè½"""
        try:
            import requests
            
            # æº–å‚™æ•¸æ“š
            source_texts = [p['content'] for p in source_paragraphs]
            target_texts = [p['content'] for p in target_paragraphs]
            
            # æ§‹å»ºæç¤º
            prompt = f"""
            æˆ‘æœ‰å…©çµ„æ–‡æœ¬æ®µè½ï¼Œéœ€è¦æ‰¾å‡ºå®ƒå€‘ä¹‹é–“çš„æœ€ä½³åŒ¹é…é—œä¿‚ã€‚
            
            ç¬¬ä¸€çµ„æ®µè½ï¼ˆåŸå§‹æ–‡æœ¬ï¼‰ï¼š
            {json.dumps(source_texts, ensure_ascii=False)}
            
            ç¬¬äºŒçµ„æ®µè½ï¼ˆç›®æ¨™æ–‡æœ¬ï¼‰ï¼š
            {json.dumps(target_texts, ensure_ascii=False)}
            
            è«‹åˆ†æé€™äº›æ®µè½ï¼Œæ‰¾å‡ºæ¯å€‹åŸå§‹æ®µè½åœ¨ç›®æ¨™æ®µè½ä¸­çš„æœ€ä½³åŒ¹é…ã€‚è¿”å›ä¸€å€‹JSONæ•¸çµ„ï¼Œæ¯å€‹å…ƒç´ åŒ…å«ï¼š
            1. "source_index": åŸå§‹æ®µè½çš„ç´¢å¼•ï¼ˆå¾0é–‹å§‹ï¼‰
            2. "target_index": åŒ¹é…çš„ç›®æ¨™æ®µè½çš„ç´¢å¼•ï¼ˆå¾0é–‹å§‹ï¼‰
            3. "similarity": ç›¸ä¼¼åº¦è©•åˆ†ï¼ˆ0åˆ°1ä¹‹é–“ï¼‰
            
            åªè¿”å›JSONæ•¸çµ„ï¼Œä¸è¦æœ‰å…¶ä»–è§£é‡‹ã€‚
            """
            
            # æº–å‚™APIè«‹æ±‚
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            }
            
            payload = {
                "model": "qwen-max",
                "input": {
                    "messages": [
                        {
                            "role": "system",
                            "content": "ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„æ–‡æœ¬åŒ¹é…åŠ©æ‰‹ï¼Œæ“…é•·åˆ†ææ–‡æœ¬ç›¸ä¼¼åº¦å’Œæ‰¾å‡ºæœ€ä½³åŒ¹é…ã€‚"
                        },
                        {
                            "role": "user",
                            "content": prompt
                        }
                    ]
                },
                "parameters": {
                    "result_format": "message"
                }
            }
            
            # ç™¼é€APIè«‹æ±‚
            response = requests.post(
                "https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation",
                headers=headers,
                json=payload
            )
            
            if response.status_code == 200:
                result = response.json()
                result_text = result["output"]["choices"][0]["message"]["content"]
                
                # æå–JSON
                import re
                json_match = re.search(r'\[.*\]', result_text, re.DOTALL)
                if json_match:
                    json_str = json_match.group(0)
                    matches_data = json.loads(json_str)
                    
                    # è½‰æ›ç‚ºæ‰€éœ€æ ¼å¼
                    matches = []
                    for item in matches_data:
                        matches.append({
                            "doc1_index": item["source_index"],
                            "doc2_index": item["target_index"],
                            "similarity": item["similarity"]
                        })
                    
                    return matches
            
            return None
        
        except Exception as e:
            st.error(f"Qwen APIåŒ¹é…å¤±æ•—: {e}")
            return None

# é¡¯ç¤ºç³»çµ±ç‹€æ…‹
with st.expander("ç³»çµ±ç‹€æ…‹", expanded=True):
    col1, col2, col3, col4, col5 = st.columns(5)
    
    with col1:
        if is_java_installed():
            st.success("Javaå·²å®‰è£")
        else:
            st.error("Javaæœªå®‰è£ï¼Œè¡¨æ ¼æå–åŠŸèƒ½å¯èƒ½å—é™")
    
    with col2:
        if is_easyocr_available():
            st.success("OpenOCR (EasyOCR) å·²å®‰è£")
        else:
            st.warning("OpenOCRæœªå®‰è£ï¼Œå°‡ä½¿ç”¨Tesseractä½œç‚ºæ›¿ä»£")
    
    with col3:
        if is_tabula_available():
            st.success("è¡¨æ ¼æå–å·¥å…·å·²å®‰è£")
        else:
            st.error("è¡¨æ ¼æå–å·¥å…·æœªå®‰è£æˆ–ç„¡æ³•ä½¿ç”¨")
    
    with col4:
        if is_sentence_transformers_available():
            st.success("èªç¾©æ¨¡å‹å·²å®‰è£")
        else:
            st.warning("èªç¾©æ¨¡å‹æœªå®‰è£ï¼Œèªç¾©æ¯”å°åŠŸèƒ½å°‡ä½¿ç”¨ç°¡åŒ–ç‰ˆ")
    
    with col5:
        if CONTEXT_MATCHING_AVAILABLE:
            st.success("ä¸Šä¸‹æ–‡æ„ŸçŸ¥åŒ¹é…å¯ç”¨")
        else:
            st.warning("ä¸Šä¸‹æ–‡æ„ŸçŸ¥åŒ¹é…ä¸å¯ç”¨ï¼Œå°‡ä½¿ç”¨åŸºæœ¬åŒ¹é…")
    
    if not is_java_installed():
        st.info("å®‰è£Java: è«‹åŸ·è¡Œ 'sudo apt-get install -y default-jre' æˆ–å®‰è£é©åˆæ‚¨ç³»çµ±çš„Javaé‹è¡Œç’°å¢ƒ")
    
    if not is_easyocr_available():
        st.info("å®‰è£OpenOCR: è«‹åŸ·è¡Œ 'pip install easyocr'")
    
    if not is_sentence_transformers_available():
        st.info("å®‰è£èªç¾©æ¨¡å‹: è«‹åŸ·è¡Œ 'pip install sentence-transformers'")
        st.warning("æ³¨æ„ï¼šèªç¾©æ¨¡å‹æœªå®‰è£ï¼Œç³»çµ±å°‡ä½¿ç”¨ç°¡åŒ–ç‰ˆèªç¾©æ¯”å°ï¼Œç²¾åº¦å¯èƒ½è¼ƒä½")

# å´é‚Šæ¬„ - æ¯”å°è¨­ç½®
with st.sidebar:
    st.header("æ¯”å°è¨­ç½®")
    
    # æ¯”å°æ¨¡å¼
    comparison_mode = st.radio(
        "æ¯”å°æ¨¡å¼",
        ["ç²¾ç¢ºæ¯”å°", "èªæ„æ¯”å°", "æ··åˆæ¯”å°", "ç”Ÿæˆå¼AIæ¯”å°", "ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ¯”å°"],
        index=4,
        help="é¸æ“‡æ¯”å°æ¨¡å¼ï¼Œä¸Šä¸‹æ–‡æ„ŸçŸ¥æ¯”å°æä¾›æœ€é«˜ç²¾åº¦"
    )
    
    # å¦‚æœé¸æ“‡ç”Ÿæˆå¼AIæ¯”å°ï¼Œé¡¯ç¤ºæ¨¡å‹é¸æ“‡
    if comparison_mode == "ç”Ÿæˆå¼AIæ¯”å°":
        st.markdown('<div class="ai-model-section">', unsafe_allow_html=True)
        st.subheader("ç”Ÿæˆå¼AIè¨­ç½®")
        
        ai_model = st.selectbox(
            "é¸æ“‡AIæ¨¡å‹",
            ["BERTå¤šèªè¨€æ¨¡å‹", "MPNetä¸­æ–‡æ¨¡å‹", "RoBERTaä¸­æ–‡æ¨¡å‹", 
             "OpenAI API", "Anthropic API", "Gemini API", "Qwen API"],
            index=0,
            help="é¸æ“‡ç”¨æ–¼æ®µè½åŒ¹é…çš„ç”Ÿæˆå¼AIæ¨¡å‹"
        )
        
        # å¦‚æœé¸æ“‡APIæ¨¡å‹ï¼Œé¡¯ç¤ºAPI Keyè¼¸å…¥æ¡†
        if ai_model in ["OpenAI API", "Anthropic API", "Gemini API", "Qwen API"]:
            ai_api_key = st.text_input(f"{ai_model} Key", type="password", help=f"è¼¸å…¥æ‚¨çš„{ai_model} Key")
            
            if ai_model == "OpenAI API":
                st.info("OpenAI APIä½¿ç”¨GPT-4æ¨¡å‹ï¼Œæä¾›é«˜ç²¾åº¦æ®µè½åŒ¹é…")
            elif ai_model == "Anthropic API":
                st.info("Anthropic APIä½¿ç”¨Claudeæ¨¡å‹ï¼Œæä¾›é«˜ç²¾åº¦æ®µè½åŒ¹é…")
            elif ai_model == "Gemini API":
                st.info("Gemini APIä½¿ç”¨Googleçš„Gemini Proæ¨¡å‹ï¼Œæä¾›é«˜ç²¾åº¦æ®µè½åŒ¹é…")
            elif ai_model == "Qwen API":
                st.info("Qwen APIä½¿ç”¨é˜¿é‡Œå·´å·´çš„Qwen Maxæ¨¡å‹ï¼Œæä¾›é«˜ç²¾åº¦æ®µè½åŒ¹é…")
        else:
            ai_api_key = None
            st.info("æœ¬åœ°æ¨¡å‹ç„¡éœ€API Keyï¼Œä½†ç²¾åº¦å¯èƒ½ä½æ–¼APIæ¨¡å‹")
        
        st.markdown('</div>', unsafe_allow_html=True)
    else:
        ai_model = None
        ai_api_key = None
    
    # ç›¸ä¼¼åº¦é–¾å€¼
    similarity_threshold = st.slider(
        "ç›¸ä¼¼åº¦é–¾å€¼",
        min_value=0.0,
        max_value=1.0,
        value=0.8,
        step=0.05,
        help="ç›¸ä¼¼åº¦ä½æ–¼æ­¤é–¾å€¼çš„æ®µè½å°‡è¢«æ¨™è¨˜ç‚ºä¸ä¸€è‡´"
    )
    
    # å¿½ç•¥é¸é …
    st.subheader("å¿½ç•¥é¸é …")
    ignore_space = st.checkbox("å¿½ç•¥ç©ºæ ¼", value=True)
    ignore_punctuation = st.checkbox("å¿½ç•¥æ¨™é»ç¬¦è™Ÿ", value=True)
    ignore_case = st.checkbox("å¿½ç•¥å¤§å°å¯«", value=True)
    ignore_newline = st.checkbox("å¿½ç•¥æ›è¡Œ", value=True)
    
    # OCRè¨­ç½®
    st.subheader("OCRè¨­ç½®")
    ocr_engine = st.radio(
        "OCRå¼•æ“",
        ["è‡ªå‹•é¸æ“‡", "Tesseract", "OpenOCR (EasyOCR)", "Qwen API"],
        index=0,
        help="é¸æ“‡ç”¨æ–¼æå–PDFæ–‡æœ¬çš„OCRå¼•æ“"
    )
    
    # å¦‚æœé¸æ“‡Qwen APIï¼Œé¡¯ç¤ºAPI Keyè¼¸å…¥æ¡†
    if ocr_engine == "Qwen API":
        qwen_api_key = st.text_input("Qwen API Key", type="password", help="è¼¸å…¥æ‚¨çš„Qwen API Key")
        st.info("Qwen APIæä¾›é«˜ç²¾åº¦OCRå’Œè¡¨æ ¼è­˜åˆ¥ï¼Œç‰¹åˆ¥é©åˆè¤‡é›œæ’ç‰ˆçš„PDF")
    else:
        use_ocr = st.checkbox("ä½¿ç”¨OCRæå–", value=True, help="å•Ÿç”¨OCRå¯ä»¥æé«˜æ–‡æœ¬æå–è³ªé‡ï¼Œä½†æœƒå¢åŠ è™•ç†æ™‚é–“")
    
    # è¡¨æ ¼è™•ç†è¨­ç½®
    st.subheader("è¡¨æ ¼è™•ç†")
    table_handling = st.radio(
        "è¡¨æ ¼è™•ç†æ–¹å¼",
        ["è¼”åŠ©äºº
(Content truncated due to size limit. Use line ranges to read in chunks)