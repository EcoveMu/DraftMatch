import streamlit as st
import os
import tempfile
import pandas as pd
import numpy as np
import time
import json
import io
import re
import difflib
import base64
import shutil
from PIL import Image, ImageDraw, ImageFont
import fitz  # PyMuPDF

# å°å…¥æœ¬åœ°æ¨¡çµ„
from enhanced_extraction import extract_and_process_documents
from qwen_api import QwenOCR
from custom_ai import CustomAI

# æª¢æŸ¥sentence-transformersæ˜¯å¦å¯ç”¨
SENTENCE_TRANSFORMERS_AVAILABLE = False
try:
    from sentence_transformers import SentenceTransformer
    SENTENCE_TRANSFORMERS_AVAILABLE = True
except ImportError:
    pass

# æª¢æŸ¥æ˜¯å¦å¯ä»¥å°å…¥ç”Ÿæˆå¼AIæ¨¡çµ„
try:
    from qwen_ai import QwenAI
    QWEN_AI_AVAILABLE = True
except ImportError:
    QWEN_AI_AVAILABLE = False

# è¨­ç½®é é¢é…ç½®
st.set_page_config(
    page_title="æœŸåˆŠæ¯”å°ç³»çµ±",
    page_icon="ğŸ“Š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# è‡ªå®šç¾©CSS
def load_css():
    css = """
    <style>
        .diff-removed {
            background-color: #ffcccc;
            text-decoration: line-through;
            color: black;
        }
        .diff-added {
            background-color: #ccffcc;
            color: black;
        }
        .diff-char-removed {
            background-color: #ffcccc;
            text-decoration: line-through;
            display: inline;
            color: black;
        }
        .diff-char-added {
            background-color: #ccffcc;
            display: inline;
            color: black;
        }
        .comparison-result {
            border: 1px solid #ddd;
            padding: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
            color: black;
        }
        .similar {
            border-left: 5px solid green;
        }
        .different {
            border-left: 5px solid red;
        }
        .stTabs [data-baseweb="tab-list"] {
            gap: 2px;
        }
        .stTabs [data-baseweb="tab"] {
            height: 50px;
            white-space: pre-wrap;
            background-color: #f0f2f6;
            border-radius: 4px 4px 0 0;
            gap: 1px;
            padding-top: 10px;
            padding-bottom: 10px;
            color: black;
        }
        .stTabs [aria-selected="true"] {
            background-color: #e6f0ff;
            border-bottom: 2px solid #4c83ff;
        }
        .highlight {
            background-color: yellow;
            color: black;
        }
        .table-container {
            overflow-x: auto;
        }
        .table-container table {
            width: 100%;
            border-collapse: collapse;
        }
        .table-container th, .table-container td {
            border: 1px solid #ddd;
            padding: 8px;
            text-align: left;
            color: black;
        }
        .table-container th {
            background-color: #f2f2f2;
        }
        .table-container tr:nth-child(even) {
            background-color: #f9f9f9;
        }
        .diff-warning {
            background-color: #fff3cd;
            color: black;
        }
        .diff-error {
            background-color: #f8d7da;
            color: black;
        }
        .summary-card {
            background-color: #f9f9f9;
            border-radius: 5px;
            padding: 15px;
            margin-bottom: 20px;
            box-shadow: 0 2px 5px rgba(0,0,0,0.1);
            color: black;
        }
        .summary-card h3 {
            margin-top: 0;
            color: #333;
        }
        .metric-container {
            display: flex;
            justify-content: space-between;
            flex-wrap: wrap;
        }
        .metric-box {
            background-color: white;
            border-radius: 5px;
            padding: 10px;
            margin: 5px;
            box-shadow: 0 1px 3px rgba(0,0,0,0.1);
            text-align: center;
            flex: 1;
            min-width: 120px;
            color: black;
        }
        .metric-value {
            font-size: 24px;
            font-weight: bold;
            margin: 5px 0;
            color: black;
        }
        .metric-label {
            font-size: 14px;
            color: #333;
        }
        .ai-analysis {
            background-color: #f0f7ff;
            border-radius: 5px;
            padding: 15px;
            margin: 20px 0;
            border-left: 5px solid #4c83ff;
            color: black;
        }
        .pdf-preview {
            border: 1px solid #ddd;
            border-radius: 5px;
            padding: 10px;
            margin: 10px 0;
            background-color: white;
        }
        .pdf-preview img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 0 auto;
        }
        .pdf-preview-title {
            font-weight: bold;
            margin-bottom: 10px;
            color: #333;
        }
        .stMarkdown p, .stMarkdown h1, .stMarkdown h2, .stMarkdown h3, .stMarkdown h4, .stMarkdown h5, .stMarkdown h6, .stMarkdown li {
            color: black !important;
        }
        .stText {
            color: black !important;
        }
        .stTextInput > div > div > input {
            color: black !important;
        }
        .stSelectbox > div > div > div {
            color: black !important;
        }
        .stSlider > div > div > div {
            color: black !important;
        }
        .stCheckbox > div > div > label {
            color: black !important;
        }
        .stExpander > div > div > div > div > p {
            color: black !important;
        }
        .stExpander > div > div > div > div > div > p {
            color: black !important;
        }
        .table-tab {
            margin-top: 20px;
        }
        .ai-model-section {
            background-color: #E3F2FD;
            padding: 10px;
            border-radius: 5px;
            margin-top: 10px;
            margin-bottom: 10px;
        }
        .success-message {
            color: #2E7D32;
            background-color: #E8F5E9;
            padding: 10px;
            border-radius: 5px;
            margin-top: 10px;
            margin-bottom: 10px;
        }
        .error-message {
            color: #C62828;
            background-color: #FFEBEE;
            padding: 10px;
            border-radius: 5px;
            margin-top: 10px;
            margin-bottom: 10px;
        }
        .diff-section {
            margin-top: 10px;
            margin-bottom: 10px;
            padding: 10px;
            border: 1px solid #E0E0E0;
            border-radius: 5px;
        }
        .diff-navigation {
            margin-top: 10px;
            margin-bottom: 10px;
            text-align: center;
        }
        .diff-count {
            font-weight: bold;
            margin-left: 10px;
            margin-right: 10px;
        }
    </style>
    """
    st.markdown(css, unsafe_allow_html=True)

# æª¢æŸ¥Javaæ˜¯å¦å®‰è£
def is_java_installed():
    try:
        result = os.system("java -version > /dev/null 2>&1")
        return result == 0
    except:
        return False

# æª¢æŸ¥EasyOCRæ˜¯å¦å¯ç”¨
def is_easyocr_available():
    try:
        import easyocr
        return True
    except ImportError:
        return False

# æª¢æŸ¥tabula-pyæ˜¯å¦å¯ç”¨
def is_tabula_available():
    if not is_java_installed():
        return False
    try:
        import tabula
        return True
    except ImportError:
        return False

# æª¢æŸ¥sentence-transformersæ˜¯å¦å¯ç”¨
def is_sentence_transformers_available():
    return SENTENCE_TRANSFORMERS_AVAILABLE

# åŠ è¼‰èªç¾©æ¨¡å‹
@st.cache_resource
def load_semantic_model():
    if is_sentence_transformers_available():
        try:
            model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
            return model
        except Exception as e:
            st.error(f"åŠ è¼‰èªç¾©æ¨¡å‹å¤±æ•—: {e}")
            return None
    return None

# ç”Ÿæˆå¼AIæ¨¡å‹é¡
class GenerativeAI:
    def __init__(self, model_name, api_key=None, api_url=None):
        self.model_name = model_name
        self.api_key = api_key
        self.api_url = api_url
        self.is_available = self._check_availability()
        self.custom_ai = None
        
        # å¦‚æœæ˜¯è‡ªå®šç¾©AIï¼Œåˆå§‹åŒ–CustomAIå¯¦ä¾‹
        if self.model_name == "è‡ªå®šç¾©AI":
            self.custom_ai = CustomAI(api_key=api_key, api_url=api_url)
    
    def _check_availability(self):
        """æª¢æŸ¥æ¨¡å‹æ˜¯å¦å¯ç”¨"""
        if self.model_name == "è‡ªå®šç¾©AI":
            # è‡ªå®šç¾©AIå§‹çµ‚å¯ç”¨ï¼Œå› ç‚ºå®ƒæœ‰å…è²»APIé¸é …
            return True
        elif self.model_name in ["BERTå¤šèªè¨€æ¨¡å‹", "MPNetä¸­æ–‡æ¨¡å‹", "RoBERTaä¸­æ–‡æ¨¡å‹"]:
            # æª¢æŸ¥æœ¬åœ°æ¨¡å‹
            try:
                from transformers import AutoModel, AutoTokenizer
                return True
            except ImportError:
                st.warning(f"æœªå®‰è£transformersåº«ï¼Œ{self.model_name}ä¸å¯ç”¨")
                return False
        elif self.model_name in ["OpenAI API", "Anthropic API", "Gemini API", "Qwen API"]:
            # æª¢æŸ¥APIæ¨¡å‹
            if self.api_key is not None and len(self.api_key) > 0:
                return True
            else:
                st.warning(f"{self.model_name}éœ€è¦API Keyæ‰èƒ½ä½¿ç”¨")
                return False
        return False
    
    def match_paragraphs(self, source_paragraphs, target_paragraphs):
        """ä½¿ç”¨ç”Ÿæˆå¼AIåŒ¹é…æ®µè½"""
        if not self.is_available:
            st.warning(f"{self.model_name}ä¸å¯ç”¨ï¼Œå°‡ä½¿ç”¨åŸºæœ¬åŒ¹é…ç®—æ³•")
            return None
        
        try:
            if self.model_name == "è‡ªå®šç¾©AI":
                return self._match_with_custom_ai(source_paragraphs, target_paragraphs)
            elif self.model_name == "BERTå¤šèªè¨€æ¨¡å‹":
                return self._match_with_bert(source_paragraphs, target_paragraphs)
            elif self.model_name == "MPNetä¸­æ–‡æ¨¡å‹":
                return self._match_with_mpnet(source_paragraphs, target_paragraphs)
            elif self.model_name == "RoBERTaä¸­æ–‡æ¨¡å‹":
                return self._match_with_roberta(source_paragraphs, target_paragraphs)
            elif self.model_name == "OpenAI API":
                return self._match_with_openai(source_paragraphs, target_paragraphs)
            elif self.model_name == "Anthropic API":
                return self._match_with_anthropic(source_paragraphs, target_paragraphs)
            elif self.model_name == "Gemini API":
                return self._match_with_gemini(source_paragraphs, target_paragraphs)
            elif self.model_name == "Qwen API":
                return self._match_with_qwen(source_paragraphs, target_paragraphs)
        except Exception as e:
            st.error(f"{self.model_name}åŒ¹é…å¤±æ•—: {str(e)}")
            st.info("å°‡ä½¿ç”¨åŸºæœ¬åŒ¹é…ç®—æ³•ä½œç‚ºæ›¿ä»£")
            return None
        
        return None
    
    def _match_with_custom_ai(self, source_paragraphs, target_paragraphs):
        """ä½¿ç”¨è‡ªå®šç¾©AIåŒ¹é…æ®µè½"""
        if not self.custom_ai:
            return None
        
        # è¨ˆç®—ç›¸ä¼¼åº¦çŸ©é™£
        similarity_matrix = np.zeros((len(source_paragraphs), len(target_paragraphs)))
        for i, source_para in enumerate(source_paragraphs):
            for j, target_para in enumerate(target_paragraphs):
                similarity, error = self.custom_ai.semantic_comparison(source_para['content'], target_para['content'])
                if error:
                    st.warning(f"è¨ˆç®—ç›¸ä¼¼åº¦æ™‚å‡ºéŒ¯: {error}")
                similarity_matrix[i, j] = similarity
        
        # ä½¿ç”¨è²ªå©ªç®—æ³•æ‰¾åˆ°æœ€ä½³åŒ¹é…
        matches = []
        used_target_indices = set()
        
        # ç‚ºæ¯å€‹sourceæ®µè½æ‰¾åˆ°æœ€ä½³åŒ¹é…çš„targetæ®µè½
        for i in range(len(source_paragraphs)):
            best_similarity = -1
            best_j = -1
            
            # æ‰¾åˆ°æœ€ç›¸ä¼¼çš„æœªä½¿ç”¨çš„targetæ®µè½
            for j in range(len(target_paragraphs)):
                if j not in used_target_indices and similarity_matrix[i, j] > best_similarity:
                    best_similarity = similarity_matrix[i, j]
                    best_j = j
            
            # å¦‚æœæ‰¾åˆ°åŒ¹é…ï¼Œå‰‡æ·»åŠ åˆ°çµæœä¸­
            if best_j != -1:
                matches.append({
                    "doc1_index": i,
                    "doc2_index": best_j,
                    "similarity": best_similarity
                })
                used_target_indices.add(best_j)
            else:
                # å¦‚æœæ²’æœ‰æœªä½¿ç”¨çš„targetæ®µè½ï¼Œå‰‡é¸æ“‡æœ€ç›¸ä¼¼çš„æ®µè½
                best_j = np.argmax(similarity_matrix[i])
                matches.append({
                    "doc1_index": i,
                    "doc2_index": best_j,
                    "similarity": similarity_matrix[i, best_j]
                })
        
        return matches
    
    def _match_with_bert(self, source_paragraphs, target_paragraphs):
        """ä½¿ç”¨BERTå¤šèªè¨€æ¨¡å‹åŒ¹é…æ®µè½"""
        try:
            from transformers import BertModel, BertTokenizer
            import torch
            
            # åŠ è¼‰æ¨¡å‹
            tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')
            model = BertModel.from_pretrained('bert-base-multilingual-cased')
            
            # è¨ˆç®—åµŒå…¥
            source_embeddings = []
            for para in source_paragraphs:
                inputs = tokenizer(para['content'], return_tensors="pt", truncation=True, max_length=512)
                with torch.no_grad():
                    outputs = model(**inputs)
                embeddings = outputs.last_hidden_state.mean(dim=1)
                source_embeddings.append(embeddings.squeeze().numpy())
            
            target_embeddings = []
            for para in target_paragraphs:
                inputs = tokenizer(para['content'], return_tensors="pt", truncation=True, max_length=512)
                with torch.no_grad():
                    outputs = model(**inputs)
                embeddings = outputs.last_hidden_state.mean(dim=1)
                target_embeddings.append(embeddings.squeeze().numpy())
            
            # è¨ˆç®—ç›¸ä¼¼åº¦çŸ©é™£
            similarity_matrix = np.zeros((len(source_paragraphs), len(target_paragraphs)))
            for i, source_emb in enumerate(source_embeddings):
                for j, target_emb in enumerate(target_embeddings):
                    similarity = np.dot(source_emb, target_emb) / (np.linalg.norm(source_emb) * np.linalg.norm(target_emb))
                    similarity_matrix[i, j] = similarity
            
            # ä½¿ç”¨åŒˆç‰™åˆ©ç®—æ³•æ‰¾åˆ°æœ€ä½³åŒ¹é…
            try:
                from scipy.optimize import linear_sum_assignment
                row_ind, col_ind = linear_sum_assignment(-similarity_matrix)
                
                # æ§‹å»ºåŒ¹é…çµæœ
                matches = []
                for i, j in zip(row_ind, col_ind):
                    matches.append({
                        "doc1_index": i,
                        "doc2_index": j,
                        "similarity": similarity_matrix[i, j]
                    })
                
                return matches
            except ImportError:
                # ä½¿ç”¨è²ªå©ªç®—æ³•ä½œç‚ºæ›¿ä»£
                matches = []
                used_target_indices = set()
                
                # ç‚ºæ¯å€‹sourceæ®µè½æ‰¾åˆ°æœ€ä½³åŒ¹é…çš„targetæ®µè½
                for i in range(len(source_paragraphs)):
                    best_similarity = -1
                    best_j = -1
                    
                    # æ‰¾åˆ°æœ€ç›¸ä¼¼çš„æœªä½¿ç”¨çš„targetæ®µè½
                    for j in range(len(target_paragraphs)):
                        if j not in used_target_indices and similarity_matrix[i, j] > best_similarity:
                            best_similarity = similarity_matrix[i, j]
                            best_j = j
                    
                    # å¦‚æœæ‰¾åˆ°åŒ¹é…ï¼Œå‰‡æ·»åŠ åˆ°çµæœä¸­
                    if best_j != -1:
                        matches.append({
                            "doc1_index": i,
                            "doc2_index": best_j,
                            "similarity": best_similarity
                        })
                        used_target_indices.add(best_j)
                    else:
                        # å¦‚æœæ²’æœ‰æœªä½¿ç”¨çš„targetæ®µè½ï¼Œå‰‡é¸æ“‡æœ€ç›¸ä¼¼çš„æ®µè½
                        best_j = np.argmax(similarity_matrix[i])
                        matches.append({
                            "doc1_index": i,
                            "doc2_index": best_j,
                            "similarity": similarity_matrix[i, best_j]
                        })
                
                return matches
            
        except Exception as e:
            st.error(f"BERTåŒ¹é…å¤±æ•—: {e}")
            return None
    
    def _match_with_qwen(self, source_paragraphs, target_paragraphs):
        """ä½¿ç”¨Qwen APIåŒ¹é…æ®µè½"""
        try:
            import requests
            
            # æº–å‚™æ•¸æ“š
            source_texts = [p['content'] for p in source_paragraphs]
            target_texts = [p['content'] for p in target_paragraphs]
            
            # æ§‹å»ºæç¤º
            prompt = f"""
            æˆ‘æœ‰å…©çµ„æ–‡æœ¬æ®µè½ï¼Œéœ€è¦æ‰¾å‡ºå®ƒå€‘ä¹‹é–“çš„æœ€ä½³åŒ¹é…é—œä¿‚ã€‚
            
            ç¬¬ä¸€çµ„æ®µè½ï¼ˆåŸå§‹æ–‡æœ¬ï¼‰ï¼š
            {json.dumps(source_texts, ensure_ascii=False)}
            
            ç¬¬äºŒçµ„æ®µè½ï¼ˆç›®æ¨™æ–‡æœ¬ï¼‰ï¼š
            {json.dumps(target_texts, ensure_ascii=False)}
            
            è«‹åˆ†æé€™äº›æ®µè½ï¼Œæ‰¾å‡ºæ¯å€‹åŸå§‹æ®µè½åœ¨ç›®æ¨™æ®µè½ä¸­çš„æœ€ä½³åŒ¹é…ã€‚è¿”å›ä¸€å€‹JSONæ•¸çµ„ï¼Œæ¯å€‹å…ƒç´ åŒ…å«ï¼š
            1. "source_index": åŸå§‹æ®µè½çš„ç´¢å¼•ï¼ˆå¾0é–‹å§‹ï¼‰
            2. "target_index": åŒ¹é…çš„ç›®æ¨™æ®µè½çš„ç´¢å¼•ï¼ˆå¾0é–‹å§‹ï¼‰
            3. "similarity": ç›¸ä¼¼åº¦è©•åˆ†ï¼ˆ0åˆ°1ä¹‹é–“ï¼‰
            
            åªè¿”å›JSONæ•¸çµ„ï¼Œä¸è¦æœ‰å…¶ä»–è§£é‡‹ã€‚
            """
            
            # æº–å‚™APIè«‹æ±‚
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            }
            
            payload = {
                "model": "qwen-max",
                "input": {
                    "messages": [
                        {
                            "role": "system",
                            "content": "ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„æ–‡æœ¬åŒ¹é…åŠ©æ‰‹ï¼Œæ“…é•·åˆ†ææ–‡æœ¬ç›¸ä¼¼åº¦å’Œæ‰¾å‡ºæœ€ä½³åŒ¹é…ã€‚"
                        },
                        {
                            "role": "user",
                            "content": prompt
                        }
                    ]
                },
                "parameters": {
                    "result_format": "message"
                }
            }
            
            # ç™¼é€APIè«‹æ±‚
            response = requests.post(
                "https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation",
                headers=headers,
                json=payload
            )
            
            if response.status_code == 200:
                result = response.json()
                result_text = result["output"]["choices"][0]["message"]["content"]
                
                # æå–JSON
                import re
                json_match = re.search(r'\[.*\]', result_text, re.DOTALL)
                if json_match:
                    json_str = json_match.group(0)
                    matches_data = json.loads(json_str)
                    
                    # è½‰æ›ç‚ºæ‰€éœ€æ ¼å¼
                    matches = []
                    for item in matches_data:
                        matches.append({
                            "doc1_index": item["source_index"],
                            "doc2_index": item["target_index"],
                            "similarity": item["similarity"]
                        })
                    
                    return matches
            
            return None
        
        except Exception as e:
            st.error(f"Qwen APIåŒ¹é…å¤±æ•—: {e}")
            return None
    
    def analyze_comparison_results(self, comparison_results):
        """ä½¿ç”¨ç”Ÿæˆå¼AIåˆ†ææ¯”å°çµæœ"""
        if not self.is_available:
            return "AIåˆ†æä¸å¯ç”¨ï¼Œè«‹æª¢æŸ¥APIè¨­ç½®ã€‚"
        
        try:
            # æº–å‚™æ•¸æ“š
            total_paragraphs = comparison_results["statistics"]["total_paragraphs"]
            similar_paragraphs = comparison_results["statistics"]["similar_paragraphs"]
            different_paragraphs = comparison_results["statistics"]["different_paragraphs"]
            
            paragraph_similarity_percentage = (similar_paragraphs / total_paragraphs * 100) if total_paragraphs > 0 else 100
            
            # æå–ä¸åŒçš„æ®µè½
            different_paragraph_details = []
            for result in comparison_results["paragraph_results"]:
                if not result["is_similar"]:
                    different_paragraph_details.append({
                        "original_text": result["original_text"],
                        "matched_text": result["matched_text"],
                        "exact_similarity": result["exact_similarity"]
                    })
            
            # å¦‚æœæ˜¯è‡ªå®šç¾©AIï¼Œä½¿ç”¨CustomAIé€²è¡Œåˆ†æ
            if self.model_name == "è‡ªå®šç¾©AI" and self.custom_ai:
                # æå–åŸå§‹æ–‡æœ¬å’Œç·¨è¼¯å¾Œæ–‡æœ¬
                original_text = "\n".join([p["original_text"] for p in comparison_results["paragraph_results"]])
                edited_text = "\n".join([p["matched_text"] for p in comparison_results["paragraph_results"] if p["matched_text"]])
                
                return self.custom_ai.analyze_comparison_results(original_text, edited_text, comparison_results)
            
            # æ§‹å»ºæç¤º
            prompt = f"""
            æˆ‘éœ€è¦ä½ åˆ†ææ–‡ä»¶æ¯”å°çš„çµæœï¼Œä¸¦æä¾›å°ˆæ¥­çš„åˆ†æå ±å‘Šã€‚
            
            æ¯”å°çµ±è¨ˆä¿¡æ¯ï¼š
            - ç¸½æ®µè½æ•¸ï¼š{total_paragraphs}
            - ç›¸ä¼¼æ®µè½æ•¸ï¼š{similar_paragraphs}
            - ä¸åŒæ®µè½æ•¸ï¼š{different_paragraphs}
            - æ®µè½ç›¸ä¼¼åº¦ç™¾åˆ†æ¯”ï¼š{paragraph_similarity_percentage:.2f}%
            
            ä¸åŒæ®µè½çš„è©³ç´°ä¿¡æ¯ï¼ˆæœ€å¤šé¡¯ç¤ºå‰5å€‹ï¼‰ï¼š
            {json.dumps(different_paragraph_details[:5], ensure_ascii=False, indent=2)}
            
            è«‹æä¾›ä»¥ä¸‹åˆ†æï¼š
            1. æ•´é«”ç›¸ä¼¼åº¦è©•ä¼°
            2. ä¸»è¦å·®ç•°é¡å‹åˆ†é¡ï¼ˆä¾‹å¦‚ï¼šæ ¼å¼å·®ç•°ã€å…§å®¹éºæ¼ã€å…§å®¹æ›¿æ›ç­‰ï¼‰
            3. å·®ç•°çš„åš´é‡ç¨‹åº¦è©•ä¼°
            4. å»ºè­°çš„å¾ŒçºŒæ ¡å°é‡é»
            
            è«‹ç”¨å°ˆæ¥­ä½†æ˜“æ–¼ç†è§£çš„èªè¨€æ’°å¯«åˆ†æå ±å‘Šã€‚
            """
            
            if self.model_name == "Qwen API":
                return self._analyze_with_qwen(prompt)
            elif self.model_name == "OpenAI API":
                return self._analyze_with_openai(prompt)
            else:
                return "æ‰€é¸AIæ¨¡å‹ä¸æ”¯æŒåˆ†æåŠŸèƒ½ã€‚"
        
        except Exception as e:
            return f"AIåˆ†æå¤±æ•—: {str(e)}"
    
    def _analyze_with_qwen(self, prompt):
        """ä½¿ç”¨Qwen APIåˆ†ææ¯”å°çµæœ"""
        try:
            import requests
            
            # æº–å‚™APIè«‹æ±‚
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            }
            
            payload = {
                "model": "qwen-max",
                "input": {
                    "messages": [
                        {
                            "role": "system",
                            "content": "ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„æ–‡æœ¬æ¯”å°åˆ†æå¸«ï¼Œæ“…é•·åˆ†ææ–‡ä»¶æ¯”å°çµæœä¸¦æä¾›å°ˆæ¥­è¦‹è§£ã€‚"
                        },
                        {
                            "role": "user",
                            "content": prompt
                        }
                    ]
                },
                "parameters": {
                    "result_format": "message"
                }
            }
            
            # ç™¼é€APIè«‹æ±‚
            response = requests.post(
                "https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation",
                headers=headers,
                json=payload
            )
            
            if response.status_code == 200:
                result = response.json()
                analysis = result["output"]["choices"][0]["message"]["content"]
                return analysis
            else:
                return f"APIè«‹æ±‚å¤±æ•—: {response.status_code} - {response.text}"
        
        except Exception as e:
            return f"Qwen APIåˆ†æå¤±æ•—: {str(e)}"
    
    def _analyze_with_openai(self, prompt):
        """ä½¿ç”¨OpenAI APIåˆ†ææ¯”å°çµæœ"""
        try:
            import openai
            
            # è¨­ç½®APIå¯†é‘°
            openai.api_key = self.api_key
            
            # èª¿ç”¨API
            response = openai.ChatCompletion.create(
                model="gpt-4",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„æ–‡æœ¬æ¯”å°åˆ†æå¸«ï¼Œæ“…é•·åˆ†ææ–‡ä»¶æ¯”å°çµæœä¸¦æä¾›å°ˆæ¥­è¦‹è§£ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.2
            )
            
            # è§£æçµæœ
            analysis = response.choices[0].message.content
            return analysis
        
        except Exception as e:
            return f"OpenAI APIåˆ†æå¤±æ•—: {str(e)}"

# ç°¡åŒ–ç‰ˆçš„æ¯”å°ç®—æ³•ï¼Œä¸ä¾è³´sentence-transformers
def exact_matching(text1, text2, ignore_space=True, ignore_punctuation=True, ignore_case=True, ignore_newline=True):
    """ç²¾ç¢ºæ¯”å°å…©æ®µæ–‡æœ¬çš„ç›¸ä¼¼åº¦"""
    # æ–‡æœ¬é è™•ç†
    if ignore_space:
        text1 = re.sub(r'\s+', ' ', text1)
        text2 = re.sub(r'\s+', ' ', text2)
    
    if ignore_punctuation:
        text1 = re.sub(r'[.,;:!?ï¼Œã€‚ï¼›ï¼šï¼ï¼Ÿ]', '', text1)
        text2 = re.sub(r'[.,;:!?ï¼Œã€‚ï¼›ï¼šï¼ï¼Ÿ]', '', text2)
    
    if ignore_case:
        text1 = text1.lower()
        text2 = text2.lower()
    
    if ignore_newline:
        text1 = text1.replace('\n', ' ')
        text2 = text2.replace('\n', ' ')
    
    # ä½¿ç”¨SequenceMatcherè¨ˆç®—ç›¸ä¼¼åº¦
    matcher = difflib.SequenceMatcher(None, text1, text2)
    similarity = matcher.ratio()
    
    # ç”Ÿæˆå·®ç•°
    diff = list(difflib.ndiff(text1.splitlines(), text2.splitlines()))
    
    return similarity, diff

# èªæ„æ¯”å°å‡½æ•¸
def semantic_matching(text1, text2, model=None):
    """èªæ„æ¯”å°ï¼ˆä½¿ç”¨Sentence-BERTæˆ–ç°¡åŒ–ç‰ˆï¼‰"""
    if model is not None and is_sentence_transformers_available():
        try:
            # ä½¿ç”¨Sentence-BERTè¨ˆç®—èªç¾©ç›¸ä¼¼åº¦
            embedding1 = model.encode(text1)
            embedding2 = model.encode(text2)
            
            # è¨ˆç®—é¤˜å¼¦ç›¸ä¼¼åº¦
            similarity = np.dot(embedding1, embedding2) / (np.linalg.norm(embedding1) * np.linalg.norm(embedding2))
            return float(similarity)
        except Exception as e:
            st.warning(f"èªç¾©æ¨¡å‹è¨ˆç®—å¤±æ•—: {e}ï¼Œä½¿ç”¨ç°¡åŒ–ç‰ˆèªç¾©æ¯”å°")
    
    # ç°¡åŒ–ç‰ˆèªç¾©æ¯”å°ï¼ˆè©è¢‹æ¨¡å‹ï¼‰
    words1 = set(re.sub(r'[^\w\s]', '', text1.lower()).split())
    words2 = set(re.sub(r'[^\w\s]', '', text2.lower()).split())
    
    if not words1 or not words2:
        return 0.0
    
    # è¨ˆç®—Jaccardç›¸ä¼¼åº¦
    intersection = len(words1.intersection(words2))
    union = len(words1.union(words2))
    
    return intersection / union if union > 0 else 0.0

# ä¸Šä¸‹æ–‡æ„ŸçŸ¥åŒ¹é…
def context_aware_matching(text1, text2, context1=None, context2=None, ignore_options=None, model=None):
    """ä¸Šä¸‹æ–‡æ„ŸçŸ¥åŒ¹é…"""
    if ignore_options is None:
        ignore_options = {}
    
    # åŸºæœ¬ç›¸ä¼¼åº¦è¨ˆç®—
    exact_sim, diff = exact_matching(
        text1, text2,
        ignore_space=ignore_options.get("ignore_space", True),
        ignore_punctuation=ignore_options.get("ignore_punctuation", True),
        ignore_case=ignore_options.get("ignore_case", True),
        ignore_newline=ignore_options.get("ignore_newline", True)
    )
    
    # èªç¾©ç›¸ä¼¼åº¦è¨ˆç®—
    semantic_sim = semantic_matching(text1, text2, model)
    
    # å¦‚æœæœ‰ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œè¨ˆç®—ä¸Šä¸‹æ–‡ç›¸ä¼¼åº¦
    context_sim = 0.0
    if context1 and context2:
        # æå–å‰å¾Œæ–‡
        prev_sim = semantic_matching(context1.get("previous_text", ""), context2.get("previous_text", ""), model)
        next_sim = semantic_matching(context1.get("next_text", ""), context2.get("next_text", ""), model)
        
        # ä¸Šä¸‹æ–‡ç›¸ä¼¼åº¦ç‚ºå‰å¾Œæ–‡ç›¸ä¼¼åº¦çš„å¹³å‡å€¼
        context_sim = (prev_sim + next_sim) / 2
    
    # ç¶œåˆç›¸ä¼¼åº¦è¨ˆç®—ï¼ˆæ¬Šé‡å¯èª¿æ•´ï¼‰
    exact_weight = 0.5
    semantic_weight = 0.3
    context_weight = 0.2
    
    if not context1 or not context2:
        # å¦‚æœæ²’æœ‰ä¸Šä¸‹æ–‡ï¼Œèª¿æ•´æ¬Šé‡
        exact_weight = 0.6
        semantic_weight = 0.4
        context_weight = 0.0
    
    combined_similarity = (
        exact_weight * exact_sim +
        semantic_weight * semantic_sim +
        context_weight * context_sim
    )
    
    return combined_similarity, diff, {
        "exact_similarity": exact_sim,
        "semantic_similarity": semantic_sim,
        "context_similarity": context_sim
    }

# æ··åˆæ¯”å°å‡½æ•¸
def hybrid_matching(text1, text2, context1=None, context2=None, ignore_options=None, model=None):
    """æ··åˆæ¯”å°ï¼ˆçµåˆç²¾ç¢ºæ¯”å°å’Œèªç¾©æ¯”å°ï¼‰"""
    if ignore_options is None:
        ignore_options = {}
    
    # ç²¾ç¢ºæ¯”å°
    exact_sim, diff = exact_matching(
        text1, text2,
        ignore_space=ignore_options.get("ignore_space", True),
        ignore_punctuation=ignore_options.get("ignore_punctuation", True),
        ignore_case=ignore_options.get("ignore_case", True),
        ignore_newline=ignore_options.get("ignore_newline", True)
    )
    
    # èªç¾©æ¯”å°
    semantic_sim = semantic_matching(text1, text2, model)
    
    # ç¶œåˆç›¸ä¼¼åº¦è¨ˆç®—ï¼ˆæ¬Šé‡å¯èª¿æ•´ï¼‰
    exact_weight = 0.6
    semantic_weight = 0.4
    
    combined_similarity = exact_weight * exact_sim + semantic_weight * semantic_sim
    
    return combined_similarity, diff, {
        "exact_similarity": exact_sim,
        "semantic_similarity": semantic_sim
    }

# æ¯”å°æ–‡æª”å‡½æ•¸
def compare_documents(doc1, doc2, ignore_options=None, comparison_mode='exact', similarity_threshold=0.6, ai_instance=None, semantic_model=None):
    """æ¯”å°å…©å€‹æ–‡æª”çš„å…§å®¹"""
    if ignore_options is None:
        ignore_options = {}
    
    # åˆå§‹åŒ–çµæœ
    paragraph_results = []
    table_results = []
    
    # å¦‚æœä½¿ç”¨ç”Ÿæˆå¼AIæ¯”å°
    if comparison_mode == 'ç”Ÿæˆå¼AIæ¯”å°' and ai_instance is not None:
        # ä½¿ç”¨AIåŒ¹é…æ®µè½
        matches = ai_instance.match_paragraphs(doc1["paragraphs"], doc2["paragraphs"])
        
        if matches:
            # ä½¿ç”¨AIåŒ¹é…çµæœ
            for match in matches:
                doc1_index = match["doc1_index"]
                doc2_index = match["doc2_index"]
                similarity = match["similarity"]
                
                if doc1_index < len(doc1["paragraphs"]) and doc2_index < len(doc2["paragraphs"]):
                    para1 = doc1["paragraphs"][doc1_index]
                    para2 = doc2["paragraphs"][doc2_index]
                    
                    # ä½¿ç”¨ç²¾ç¢ºæ¯”å°ç²å–å·®ç•°
                    _, diff = exact_matching(
                        para1['content'], para2['content'],
                        ignore_space=ignore_options.get("ignore_space", True),
                        ignore_punctuation=ignore_options.get("ignore_punctuation", True),
                        ignore_case=ignore_options.get("ignore_case", True),
                        ignore_newline=ignore_options.get("ignore_newline", True)
                    )
                    
                    # åˆ¤æ–·æ˜¯å¦ç›¸ä¼¼
                    is_similar = similarity >= similarity_threshold
                    
                    # æ·»åŠ çµæœ
                    paragraph_results.append({
                        "original_index": doc1_index,
                        "original_text": para1["content"],
                        "matched_index": doc2_index,
                        "matched_text": para2["content"],
                        "matched_page": para2.get("page", "æœªæ‰¾åˆ°"),
                        "exact_similarity": similarity,
                        "semantic_similarity": similarity,
                        "is_similar": is_similar
                    })
            
            # è™•ç†æœªåŒ¹é…çš„æ®µè½
            matched_doc1_indices = set(match["doc1_index"] for match in matches)
            for i, para in enumerate(doc1["paragraphs"]):
                if i not in matched_doc1_indices:
                    paragraph_results.append({
                        "original_index": i,
                        "original_text": para["content"],
                        "matched_index": -1,
                        "matched_text": "",
                        "matched_page": "æœªæ‰¾åˆ°",
                        "exact_similarity": 0.0,
                        "semantic_similarity": 0.0,
                        "is_similar": False
                    })
        else:
            # å¦‚æœAIåŒ¹é…å¤±æ•—ï¼Œä½¿ç”¨æ··åˆæ¯”å°ä½œç‚ºå¾Œå‚™
            comparison_mode = 'æ··åˆæ¯”å°'
    
    # å¦‚æœä¸æ˜¯ä½¿ç”¨ç”Ÿæˆå¼AIæ¯”å°ï¼Œæˆ–è€…AIæ¯”å°å¤±æ•—
    if comparison_mode != 'ç”Ÿæˆå¼AIæ¯”å°' or not paragraph_results:
        # æ¯”å°æ®µè½
        for i, para1 in enumerate(doc1["paragraphs"]):
            best_match = None
            best_similarity = 0
            best_index = -1
            best_page = "æœªæ‰¾åˆ°"
            best_details = {}
            
            for j, para2 in enumerate(doc2["paragraphs"]):
                # æ ¹æ“šæ¯”å°æ¨¡å¼é¸æ“‡æ¯”å°æ–¹æ³•
                if comparison_mode == 'ç²¾ç¢ºæ¯”å°':
                    # ä½¿ç”¨ç²¾ç¢ºæ¯”å°
                    sim, diff = exact_matching(
                        para1['content'], para2['content'],
                        ignore_space=ignore_options.get("ignore_space", True),
                        ignore_punctuation=ignore_options.get("ignore_punctuation", True),
                        ignore_case=ignore_options.get("ignore_case", True),
                        ignore_newline=ignore_options.get("ignore_newline", True)
                    )
                    details = {"exact_similarity": sim}
                
                elif comparison_mode == 'èªæ„æ¯”å°':
                    # ä½¿ç”¨èªç¾©æ¯”å°
                    sim = semantic_matching(para1['content'], para2['content'], semantic_model)
                    _, diff = exact_matching(para1['content'], para2['content'])  # ä»ç„¶éœ€è¦å·®ç•°ä¿¡æ¯
                    details = {"semantic_similarity": sim}
                
                elif comparison_mode == 'æ··åˆæ¯”å°':
                    # ä½¿ç”¨æ··åˆæ¯”å°
                    sim, diff, details = hybrid_matching(
                        para1['content'], para2['content'],
                        None, None,
                        ignore_options,
                        semantic_model
                    )
                
                elif comparison_mode == 'ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ¯”å°':
                    # ä½¿ç”¨ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ¯”å°
                    # æå–ä¸Šä¸‹æ–‡ä¿¡æ¯
                    context1 = para1.get('context', {})
                    context2 = para2.get('context', {})
                    
                    sim, diff, details = context_aware_matching(
                        para1['content'], para2['content'],
                        context1, context2,
                        ignore_options,
                        semantic_model
                    )
                
                else:
                    # é»˜èªä½¿ç”¨ç²¾ç¢ºæ¯”å°
                    sim, diff = exact_matching(
                        para1['content'], para2['content'],
                        ignore_space=ignore_options.get("ignore_space", True),
                        ignore_punctuation=ignore_options.get("ignore_punctuation", True),
                        ignore_case=ignore_options.get("ignore_case", True),
                        ignore_newline=ignore_options.get("ignore_newline", True)
                    )
                    details = {"exact_similarity": sim}
                
                if sim > best_similarity:
                    best_similarity = sim
                    best_match = para2
                    best_index = j
                    best_page = para2.get("page", "æœªæ‰¾åˆ°")
                    best_details = details
            
            # åˆ¤æ–·æ˜¯å¦ç›¸ä¼¼
            is_similar = best_similarity >= similarity_threshold
            
            # æ·»åŠ çµæœ
            result = {
                "original_index": i,
                "original_text": para1["content"],
                "matched_index": best_index,
                "matched_text": best_match["content"] if best_match else "",
                "matched_page": best_page,
                "exact_similarity": best_details.get("exact_similarity", best_similarity),
                "is_similar": is_similar
            }
            
            # æ·»åŠ å…¶ä»–ç›¸ä¼¼åº¦ä¿¡æ¯
            if "semantic_similarity" in best_details:
                result["semantic_similarity"] = best_details["semantic_similarity"]
            if "context_similarity" in best_details:
                result["context_similarity"] = best_details["context_similarity"]
            
            paragraph_results.append(result)
    
    # æ¯”å°è¡¨æ ¼
    for i, table1 in enumerate(doc1.get("tables", [])):
        best_match = None
        best_similarity = 0
        best_index = -1
        best_page = "æœªæ‰¾åˆ°"
        
        for j, table2 in enumerate(doc2.get("tables", [])):
            # è¨ˆç®—è¡¨æ ¼ç›¸ä¼¼åº¦
            table_similarity = calculate_table_similarity(table1["content"], table2["content"])
            
            if table_similarity > best_similarity:
                best_similarity = table_similarity
                best_match = table2
                best_index = j
                best_page = table2.get("page", "æœªæ‰¾åˆ°")
        
        # åˆ¤æ–·æ˜¯å¦ç›¸ä¼¼
        is_similar = best_similarity >= similarity_threshold
        
        # æ·»åŠ çµæœ
        table_results.append({
            "original_index": i,
            "original_table": table1["content"],
            "matched_index": best_index,
            "matched_table": best_match["content"] if best_match else None,
            "matched_page": best_page,
            "similarity": best_similarity,
            "is_similar": is_similar
        })
    
    # è¨ˆç®—çµ±è¨ˆä¿¡æ¯
    statistics = {
        "total_paragraphs": len(paragraph_results),
        "similar_paragraphs": sum(1 for r in paragraph_results if r["is_similar"]),
        "different_paragraphs": sum(1 for r in paragraph_results if not r["is_similar"]),
        "total_tables": len(table_results),
        "similar_tables": sum(1 for r in table_results if r["is_similar"]),
        "different_tables": sum(1 for r in table_results if not r["is_similar"])
    }
    
    return {
        "paragraph_results": paragraph_results,
        "table_results": table_results,
        "statistics": statistics
    }

def calculate_table_similarity(table1, table2):
    """è¨ˆç®—å…©å€‹è¡¨æ ¼çš„ç›¸ä¼¼åº¦"""
    # å¦‚æœè¡¨æ ¼ç‚ºç©ºï¼Œè¿”å›0
    if not table1 or not table2:
        return 0
    
    # å°‡è¡¨æ ¼è½‰æ›ç‚ºæ–‡æœ¬
    text1 = "\n".join([" ".join(row) for row in table1])
    text2 = "\n".join([" ".join(row) for row in table2])
    
    # ä½¿ç”¨ç²¾ç¢ºæ¯”å°
    similarity, _ = exact_matching(text1, text2)
    
    return similarity

def format_diff_html(diff, mode="å­—ç¬¦ç´šåˆ¥"):
    """å°‡å·®ç•°æ ¼å¼åŒ–ç‚ºHTML"""
    if not diff:
        return ""
    
    if mode == "å­—ç¬¦ç´šåˆ¥":
        # å­—ç¬¦ç´šåˆ¥å·®ç•°
        result = []
        for line in diff:
            if line.startswith('- '):
                result.append(f'<span class="diff-char-removed">{line[2:]}</span>')
            elif line.startswith('+ '):
                result.append(f'<span class="diff-char-added">{line[2:]}</span>')
            elif line.startswith('  '):
                result.append(line[2:])
        return "".join(result)
    
    elif mode == "è©èªç´šåˆ¥":
        # è©èªç´šåˆ¥å·®ç•°
        result = []
        for line in diff:
            if line.startswith('- '):
                result.append(f'<span class="diff-removed">{line[2:]}</span><br>')
            elif line.startswith('+ '):
                result.append(f'<span class="diff-added">{line[2:]}</span><br>')
            elif line.startswith('  '):
                result.append(f'{line[2:]}<br>')
        return "".join(result)
    
    else:  # è¡Œç´šåˆ¥
        # è¡Œç´šåˆ¥å·®ç•°
        result = []
        for line in diff:
            if line.startswith('- '):
                result.append(f'<div class="diff-removed">{line[2:]}</div>')
            elif line.startswith('+ '):
                result.append(f'<div class="diff-added">{line[2:]}</div>')
            elif line.startswith('  '):
                result.append(f'<div>{line[2:]}</div>')
        return "".join(result)

def generate_comparison_report(comparison_results, diff_display_mode="å­—ç¬¦ç´šåˆ¥", show_all_content=False):
    """ç”Ÿæˆæ¯”å°å ±å‘Š"""
    # è™•ç†æ®µè½æ¯”å°çµæœ
    paragraph_details = []
    for result in comparison_results["paragraph_results"]:
        # ç”Ÿæˆå·®ç•°HTML
        diff_html = ""
        if result["matched_text"]:
            # ä½¿ç”¨ç²¾ç¢ºæ¯”å°
            _, diff = exact_matching(result["original_text"], result["matched_text"])
            diff_html = format_diff_html(diff, diff_display_mode)
        
        # æ·»åŠ è©³ç´°ä¿¡æ¯
        paragraph_details.append({
            "original_index": result["original_index"],
            "original_text": result["original_text"],
            "matched_text": result["matched_text"],
            "matched_page": result["matched_page"],
            "exact_similarity": result["exact_similarity"],
            "semantic_similarity": result.get("semantic_similarity", 0.0),
            "context_similarity": result.get("context_similarity", 0.0),
            "is_similar": result["is_similar"],
            "diff_html": diff_html
        })
    
    # è™•ç†è¡¨æ ¼æ¯”å°çµæœ
    table_details = []
    for result in comparison_results["table_results"]:
        table_details.append({
            "original_index": result["original_index"],
            "original_table": result["original_table"],
            "matched_table": result["matched_table"],
            "matched_page": result["matched_page"],
            "similarity": result["similarity"],
            "is_similar": result["is_similar"]
        })
    
    # è¨ˆç®—æ‘˜è¦ä¿¡æ¯
    total_paragraphs = comparison_results["statistics"]["total_paragraphs"]
    similar_paragraphs = comparison_results["statistics"]["similar_paragraphs"]
    different_paragraphs = comparison_results["statistics"]["different_paragraphs"]
    
    total_tables = comparison_results["statistics"]["total_tables"]
    similar_tables = comparison_results["statistics"]["similar_tables"]
    different_tables = comparison_results["statistics"]["different_tables"]
    
    # è¨ˆç®—ç›¸ä¼¼åº¦ç™¾åˆ†æ¯”
    paragraph_similarity_percentage = (similar_paragraphs / total_paragraphs * 100) if total_paragraphs > 0 else 100
    table_similarity_percentage = (similar_tables / total_tables * 100) if total_tables > 0 else 100
    
    # ç”Ÿæˆæ‘˜è¦
    summary = {
        "total_paragraphs": total_paragraphs,
        "similar_paragraphs": similar_paragraphs,
        "different_paragraphs": different_paragraphs,
        "paragraph_similarity_percentage": paragraph_similarity_percentage,
        "total_tables": total_tables,
        "similar_tables": similar_tables,
        "different_tables": different_tables,
        "table_similarity_percentage": table_similarity_percentage
    }
    
    return {
        "summary": summary,
        "paragraph_details": paragraph_details,
        "table_details": table_details
    }

# ç”ŸæˆPDFé é¢é è¦½ä¸¦æ¨™è¨˜å·®ç•°
def generate_pdf_preview_with_diff(pdf_path, page_number, diff_text, temp_dir):
    """ç”ŸæˆPDFé é¢é è¦½ä¸¦æ¨™è¨˜å·®ç•°"""
    try:
        # æ‰“é–‹PDFæ–‡ä»¶
        doc = fitz.open(pdf_path)
        
        # æª¢æŸ¥é ç¢¼æ˜¯å¦æœ‰æ•ˆ
        if page_number < 1 or page_number > len(doc):
            return None, f"ç„¡æ•ˆçš„é ç¢¼: {page_number}ï¼ŒPDFå…±æœ‰{len(doc)}é "
        
        # ç²å–é é¢
        page = doc[page_number - 1]
        
        # å°‡é é¢è½‰æ›ç‚ºåœ–åƒ
        pix = page.get_pixmap(matrix=fitz.Matrix(300/72, 300/72))  # 300 DPI
        
        # ä¿å­˜ç‚ºè‡¨æ™‚åœ–åƒæ–‡ä»¶
        img_path = os.path.join(temp_dir, f"page_{page_number}.png")
        pix.save(img_path)
        
        # ä½¿ç”¨PILæ‰“é–‹åœ–åƒ
        img = Image.open(img_path)
        draw = ImageDraw.Draw(img)
        
        # å˜—è©¦åŠ è¼‰å­—é«”
        try:
            font = ImageFont.truetype("Arial", 14)
        except IOError:
            font = ImageFont.load_default()
        
        # åœ¨åœ–åƒä¸Šæ¨™è¨˜å·®ç•°
        # é€™è£¡éœ€è¦å¯¦ç¾ä¸€å€‹ç®—æ³•ä¾†å®šä½å·®ç•°æ–‡æœ¬åœ¨é é¢ä¸Šçš„ä½ç½®
        # ç”±æ–¼é€™æ˜¯ä¸€å€‹è¤‡é›œçš„ä»»å‹™ï¼Œé€™è£¡åªæ˜¯ç°¡å–®åœ°åœ¨é é¢åº•éƒ¨æ·»åŠ ä¸€å€‹å·®ç•°èªªæ˜
        
        # åœ¨é é¢åº•éƒ¨æ·»åŠ å·®ç•°èªªæ˜
        draw.rectangle([(10, img.height - 50), (img.width - 10, img.height - 10)], fill=(255, 240, 240))
        draw.text((20, img.height - 40), f"å·®ç•°: {diff_text[:100]}...", fill=(255, 0, 0), font=font)
        
        # ä¿å­˜ä¿®æ”¹å¾Œçš„åœ–åƒ
        marked_img_path = os.path.join(temp_dir, f"page_{page_number}_marked.png")
        img.save(marked_img_path)
        
        # é—œé–‰PDFæ–‡ä»¶
        doc.close()
        
        return marked_img_path, None
    
    except Exception as e:
        return None, f"ç”ŸæˆPDFé è¦½æ™‚å‡ºéŒ¯: {str(e)}"

# åˆå§‹åŒ–æœƒè©±ç‹€æ…‹
def init_session_state():
    if 'word_data' not in st.session_state:
        st.session_state.word_data = None
    if 'pdf_data' not in st.session_state:
        st.session_state.pdf_data = None
    if 'comparison_results' not in st.session_state:
        st.session_state.comparison_results = None
    if 'comparison_report' not in st.session_state:
        st.session_state.comparison_report = None
    if 'ai_analysis' not in st.session_state:
        st.session_state.ai_analysis = None
    if 'ai_summary_report' not in st.session_state:
        st.session_state.ai_summary_report = None
    if 'processing_complete' not in st.session_state:
        st.session_state.processing_complete = False
    if 'current_paragraph_index' not in st.session_state:
        st.session_state.current_paragraph_index = 0
    if 'current_table_index' not in st.session_state:
        st.session_state.current_table_index = 0
    if 'show_all_content' not in st.session_state:
        st.session_state.show_all_content = False
    if 'diff_display_mode' not in st.session_state:
        st.session_state.diff_display_mode = "å­—ç¬¦ç´šåˆ¥"
    if 'comparison_mode' not in st.session_state:
        st.session_state.comparison_mode = "æ··åˆæ¯”å°"
    if 'similarity_threshold' not in st.session_state:
        st.session_state.similarity_threshold = 0.8
    if 'ignore_options' not in st.session_state:
        st.session_state.ignore_options = {
            "ignore_space": True,
            "ignore_punctuation": True,
            "ignore_case": True,
            "ignore_newline": True
        }
    if 'use_ocr' not in st.session_state:
        st.session_state.use_ocr = True
    if 'ocr_engine' not in st.session_state:
        st.session_state.ocr_engine = "è‡ªå‹•é¸æ“‡"
    if 'qwen_api_key' not in st.session_state:
        st.session_state.qwen_api_key = ""
    if 'ai_model' not in st.session_state:
        st.session_state.ai_model = "Qwen API"
    if 'ai_api_key' not in st.session_state:
        st.session_state.ai_api_key = ""
    if 'ai_api_url' not in st.session_state:
        st.session_state.ai_api_url = ""
    if 'pdf_page_images' not in st.session_state:
        st.session_state.pdf_page_images = {}
    if 'highlighted_images' not in st.session_state:
        st.session_state.highlighted_images = {}
    if 'use_example_files' not in st.session_state:
        st.session_state.use_example_files = False
    if 'temp_dir' not in st.session_state:
        st.session_state.temp_dir = tempfile.mkdtemp()

# å´é‚Šæ¬„è¨­ç½®
def sidebar_settings():
    with st.sidebar:
        st.title("æœŸåˆŠæ¯”å°ç³»çµ±")
        
        # é¡¯ç¤ºç³»çµ±ç‹€æ…‹
        with st.expander("ç³»çµ±ç‹€æ…‹", expanded=False):
            col1, col2 = st.columns(2)
            
            with col1:
                if is_java_installed():
                    st.success("Javaå·²å®‰è£")
                else:
                    st.error("Javaæœªå®‰è£ï¼Œè¡¨æ ¼æå–åŠŸèƒ½å¯èƒ½å—é™")
                
                if is_easyocr_available():
                    st.success("OpenOCR (EasyOCR) å·²å®‰è£")
                else:
                    st.warning("OpenOCRæœªå®‰è£ï¼Œå°‡ä½¿ç”¨Tesseractä½œç‚ºæ›¿ä»£")
            
            with col2:
                if is_tabula_available():
                    st.success("è¡¨æ ¼æå–å·¥å…·å·²å®‰è£")
                else:
                    st.error("è¡¨æ ¼æå–å·¥å…·æœªå®‰è£æˆ–ç„¡æ³•ä½¿ç”¨")
                
                if is_sentence_transformers_available():
                    st.success("èªç¾©æ¨¡å‹å·²å®‰è£")
                else:
                    st.warning("èªç¾©æ¨¡å‹æœªå®‰è£ï¼Œèªç¾©æ¯”å°åŠŸèƒ½å°‡ä½¿ç”¨ç°¡åŒ–ç‰ˆ")
        
        # ç³»çµ±è¨­ç½®
        st.header("ç³»çµ±è¨­ç½®")
        
        # ç¤ºä¾‹æ–‡ä»¶é¸é …
        st.session_state.use_example_files = st.checkbox("ä½¿ç”¨ç¤ºä¾‹æ–‡ä»¶é€²è¡Œæ¼”ç¤º", value=st.session_state.use_example_files)
        
        # æ¯”å°è¨­ç½®
        st.subheader("æ¯”å°è¨­ç½®")
        st.session_state.comparison_mode = st.radio(
            "æ¯”å°æ¨¡å¼",
            ["ç²¾ç¢ºæ¯”å°", "èªæ„æ¯”å°", "æ··åˆæ¯”å°", "ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ¯”å°", "ç”Ÿæˆå¼AIæ¯”å°"],
            index=2
        )
        
        # å¦‚æœé¸æ“‡ç”Ÿæˆå¼AIæ¯”å°ï¼Œé¡¯ç¤ºæ¨¡å‹é¸æ“‡
        if st.session_state.comparison_mode == "ç”Ÿæˆå¼AIæ¯”å°":
            st.markdown('<div class="ai-model-section">', unsafe_allow_html=True)
            st.subheader("ç”Ÿæˆå¼AIè¨­ç½®")
            
            st.session_state.ai_model = st.selectbox(
                "é¸æ“‡AIæ¨¡å‹",
                ["BERTå¤šèªè¨€æ¨¡å‹", "MPNetä¸­æ–‡æ¨¡å‹", "RoBERTaä¸­æ–‡æ¨¡å‹", 
                 "OpenAI API", "Anthropic API", "Gemini API", "Qwen API", "è‡ªå®šç¾©AI"],
                index=6,
                help="é¸æ“‡ç”¨æ–¼æ®µè½åŒ¹é…çš„ç”Ÿæˆå¼AIæ¨¡å‹"
            )
            
            # å¦‚æœé¸æ“‡APIæ¨¡å‹ï¼Œé¡¯ç¤ºAPI Keyè¼¸å…¥æ¡†
            if st.session_state.ai_model in ["OpenAI API", "Anthropic API", "Gemini API", "Qwen API"]:
                st.session_state.ai_api_key = st.text_input(f"{st.session_state.ai_model} Key", 
                                                          type="password", 
                                                          value=st.session_state.ai_api_key,
                                                          help=f"è¼¸å…¥æ‚¨çš„{st.session_state.ai_model} Key")
                
                if st.session_state.ai_model == "OpenAI API":
                    st.info("OpenAI APIä½¿ç”¨GPT-4æ¨¡å‹ï¼Œæä¾›é«˜ç²¾åº¦æ®µè½åŒ¹é…")
                elif st.session_state.ai_model == "Anthropic API":
                    st.info("Anthropic APIä½¿ç”¨Claudeæ¨¡å‹ï¼Œæä¾›é«˜ç²¾åº¦æ®µè½åŒ¹é…")
                elif st.session_state.ai_model == "Gemini API":
                    st.info("Gemini APIä½¿ç”¨Googleçš„Gemini Proæ¨¡å‹ï¼Œæä¾›é«˜ç²¾åº¦æ®µè½åŒ¹é…")
                elif st.session_state.ai_model == "Qwen API":
                    st.info("Qwen APIä½¿ç”¨é˜¿é‡Œå·´å·´çš„Qwen Maxæ¨¡å‹ï¼Œæä¾›é«˜ç²¾åº¦æ®µè½åŒ¹é…")
            elif st.session_state.ai_model == "è‡ªå®šç¾©AI":
                st.session_state.ai_api_key = st.text_input("API Key (å¯é¸)", 
                                                          type="password", 
                                                          value=st.session_state.ai_api_key,
                                                          help="è¼¸å…¥æ‚¨çš„API Keyï¼Œå¦‚æœä¸æä¾›å°‡ä½¿ç”¨å…è²»API")
                
                st.session_state.ai_api_url = st.text_input("API URL (å¯é¸)", 
                                                          value=st.session_state.ai_api_url,
                                                          help="è¼¸å…¥API URLï¼Œå¦‚æœä¸æä¾›å°‡ä½¿ç”¨å…è²»API")
                
                st.info("è‡ªå®šç¾©AIæ”¯æ´å¤šç¨®APIæ ¼å¼ï¼ˆOpenAIã€Anthropicã€Qwenï¼‰ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨å…è²»API")
            else:
                st.info("æœ¬åœ°æ¨¡å‹ç„¡éœ€API Keyï¼Œä½†ç²¾åº¦å¯èƒ½ä½æ–¼APIæ¨¡å‹")
            
            st.markdown('</div>', unsafe_allow_html=True)
        
        # ç›¸ä¼¼åº¦é–¾å€¼
        st.session_state.similarity_threshold = st.slider(
            "ç›¸ä¼¼åº¦é–¾å€¼",
            min_value=0.0,
            max_value=1.0,
            value=st.session_state.similarity_threshold,
            step=0.05,
            help="ç›¸ä¼¼åº¦ä½æ–¼æ­¤é–¾å€¼çš„æ®µè½å°‡è¢«æ¨™è¨˜ç‚ºä¸ä¸€è‡´"
        )
        
        # å¿½ç•¥é¸é …
        st.subheader("å¿½ç•¥é¸é …")
        st.session_state.ignore_options["ignore_space"] = st.checkbox("å¿½ç•¥ç©ºæ ¼", value=st.session_state.ignore_options["ignore_space"])
        st.session_state.ignore_options["ignore_punctuation"] = st.checkbox("å¿½ç•¥æ¨™é»ç¬¦è™Ÿ", value=st.session_state.ignore_options["ignore_punctuation"])
        st.session_state.ignore_options["ignore_case"] = st.checkbox("å¿½ç•¥å¤§å°å¯«", value=st.session_state.ignore_options["ignore_case"])
        st.session_state.ignore_options["ignore_newline"] = st.checkbox("å¿½ç•¥æ›è¡Œ", value=st.session_state.ignore_options["ignore_newline"])
        
        # OCRè¨­ç½®
        st.subheader("OCRè¨­ç½®")
        st.session_state.ocr_engine = st.radio(
            "OCRå¼•æ“",
            ["è‡ªå‹•é¸æ“‡", "Tesseract", "OpenOCR (EasyOCR)", "Qwen API"],
            index=0,
            help="é¸æ“‡ç”¨æ–¼æå–PDFæ–‡æœ¬çš„OCRå¼•æ“"
        )
        
        # å¦‚æœé¸æ“‡Qwen APIï¼Œé¡¯ç¤ºAPI Keyè¼¸å…¥æ¡†
        if st.session_state.ocr_engine == "Qwen API":
            st.session_state.qwen_api_key = st.text_input("Qwen API Key", 
                                                        type="password", 
                                                        value=st.session_state.qwen_api_key,
                                                        help="è¼¸å…¥æ‚¨çš„Qwen API Key")
            st.info("Qwen APIæä¾›é«˜ç²¾åº¦OCRå’Œè¡¨æ ¼è­˜åˆ¥ï¼Œç‰¹åˆ¥é©åˆè¤‡é›œæ’ç‰ˆçš„PDF")
        else:
            st.session_state.use_ocr = st.checkbox("ä½¿ç”¨OCRæå–", value=st.session_state.use_ocr, help="å•Ÿç”¨OCRå¯ä»¥æé«˜æ–‡æœ¬æå–è³ªé‡ï¼Œä½†æœƒå¢åŠ è™•ç†æ™‚é–“")
        
        # é¡¯ç¤ºè¨­ç½®
        st.subheader("é¡¯ç¤ºè¨­ç½®")
        st.session_state.diff_display_mode = st.selectbox(
            "å·®ç•°é¡¯ç¤ºæ¨¡å¼",
            ["å­—ç¬¦ç´šåˆ¥", "è©èªç´šåˆ¥", "è¡Œç´šåˆ¥"],
            index=0
        )
        
        st.session_state.show_all_content = st.checkbox("é¡¯ç¤ºæ‰€æœ‰å…§å®¹", value=st.session_state.show_all_content)
        
        # ç³»çµ±è³‡è¨Š
        st.subheader("ç³»çµ±è³‡è¨Š")
        st.info("æœ¬ç³»çµ±ç”¨æ–¼æ¯”å°åŸå§‹Wordæ–‡ä»¶èˆ‡ç¾ç·¨å¾ŒPDFæ–‡ä»¶çš„å…§å®¹å·®ç•°ï¼Œå¹«åŠ©æ ¡å°äººå“¡å¿«é€Ÿæ‰¾å‡ºä¸ä¸€è‡´ä¹‹è™•ã€‚")

# æ–‡ä»¶ä¸Šå‚³å€åŸŸ
def file_upload_section():
    st.header("æ–‡ä»¶ä¸Šå‚³")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("åŸå§‹Wordæ–‡ä»¶")
        word_file = st.file_uploader("ä¸Šå‚³åŸå§‹Wordæ–‡ä»¶", type=["docx"], key="word_uploader", disabled=st.session_state.use_example_files)
        
        if word_file:
            st.success(f"å·²ä¸Šå‚³: {word_file.name}")
    
    with col2:
        st.subheader("ç¾ç·¨å¾ŒPDFæ–‡ä»¶")
        pdf_file = st.file_uploader("ä¸Šå‚³ç¾ç·¨å¾ŒPDFæ–‡ä»¶", type=["pdf"], key="pdf_uploader", disabled=st.session_state.use_example_files)
        
        if pdf_file:
            st.success(f"å·²ä¸Šå‚³: {pdf_file.name}")
    
    # ä½¿ç”¨ç¤ºä¾‹æ–‡ä»¶
    if st.session_state.use_example_files:
        st.warning("ç¤ºä¾‹æ–‡ä»¶åŠŸèƒ½éœ€è¦ä¸Šå‚³æ‚¨è‡ªå·±çš„æ–‡ä»¶ã€‚è«‹å–æ¶ˆå‹¾é¸ã€Œä½¿ç”¨ç¤ºä¾‹æ–‡ä»¶é€²è¡Œæ¼”ç¤ºã€é¸é …ï¼Œç„¶å¾Œä¸Šå‚³æ‚¨çš„æ–‡ä»¶ã€‚")
        word_file = None
        pdf_file = None
    
    if word_file and pdf_file:
        if st.button("é–‹å§‹æ¯”å°", key="start_comparison"):
            with st.spinner("æ­£åœ¨æå–æ–‡ä»¶å…§å®¹ä¸¦é€²è¡Œæ¯”å°..."):
                # åˆå§‹åŒ–OCRå¯¦ä¾‹
                ocr_instance = None
                if st.session_state.ocr_engine == "Qwen API" and st.session_state.qwen_api_key:
                    ocr_instance = QwenOCR(st.session_state.qwen_api_key)
                
                # åˆå§‹åŒ–AIå¯¦ä¾‹
                ai_instance = None
                if st.session_state.comparison_mode == "ç”Ÿæˆå¼AIæ¯”å°":
                    ai_instance = GenerativeAI(
                        st.session_state.ai_model, 
                        st.session_state.ai_api_key,
                        st.session_state.ai_api_url
                    )
                
                # åŠ è¼‰èªç¾©æ¨¡å‹
                semantic_model = None
                if st.session_state.comparison_mode in ["èªæ„æ¯”å°", "æ··åˆæ¯”å°", "ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ¯”å°"]:
                    semantic_model = load_semantic_model()
                
                # æå–æ–‡ä»¶å…§å®¹
                word_data, pdf_data = extract_and_process_documents(
                    word_file, 
                    pdf_file, 
                    st.session_state.use_ocr, 
                    st.session_state.ocr_engine,
                    ocr_instance
                )
                
                st.session_state.word_data = word_data
                st.session_state.pdf_data = pdf_data
                
                # é€²è¡Œæ¯”å°
                comparison_results = compare_documents(
                    word_data,
                    pdf_data,
                    st.session_state.ignore_options,
                    st.session_state.comparison_mode,
                    st.session_state.similarity_threshold,
                    ai_instance,
                    semantic_model
                )
                
                st.session_state.comparison_results = comparison_results
                
                # ç”Ÿæˆæ¯”å°å ±å‘Š
                comparison_report = generate_comparison_report(
                    comparison_results,
                    st.session_state.diff_display_mode,
                    st.session_state.show_all_content
                )
                
                st.session_state.comparison_report = comparison_report
                
                # å¦‚æœä½¿ç”¨ç”Ÿæˆå¼AIï¼Œç”ŸæˆAIåˆ†æå ±å‘Š
                if ai_instance and ai_instance.is_available:
                    with st.spinner("æ­£åœ¨ä½¿ç”¨AIåˆ†ææ¯”å°çµæœ..."):
                        ai_analysis = ai_instance.analyze_comparison_results(comparison_results)
                        st.session_state.ai_analysis = ai_analysis
                
                # æå–PDFé é¢åœ–åƒ
                with st.spinner("æ­£åœ¨æå–PDFé é¢åœ–åƒ..."):
                    # ä¿å­˜ä¸Šå‚³çš„PDFæ–‡ä»¶åˆ°è‡¨æ™‚æ–‡ä»¶
                    temp_pdf_path = os.path.join(st.session_state.temp_dir, "temp.pdf")
                    
                    with open(temp_pdf_path, "wb") as f:
                        f.write(pdf_file.getvalue())
                    
                    # æ‰“é–‹PDFæ–‡ä»¶
                    pdf_doc = fitz.open(temp_pdf_path)
                    
                    # æå–æ¯ä¸€é çš„åœ–åƒ
                    for page_num in range(len(pdf_doc)):
                        page = pdf_doc[page_num]
                        pix = page.get_pixmap(matrix=fitz.Matrix(300/72, 300/72))
                        
                        # å°‡åœ–åƒä¿å­˜åˆ°è‡¨æ™‚æ–‡ä»¶
                        img_path = os.path.join(st.session_state.temp_dir, f"page_{page_num+1}.png")
                        pix.save(img_path)
                        
                        # å°‡åœ–åƒè®€å–ç‚ºPILåœ–åƒ
                        img = Image.open(img_path)
                        
                        # å°‡åœ–åƒè½‰æ›ç‚ºbytes
                        img_byte_arr = io.BytesIO()
                        img.save(img_byte_arr, format='PNG')
                        img_byte_arr = img_byte_arr.getvalue()
                        
                        # ä¿å­˜åˆ°session_state
                        st.session_state.pdf_page_images[page_num+1] = img_byte_arr
                    
                    # é—œé–‰PDFæ–‡ä»¶
                    pdf_doc.close()
                
                # ç‚ºä¸åŒçš„æ®µè½ç”Ÿæˆæ¨™è¨˜å¾Œçš„PDFé é¢é è¦½
                with st.spinner("æ­£åœ¨ç”Ÿæˆæ¨™è¨˜å¾Œçš„PDFé é¢é è¦½..."):
                    for result in comparison_report["paragraph_details"]:
                        if not result["is_similar"] and result["matched_page"] != "æœªæ‰¾åˆ°":
                            try:
                                page_num = int(result["matched_page"])
                                
                                # ç”Ÿæˆæ¨™è¨˜å¾Œçš„é é¢é è¦½
                                marked_img_path, error = generate_pdf_preview_with_diff(
                                    temp_pdf_path,
                                    page_num,
                                    result["original_text"],
                                    st.session_state.temp_dir
                                )
                                
                                if marked_img_path and not error:
                                    # è®€å–æ¨™è¨˜å¾Œçš„åœ–åƒ
                                    with open(marked_img_path, "rb") as f:
                                        img_bytes = f.read()
                                    
                                    # ä¿å­˜åˆ°session_state
                                    key = f"{page_num}_{result['original_index']}"
                                    st.session_state.highlighted_images[key] = img_bytes
                            except:
                                pass
                
                st.session_state.processing_complete = True

# é¡¯ç¤ºæ¯”å°çµæœ
def display_comparison_results():
    if st.session_state.processing_complete and st.session_state.comparison_results and st.session_state.comparison_report:
        st.header("æ¯”å°çµæœ")
        
        # é¡¯ç¤ºæ‘˜è¦ä¿¡æ¯
        st.subheader("æ‘˜è¦")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric(
                "æ®µè½ç›¸ä¼¼åº¦",
                f"{st.session_state.comparison_report['summary']['paragraph_similarity_percentage']:.2f}%",
                delta=None
            )
        
        with col2:
            st.metric(
                "è¡¨æ ¼ç›¸ä¼¼åº¦",
                f"{st.session_state.comparison_report['summary']['table_similarity_percentage']:.2f}%",
                delta=None
            )
        
        with col3:
            total_items = st.session_state.comparison_report['summary']['total_paragraphs'] + st.session_state.comparison_report['summary']['total_tables']
            similar_items = st.session_state.comparison_report['summary']['similar_paragraphs'] + st.session_state.comparison_report['summary']['similar_tables']
            
            if total_items > 0:
                overall_similarity = similar_items / total_items * 100
            else:
                overall_similarity = 0
            
            st.metric(
                "æ•´é«”ç›¸ä¼¼åº¦",
                f"{overall_similarity:.2f}%",
                delta=None
            )
        
        # é¡¯ç¤ºAIåˆ†æå ±å‘Š
        if st.session_state.ai_analysis:
            with st.expander("AIåˆ†æå ±å‘Š", expanded=True):
                st.markdown('<div class="ai-analysis">', unsafe_allow_html=True)
                st.markdown(st.session_state.ai_analysis)
                st.markdown('</div>', unsafe_allow_html=True)
        
        # é¡¯ç¤ºè©³ç´°çµ±è¨ˆä¿¡æ¯
        with st.expander("è©³ç´°çµ±è¨ˆä¿¡æ¯"):
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown("**æ®µè½çµ±è¨ˆ**")
                st.markdown(f"ç¸½æ®µè½æ•¸: {st.session_state.comparison_report['summary']['total_paragraphs']}")
                st.markdown(f"ç›¸ä¼¼æ®µè½æ•¸: {st.session_state.comparison_report['summary']['similar_paragraphs']}")
                st.markdown(f"ä¸åŒæ®µè½æ•¸: {st.session_state.comparison_report['summary']['different_paragraphs']}")
            
            with col2:
                st.markdown("**è¡¨æ ¼çµ±è¨ˆ**")
                st.markdown(f"ç¸½è¡¨æ ¼æ•¸: {st.session_state.comparison_report['summary']['total_tables']}")
                st.markdown(f"ç›¸ä¼¼è¡¨æ ¼æ•¸: {st.session_state.comparison_report['summary']['similar_tables']}")
                st.markdown(f"ä¸åŒè¡¨æ ¼æ•¸: {st.session_state.comparison_report['summary']['different_tables']}")
        
        # å‰µå»ºæ¨™ç±¤é 
        tab1, tab2 = st.tabs(["æ®µè½æ¯”å°çµæœ", "è¡¨æ ¼æ¯”å°çµæœ"])
        
        # æ®µè½æ¯”å°çµæœæ¨™ç±¤é 
        with tab1:
            # é¡¯ç¤ºæ®µè½æ¯”å°çµæœ
            st.subheader("æ®µè½æ¯”å°çµæœ")
            
            # éæ¿¾çµæœ
            if st.session_state.show_all_content:
                paragraph_details = st.session_state.comparison_report["paragraph_details"]
            else:
                paragraph_details = [detail for detail in st.session_state.comparison_report["paragraph_details"] if not detail["is_similar"]]
            
            # æ’åºçµæœï¼Œå°‡ç›¸ä¼¼åº¦æœ€ä½çš„æ”¾åœ¨å‰é¢
            paragraph_details.sort(key=lambda x: x["exact_similarity"])
            
            # é¡¯ç¤ºæ®µè½æ¯”å°çµæœ
            for i, detail in enumerate(paragraph_details):
                with st.container():
                    col1, col2 = st.columns([1, 3])
                    
                    with col1:
                        st.markdown(f"**æ®µè½ {detail['original_index'] + 1}**")
                        st.markdown(f"ç›¸ä¼¼åº¦: {detail['exact_similarity']:.2f}")
                        
                        # å¦‚æœæœ‰èªç¾©ç›¸ä¼¼åº¦ï¼Œé¡¯ç¤º
                        if "semantic_similarity" in detail:
                            st.markdown(f"èªç¾©ç›¸ä¼¼åº¦: {detail['semantic_similarity']:.2f}")
                        
                        # å¦‚æœæœ‰ä¸Šä¸‹æ–‡ç›¸ä¼¼åº¦ï¼Œé¡¯ç¤º
                        if "context_similarity" in detail:
                            st.markdown(f"ä¸Šä¸‹æ–‡ç›¸ä¼¼åº¦: {detail['context_similarity']:.2f}")
                        
                        st.markdown(f"é ç¢¼: {detail['matched_page']}")
                    
                    with col2:
                        # é¡¯ç¤ºåŸå§‹æ–‡æœ¬å’ŒåŒ¹é…æ–‡æœ¬
                        st.markdown("**åŸå§‹æ–‡æœ¬:**")
                        st.markdown(detail["original_text"])
                        
                        st.markdown("**ç¾ç·¨å¾Œæ–‡æœ¬:**")
                        if detail["matched_text"]:
                            st.markdown(detail["matched_text"])
                        else:
                            st.markdown("æœªæ‰¾åˆ°åŒ¹é…æ–‡æœ¬")
                        
                        # é¡¯ç¤ºå·®ç•°
                        if detail["diff_html"]:
                            st.markdown("**å·®ç•°:**")
                            st.markdown(detail["diff_html"], unsafe_allow_html=True)
                    
                    # é¡¯ç¤ºPDFé é¢é è¦½
                    if detail["matched_page"] != "æœªæ‰¾åˆ°":
                        try:
                            page_num = int(detail["matched_page"])
                            
                            # æª¢æŸ¥æ˜¯å¦æœ‰æ¨™è¨˜å¾Œçš„åœ–åƒ
                            key = f"{page_num}_{detail['original_index']}"
                            if key in st.session_state.highlighted_images:
                                st.image(
                                    st.session_state.highlighted_images[key],
                                    caption=f"é é¢ {page_num} (å·²æ¨™è¨˜å·®ç•°)",
                                    use_column_width=True
                                )
                            # å¦å‰‡é¡¯ç¤ºåŸå§‹é é¢
                            elif page_num in st.session_state.pdf_page_images:
                                st.image(
                                    st.session_state.pdf_page_images[page_num],
                                    caption=f"é é¢ {page_num}",
                                    use_column_width=True
                                )
                        except:
                            pass
                
                st.markdown("---")
        
        # è¡¨æ ¼æ¯”å°çµæœæ¨™ç±¤é 
        with tab2:
            # é¡¯ç¤ºè¡¨æ ¼æ¯”å°çµæœ
            st.subheader("è¡¨æ ¼æ¯”å°çµæœ")
            
            # éæ¿¾çµæœ
            if "table_details" in st.session_state.comparison_report:
                if st.session_state.show_all_content:
                    table_details = st.session_state.comparison_report["table_details"]
                else:
                    table_details = [detail for detail in st.session_state.comparison_report["table_details"] if not detail["is_similar"]]
                
                # æ’åºçµæœï¼Œå°‡ç›¸ä¼¼åº¦æœ€ä½çš„æ”¾åœ¨å‰é¢
                table_details.sort(key=lambda x: x["similarity"])
                
                # é¡¯ç¤ºè¡¨æ ¼æ¯”å°çµæœ
                for i, detail in enumerate(table_details):
                    with st.container():
                        col1, col2 = st.columns([1, 3])
                        
                        with col1:
                            st.markdown(f"**è¡¨æ ¼ {detail['original_index'] + 1}**")
                            st.markdown(f"ç›¸ä¼¼åº¦: {detail['similarity']:.2f}")
                            st.markdown(f"é ç¢¼: {detail['matched_page']}")
                        try:    
                            with col2:
                                # é¡¯ç¤ºåŸå§‹è¡¨æ ¼å’ŒåŒ¹é…è¡¨æ ¼
                                st.markdown("**åŸå§‹è¡¨æ ¼:**")
                                if detail["original_table"]:
                                    df1 = pd.DataFrame(detail["original_table"])
                                    st.dataframe(df1)
                                else:
                                    st.markdown("ç„¡è¡¨æ ¼æ•¸æ“š")
                                
                                st.markdown("**ç¾ç·¨å¾Œè¡¨æ ¼:**")
                                if detail["matched_table"]:
                                    df2 = pd.DataFrame(detail["matched_table"])
                                    st.dataframe(df2)
                                else:
                                    st.warning(f"é ç¢¼ {detail['matched_page']} è¶…å‡ºç¯„åœ")
                        except Exception as e:
                                st.error(f"ç„¡æ³•é¡¯ç¤ºè¡¨æ ¼: {e}")
                        
                        # é¡¯ç¤ºPDFé é¢é è¦½
                        if detail["matched_page"] != "æœªæ‰¾åˆ°":
                            try:
                                page_num = int(detail["matched_page"])
                                
                                # é¡¯ç¤ºåŸå§‹é é¢
                                if page_num in st.session_state.pdf_page_images:
                                    st.image(
                                        st.session_state.pdf_page_images[page_num],
                                        caption=f"é é¢ {page_num}",
                                        use_column_width=True
                                    )
                            except:
                                pass
                    
                    st.markdown("---")
            else:
                st.warning("æœªæ¯”å°åˆ°æœ‰æ•ˆè¡¨æ ¼ï¼Œè«‹æª¢æŸ¥æ–‡ä»¶å…§å®¹æ˜¯å¦åŒ…å«è¡¨æ ¼ã€‚")
    else:
        if not st.session_state.processing_complete:
            st.info("è«‹ä¸Šå‚³æ–‡ä»¶ä¸¦é»æ“Šã€Œé–‹å§‹æ¯”å°ã€æŒ‰éˆ•ã€‚")
        else:
            st.warning("æœªæ¯”å°åˆ°æœ‰æ•ˆæ®µè½ï¼Œè«‹æª¢æŸ¥æ–‡ä»¶å…§å®¹æ˜¯å¦æ­£ç¢ºã€‚")

# ä¸»å‡½æ•¸
def main():
    # åŠ è¼‰CSS
    load_css()
    
    # åˆå§‹åŒ–æœƒè©±ç‹€æ…‹
    init_session_state()
    
    # å´é‚Šæ¬„è¨­ç½®
    sidebar_settings()
    
    # æ–‡ä»¶ä¸Šå‚³å€åŸŸ
    file_upload_section()
    
    # é¡¯ç¤ºæ¯”å°çµæœ
    display_comparison_results()
    
    # æ¸…ç†è‡¨æ™‚æ–‡ä»¶
    def cleanup():
        if 'temp_dir' in st.session_state and os.path.exists(st.session_state.temp_dir):
            try:
                shutil.rmtree(st.session_state.temp_dir)
            except:
                pass
    
    # è¨»å†Šæ¸…ç†å‡½æ•¸
    import atexit
    atexit.register(cleanup)

if __name__ == "__main__":
    main()
