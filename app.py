import streamlit as st
import os
import tempfile
import docx
import re
from comparison_algorithm import semantic_matching
import difflib
import pandas as pd
import numpy as np
import base64
from io import BytesIO
import json
import shutil
import sys
from pathlib import Path
import fitz  # PyMuPDF

# æª¢æŸ¥sentence-transformersæ˜¯å¦å¯ç”¨
try:
    from sentence_transformers import SentenceTransformer
    SENTENCE_TRANSFORMERS_AVAILABLE = True
except ImportError:
    SENTENCE_TRANSFORMERS_AVAILABLE = False

# æª¢æŸ¥easyocræ˜¯å¦å¯ç”¨
try:
    import easyocr
    EASYOCR_AVAILABLE = True
except ImportError:
    EASYOCR_AVAILABLE = False

# æª¢æŸ¥tabulaæ˜¯å¦å¯ç”¨
try:
    import tabula
    TABULA_AVAILABLE = True
except ImportError:
    TABULA_AVAILABLE = False

# æª¢æŸ¥pdfplumberæ˜¯å¦å¯ç”¨
try:
    import pdfplumber
    PDFPLUMBER_AVAILABLE = True
except ImportError:
    PDFPLUMBER_AVAILABLE = False

# æª¢æŸ¥pytesseractæ˜¯å¦å¯ç”¨
try:
    import pytesseract
    PYTESSERACT_AVAILABLE = True
except ImportError:
    PYTESSERACT_AVAILABLE = False

# æª¢æŸ¥qwen_ocræ¨¡çµ„æ˜¯å¦å¯ç”¨
try:
    from qwen_ocr import QwenOCR
    QWEN_OCR_AVAILABLE = True
except ImportError:
    QWEN_OCR_AVAILABLE = False

# è¨­ç½®é é¢é…ç½®
st.set_page_config(
    page_title="æœŸåˆŠæ¯”å°ç³»çµ±",
    page_icon="ğŸ“Š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# è‡ªå®šç¾©CSSæ¨£å¼
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        color: #1E88E5;
        text-align: center;
        margin-bottom: 1rem;
    }
    .sub-header {
        font-size: 1.5rem;
        color: #0D47A1;
        margin-top: 2rem;
        margin-bottom: 1rem;
    }
    .highlight-diff {
        background-color: #FFECB3;
        padding: 2px;
        border-radius: 3px;
    }
    .diff-added {
        color: #000000;
        background-color: #C8E6C9;
        padding: 2px;
        border-radius: 3px;
    }
    .diff-removed {
        color: #000000;
        background-color: #FFCDD2;
        padding: 2px;
        border-radius: 3px;
    }
    .result-container {
        border: 1px solid #E0E0E0;
        border-radius: 5px;
        padding: 15px;
        margin-bottom: 15px;
    }
    .similarity-high {
        color: #2E7D32;
        font-weight: bold;
    }
    .similarity-medium {
        color: #F57F17;
        font-weight: bold;
    }
    .similarity-low {
        color: #C62828;
        font-weight: bold;
    }
    .table-warning {
        background-color: #FFF3E0;
        padding: 10px;
        border-left: 4px solid #FF9800;
        margin-bottom: 10px;
    }
    .file-uploader-container {
        border: 1px dashed #BDBDBD;
        border-radius: 5px;
        padding: 10px;
        margin-bottom: 10px;
    }
    .multi-file-uploader {
        margin-bottom: 20px;
    }
    .chapter-selector {
        margin-top: 10px;
        margin-bottom: 20px;
    }
    .diff-char-removed {
        color: #000000;
        background-color: #FFCDD2;
        font-weight: bold;
        padding: 1px;
        border-radius: 2px;
    }
    .diff-char-added {
        color: #000000;
        background-color: #C8E6C9;
        font-weight: bold;
        padding: 1px;
        border-radius: 2px;
    }
    .diff-section {
        margin-top: 10px;
        margin-bottom: 10px;
        padding: 10px;
        border: 1px solid #E0E0E0;
        border-radius: 5px;
    }
    .diff-navigation {
        margin-top: 10px;
        margin-bottom: 10px;
        text-align: center;
    }
    .diff-count {
        font-weight: bold;
        margin-left: 10px;
        margin-right: 10px;
    }
    .api-key-input {
        margin-top: 10px;
        margin-bottom: 10px;
    }
    .success-message {
        color: #2E7D32;
        background-color: #E8F5E9;
        padding: 10px;
        border-radius: 5px;
        margin-top: 10px;
        margin-bottom: 10px;
    }
    .error-message {
        color: #C62828;
        background-color: #FFEBEE;
        padding: 10px;
        border-radius: 5px;
        margin-top: 10px;
        margin-bottom: 10px;
    }
    .ai-model-section {
        background-color: #E3F2FD;
        padding: 10px;
        border-radius: 5px;
        margin-top: 10px;
        margin-bottom: 10px;
    }
    /* é©æ‡‰æ·±è‰²ä¸»é¡Œçš„æ¨£å¼ */
    @media (prefers-color-scheme: dark) {
        .stMarkdown p, .stMarkdown h1, .stMarkdown h2, .stMarkdown h3, .stMarkdown h4, .stMarkdown h5, .stMarkdown h6, .stMarkdown li {
            color: white !important;
        }
        .stText {
            color: white !important;
        }
        .stTextInput > div > div > input {
            color: white !important;
        }
        .stSelectbox > div > div > div {
            color: white !important;
        }
        .stSlider > div > div > div {
            color: white !important;
        }
        .stCheckbox > div > div > label {
            color: white !important;
        }
        .stExpander > div > div > div > div > p {
            color: white !important;
        }
        .stExpander > div > div > div > div > div > p {
            color: white !important;
        }
    }
</style>
""", unsafe_allow_html=True)

# é¡¯ç¤ºæ¨™é¡Œ
st.markdown('<h1 class="main-header">æœŸåˆŠæ¯”å°ç³»çµ±</h1>', unsafe_allow_html=True)
st.markdown('æœ¬ç³»çµ±ç”¨æ–¼æ¯”å°åŸå§‹Wordæ–‡ä»¶èˆ‡ç¾ç·¨å¾ŒPDFæ–‡ä»¶çš„å…§å®¹å·®ç•°ï¼Œå¹«åŠ©æ ¡å°äººå“¡å¿«é€Ÿæ‰¾å‡ºä¸ä¸€è‡´ä¹‹è™•ã€‚')

# --- SessionState é è¨­å€¼èˆ‡åˆå§‹åŒ– ------------------------------------
default_states = {
    "comparison_mode": "hybrid",
    "similarity_threshold": 0.6,
    "use_ocr": False,
    "ocr_engine": "qwen_builtin",
    "ocr_api_key": "",
    "use_ai": False,
    "ai_provider": "deepseek_builtin",
    "ai_api_key": "",
    "ignore_whitespace": True,
    "ignore_punctuation": True,
    "ignore_case": True,
    "ignore_linebreaks": True,
}
for k, v in default_states.items():
    if k not in st.session_state:
        st.session_state[k] = v

# Sidebar è¨­å®šï¼ˆç¹é«”ä¸­æ–‡ä»‹é¢ï¼‰
with st.sidebar:
    st.header("âš™ï¸ æ¯”å°è¨­å®š")

    mode_labels = {
        "ç²¾ç¢ºæ¯”å°ï¼ˆExactï¼‰": "exact",
        "èªæ„æ¯”å°ï¼ˆSemanticï¼‰": "semantic",
        "æ··åˆæ¯”å°ï¼ˆHybridï¼‰": "hybrid",
        "AI æ¯”å°": "ai",
    }
    # å–å¾—ç•¶å‰æ¨¡å¼å°æ‡‰çš„ä¸­æ–‡æ¨™ç±¤
    current_mode_label = next(k for k,v in mode_labels.items() if v == st.session_state.comparison_mode)
    selected_label = st.selectbox("æ¯”å°æ¨¡å¼", list(mode_labels.keys()),
                                  index=list(mode_labels.keys()).index(current_mode_label))
    st.session_state.comparison_mode = mode_labels[selected_label]

    st.session_state.similarity_threshold = st.slider(
        "ç›¸ä¼¼åº¦é–¾å€¼",
        0.0, 1.0,
        st.session_state.similarity_threshold,
        0.05
    )

    st.divider()
    st.subheader("ğŸ” OCR è¨­å®š")

    st.session_state.use_ocr = st.checkbox("å•Ÿç”¨ OCR", value=st.session_state.use_ocr)

    if st.session_state.use_ocr:
        ocr_choice = st.radio(
            "OCR å¼•æ“",
            ["Qwenï¼ˆå…§å»ºï¼‰", "EasyOCR", "Tesseract", "è‡ªå®šç¾© OCR API"],
            index=["qwen_builtin", "easyocr", "tesseract", "ocr_custom"]
            .index(st.session_state.ocr_engine)
        )
        st.session_state.ocr_engine = {
            "Qwenï¼ˆå…§å»ºï¼‰": "qwen_builtin",
            "EasyOCR": "easyocr",
            "Tesseract": "tesseract",
            "è‡ªå®šç¾© OCR API": "ocr_custom",
        }[ocr_choice]

        if st.session_state.ocr_engine == "ocr_custom":
            st.session_state.ocr_api_key = st.text_input(
                "ğŸ”‘ è«‹è¼¸å…¥ OCR API é‡‘é‘°",
                type="password",
                value=st.session_state.ocr_api_key
            )

    st.divider()
    st.subheader("ğŸ¤– ç”Ÿæˆå¼ AI è¨­å®š")

    st.session_state.use_ai = st.checkbox("ä½¿ç”¨ç”Ÿæˆå¼ AI", value=st.session_state.use_ai)

    if st.session_state.use_ai:
        ai_choice = st.selectbox(
            "AI ä¾†æº / æ¨¡å‹",
            ["DeepSeekï¼ˆå…§å»ºï¼‰", "Qwen2.5ï¼ˆå…§å»ºï¼‰", "Mistralâ€‘7Bï¼ˆå…§å»ºï¼‰",
             "OpenAI", "Anthropic", "Qwen (API)", "è‡ªå®šç¾© AI API"],
            index=[
                "deepseek_builtin","qwen_builtin","mistral_builtin",
                "openai","anthropic","qwen_api","ai_custom"
            ].index(st.session_state.ai_provider)
        )
        st.session_state.ai_provider = {
            "DeepSeekï¼ˆå…§å»ºï¼‰": "deepseek_builtin",
            "Qwen2.5ï¼ˆå…§å»ºï¼‰": "qwen_builtin",
            "Mistralâ€‘7Bï¼ˆå…§å»ºï¼‰": "mistral_builtin",
            "OpenAI": "openai",
            "Anthropic": "anthropic",
            "Qwen (API)": "qwen_api",
            "è‡ªå®šç¾© AI API": "ai_custom",
        }[ai_choice]

        if st.session_state.ai_provider in {"openai","anthropic","qwen_api","ai_custom"}:
            st.session_state.ai_api_key = st.text_input(
                "ğŸ”‘ ç”Ÿæˆå¼ AI API é‡‘é‘°",
                type="password",
                value=st.session_state.ai_api_key
            )

    st.divider()
    st.subheader("ğŸ§¹ å¿½ç•¥è¦å‰‡")
    st.session_state.ignore_whitespace = st.checkbox("å¿½ç•¥ç©ºæ ¼", value=st.session_state.ignore_whitespace)
    st.session_state.ignore_punctuation = st.checkbox("å¿½ç•¥æ¨™é»ç¬¦è™Ÿ", value=st.session_state.ignore_punctuation)
    st.session_state.ignore_case = st.checkbox("å¿½ç•¥å¤§å°å¯«", value=st.session_state.ignore_case)
    st.session_state.ignore_linebreaks = st.checkbox("å¿½ç•¥æ–·è¡Œ", value=st.session_state.ignore_linebreaks)

    st.divider()
    st.subheader("â„¹ï¸ ç³»çµ±è³‡è¨Š")
    st.info("æœ¬ç³»çµ±ç”¨æ–¼æ¯”å°åŸå§‹ Word èˆ‡ PDF å…§å®¹å·®ç•°ï¼Œå”åŠ©æ ¡å°ã€‚")

# æ–‡ä»¶ä¸Šå‚³å€åŸŸ
st.header("ğŸ“ æ–‡ä»¶ä¸Šå‚³")

col1, col2 = st.columns(2)
with col1:
    st.subheader("åŸå§‹Wordæ–‡ä»¶")
    word_file = st.file_uploader("ä¸Šå‚³åŸå§‹ Word æ–‡ç¨¿", type=["docx"])
    
    if word_file:
        st.success(f"å·²ä¸Šå‚³: {word_file.name}")
        
with col2:
    st.subheader("ç¾ç·¨å¾ŒPDFæ–‡ä»¶")
    pdf_file = st.file_uploader("ä¸Šå‚³ç¾ç·¨å¾Œ PDF æ–‡ä»¶", type=["pdf"])
    
    if pdf_file:
        st.success(f"å·²ä¸Šå‚³: {pdf_file.name}")

# ä½¿ç”¨ç¤ºä¾‹æ–‡ä»¶é¸é …
use_example_files = st.checkbox("ä½¿ç”¨ç¤ºä¾‹æ–‡ä»¶é€²è¡Œæ¼”ç¤º", value=False)

# æ–‡æœ¬æå–å’Œè™•ç†å‡½æ•¸
def extract_text_from_word(word_file):
    """å¾Wordæ–‡ä»¶ä¸­æå–æ–‡æœ¬"""
    doc = docx.Document(word_file)
    
    paragraphs = []
    tables = []
    
    # æå–æ®µè½
    for i, para in enumerate(doc.paragraphs):
        text = para.text.strip()
        if text:
            paragraphs.append({
                "index": i,
                "content": text,
                "type": "paragraph"
            })
    
    # æå–è¡¨æ ¼
    for i, table in enumerate(doc.tables):
        table_data = []
        for row in table.rows:
            row_data = []
            for cell in row.cells:
                row_data.append(cell.text.strip())
            table_data.append(row_data)
        
        if any(any(cell for cell in row) for row in table_data):
            tables.append({
                "index": i,
                "content": table_data,
                "type": "table"
            })
    
    return {
        "paragraphs": paragraphs,
        "tables": tables
    }

def extract_text_from_pdf(pdf_file):
    """å¾PDFæ–‡ä»¶ä¸­æå–æ–‡æœ¬ï¼ˆä½¿ç”¨PyMuPDFï¼‰"""
    doc = fitz.open(pdf_file)
    
    paragraphs = []
    page_texts = []
    
    for page_num in range(len(doc)):
        page = doc[page_num]
        text = page.get_text()
        page_texts.append(text)
        
        # ç°¡å–®åœ°æŒ‰è¡Œåˆ†å‰²æ–‡æœ¬
        lines = text.split('\n')
        for i, line in enumerate(lines):
            line = line.strip()
            if line:
                paragraphs.append({
                    "index": len(paragraphs),
                    "content": line,
                    "type": "paragraph",
                    "page": page_num + 1
                })
    
    return {
        "paragraphs": paragraphs,
        "tables": [],  # ç°¡åŒ–ç‰ˆæœ¬ä¸æå–è¡¨æ ¼
        "page_texts": page_texts
    }

def enhanced_pdf_extraction(word_path, pdf_path):
    """å¢å¼·ç‰ˆçš„æ–‡æª”æå–å‡½æ•¸"""
    # æå–Wordæ–‡æª”å…§å®¹
    if word_path.endswith('.docx'):
        word_data = extract_text_from_word(word_path)
    else:
        # å¦‚æœä¸æ˜¯.docxæ–‡ä»¶ï¼Œå˜—è©¦ä½œç‚ºæ–‡æœ¬æ–‡ä»¶è®€å–
        try:
            with open(word_path, 'r', encoding='utf-8') as f:
                content = f.read()
                paragraphs = []
                for i, para in enumerate(content.split('\n\n')):
                    para = para.strip()
                    if para:
                        paragraphs.append({
                            "index": i,
                            "content": para,
                            "type": "paragraph"
                        })
                word_data = {
                    "paragraphs": paragraphs,
                    "tables": []
                }
        except Exception as e:
            st.error(f"ç„¡æ³•è®€å–Wordæ–‡ä»¶: {e}")
            return None, None
    
    # æå–PDFæ–‡æª”å…§å®¹
    if pdf_path.endswith('.pdf'):
        pdf_data = extract_text_from_pdf(pdf_path)
    else:
        # å¦‚æœä¸æ˜¯.pdfæ–‡ä»¶ï¼Œå˜—è©¦ä½œç‚ºæ–‡æœ¬æ–‡ä»¶è®€å–
        try:
            with open(pdf_path, 'r', encoding='utf-8') as f:
                content = f.read()
                paragraphs = []
                for i, para in enumerate(content.split('\n\n')):
                    para = para.strip()
                    if para:
                        paragraphs.append({
                            "index": i,
                            "content": para,
                            "type": "paragraph",
                            "page": 1  # å‡è¨­åªæœ‰ä¸€é 
                        })
                pdf_data = {
                    "paragraphs": paragraphs,
                    "tables": [],
                    "page_texts": [content]
                }
        except Exception as e:
            st.error(f"ç„¡æ³•è®€å–PDFæ–‡ä»¶: {e}")
            return None, None
    
    return word_data, pdf_data

def improved_matching_algorithm(word_data, pdf_data, similarity_threshold=0.6):
    """æ”¹é€²çš„åŒ¹é…ç®—æ³•"""
    matches = []
    
    # å°æ¯å€‹Wordæ®µè½ï¼Œæ‰¾åˆ°æœ€ç›¸ä¼¼çš„PDFæ®µè½
    for word_para in word_data["paragraphs"]:
        best_match = None
        best_similarity = 0
        
        for pdf_para in pdf_data["paragraphs"]:
            # ä½¿ç”¨difflibè¨ˆç®—ç›¸ä¼¼åº¦
            similarity = difflib.SequenceMatcher(None, word_para["content"], pdf_para["content"]).ratio()
            
            if similarity > best_similarity:
                best_similarity = similarity
                best_match = pdf_para
        
        # å¦‚æœæ‰¾åˆ°è¶³å¤ ç›¸ä¼¼çš„åŒ¹é…
        if best_match and best_similarity >= similarity_threshold:
            matches.append({
                "doc1_index": word_para["index"],
                "doc1_text": word_para["content"],
                "doc2_index": best_match["index"],
                "doc2_text": best_match["content"],
                "similarity": best_similarity,
                "page_number": best_match.get("page", None)
            })
    
    return matches

# æ¯”å°ç®—æ³•
def compare_documents(doc1_data, doc2_data, ignore_options=None, comparison_mode="hybrid",
                      similarity_threshold=0.6, ai_instance=None):
    """æ¯”å°å…©å€‹æ–‡æª”çš„å…§å®¹ï¼Œæ”¯æ´ç²¾ç¢º / èªæ„ / æ··åˆ / AI æ¨¡å¼ï¼Œä¸¦å…§å»ºé€²åº¦æ¢é¡¯ç¤º"""
    if ignore_options is None:
        ignore_options = {
            "ignore_whitespace": True,
            "ignore_punctuation": True,
            "ignore_case": True,
            "ignore_linebreaks": True
        }

    # é è™•ç†å‡½å¼
    def preprocess_text(text):
        if ignore_options.get("ignore_whitespace", False):
            text = re.sub(r'\s+', ' ', text)
        if ignore_options.get("ignore_punctuation", False):
            text = re.sub(r'[^\w\s]', '', text)
        if ignore_options.get("ignore_case", False):
            text = text.lower()
        if ignore_options.get("ignore_linebreaks", False):
            text = text.replace('\n', ' ')
        return text.strip()

    # æ•´ç†æ®µè½
    for para in doc1_data["paragraphs"]:
        para["processed"] = preprocess_text(para["content"])
    for para in doc2_data["paragraphs"]:
        para["processed"] = preprocess_text(para["content"])

    total = len(doc1_data["paragraphs"])
    progress_bar = st.progress(0.0)

    matches = []
    for idx, src in enumerate(doc1_data["paragraphs"]):
        best_match = None
        best_sim = 0.0

        for tgt in doc2_data["paragraphs"]:
            if comparison_mode == "exact":
                sim = difflib.SequenceMatcher(None, src["processed"], tgt["processed"]).ratio()
            elif comparison_mode == "semantic":
                sim = semantic_matching(src["processed"], tgt["processed"])
            elif comparison_mode == "hybrid":
                exact_sim = difflib.SequenceMatcher(None, src["processed"], tgt["processed"]).ratio()
                if exact_sim >= similarity_threshold:
                    sim = exact_sim
                else:
                    sem_sim = semantic_matching(src["processed"], tgt["processed"])
                    sim = max(exact_sim, sem_sim)
            elif comparison_mode == "ai" and ai_instance:
                sim, _ = ai_instance.semantic_comparison(src["content"], tgt["content"])
            else:
                sim = 0.0

            if sim > best_sim:
                best_sim = sim
                best_match = tgt

        if best_match and best_sim >= similarity_threshold:
            # ç”Ÿæˆå·®ç•° html
            d = difflib.Differ()
            diff = list(d.compare(src["content"], best_match["content"]))
            diff_html = []
            for i, s in enumerate(diff):
                if s.startswith('  '):
                    diff_html.append(s[2:])
                elif s.startswith('- '):
                    diff_html.append(f'<span class="diff-removed">{s[2:]}</span>')
                elif s.startswith('+ '):
                    diff_html.append(f'<span class="diff-added">{s[2:]}</span>')
            matches.append({
                "doc1_index": src["index"],
                "doc1_text": src["content"],
                "doc2_index": best_match["index"],
                "doc2_text": best_match["content"],
                "similarity": best_sim,
                "page_number": best_match.get("page", None),
                "diff_html": ''.join(diff_html)
            })

        progress_bar.progress((idx + 1) / total)

    return matches

import streamlit as st
import os
import tempfile
import docx
import re
import difflib
import pandas as pd
import numpy as np
import base64
from io import BytesIO
import json
import shutil
import sys
from pathlib import Path
import fitz  # PyMuPDF

# æª¢æŸ¥sentence-transformersæ˜¯å¦å¯ç”¨
try:
    from sentence_transformers import SentenceTransformer
    SENTENCE_TRANSFORMERS_AVAILABLE = True
except ImportError:
    SENTENCE_TRANSFORMERS_AVAILABLE = False

# æª¢æŸ¥easyocræ˜¯å¦å¯ç”¨
try:
    import easyocr
    EASYOCR_AVAILABLE = True
except ImportError:
    EASYOCR_AVAILABLE = False

# æª¢æŸ¥tabulaæ˜¯å¦å¯ç”¨
try:
    import tabula
    TABULA_AVAILABLE = True
except ImportError:
    TABULA_AVAILABLE = False

# æª¢æŸ¥pdfplumberæ˜¯å¦å¯ç”¨
try:
    import pdfplumber
    PDFPLUMBER_AVAILABLE = True
except ImportError:
    PDFPLUMBER_AVAILABLE = False

# æª¢æŸ¥pytesseractæ˜¯å¦å¯ç”¨
try:
    import pytesseract
    PYTESSERACT_AVAILABLE = True
except ImportError:
    PYTESSERACT_AVAILABLE = False

# æª¢æŸ¥qwen_ocræ¨¡çµ„æ˜¯å¦å¯ç”¨
try:
    from qwen_ocr import QwenOCR
    QWEN_OCR_AVAILABLE = True
except ImportError:
    QWEN_OCR_AVAILABLE = False

# è¨­ç½®é é¢é…ç½®
st.set_page_config(
    page_title="æœŸåˆŠæ¯”å°ç³»çµ±",
    page_icon="ğŸ“Š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# è‡ªå®šç¾©CSSæ¨£å¼
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        color: #1E88E5;
        text-align: center;
        margin-bottom: 1rem;
    }
    .sub-header {
        font-size: 1.5rem;
        color: #0D47A1;
        margin-top: 2rem;
        margin-bottom: 1rem;
    }
    .highlight-diff {
        background-color: #FFECB3;
        padding: 2px;
        border-radius: 3px;
    }
    .diff-added {
        color: #000000;
        background-color: #C8E6C9;
        padding: 2px;
        border-radius: 3px;
    }
    .diff-removed {
        color: #000000;
        background-color: #FFCDD2;
        padding: 2px;
        border-radius: 3px;
    }
    .result-container {
        border: 1px solid #E0E0E0;
        border-radius: 5px;
        padding: 15px;
        margin-bottom: 15px;
    }
    .similarity-high {
        color: #2E7D32;
        font-weight: bold;
    }
    .similarity-medium {
        color: #F57F17;
        font-weight: bold;
    }
    .similarity-low {
        color: #C62828;
        font-weight: bold;
    }
    .table-warning {
        background-color: #FFF3E0;
        padding: 10px;
        border-left: 4px solid #FF9800;
        margin-bottom: 10px;
    }
    .file-uploader-container {
        border: 1px dashed #BDBDBD;
        border-radius: 5px;
        padding: 10px;
        margin-bottom: 10px;
    }
    .multi-file-uploader {
        margin-bottom: 20px;
    }
    .chapter-selector {
        margin-top: 10px;
        margin-bottom: 20px;
    }
    .diff-char-removed {
        color: #000000;
        background-color: #FFCDD2;
        font-weight: bold;
        padding: 1px;
        border-radius: 2px;
    }
    .diff-char-added {
        color: #000000;
        background-color: #C8E6C9;
        font-weight: bold;
        padding: 1px;
        border-radius: 2px;
    }
    .diff-section {
        margin-top: 10px;
        margin-bottom: 10px;
        padding: 10px;
        border: 1px solid #E0E0E0;
        border-radius: 5px;
    }
    .diff-navigation {
        margin-top: 10px;
        margin-bottom: 10px;
        text-align: center;
    }
    .diff-count {
        font-weight: bold;
        margin-left: 10px;
        margin-right: 10px;
    }
    .api-key-input {
        margin-top: 10px;
        margin-bottom: 10px;
    }
    .success-message {
        color: #2E7D32;
        background-color: #E8F5E9;
        padding: 10px;
        border-radius: 5px;
        margin-top: 10px;
        margin-bottom: 10px;
    }
    .error-message {
        color: #C62828;
        background-color: #FFEBEE;
        padding: 10px;
        border-radius: 5px;
        margin-top: 10px;
        margin-bottom: 10px;
    }
    .ai-model-section {
        background-color: #E3F2FD;
        padding: 10px;
        border-radius: 5px;
        margin-top: 10px;
        margin-bottom: 10px;
    }
    /* é©æ‡‰æ·±è‰²ä¸»é¡Œçš„æ¨£å¼ */
    @media (prefers-color-scheme: dark) {
        .stMarkdown p, .stMarkdown h1, .stMarkdown h2, .stMarkdown h3, .stMarkdown h4, .stMarkdown h5, .stMarkdown h6, .stMarkdown li {
            color: white !important;
        }
        .stText {
            color: white !important;
        }
        .stTextInput > div > div > input {
            color: white !important;
        }
        .stSelectbox > div > div > div {
            color: white !important;
        }
        .stSlider > div > div > div {
            color: white !important;
        }
        .stCheckbox > div > div > label {
            color: white !important;
        }
        .stExpander > div > div > div > div > p {
            color: white !important;
        }
        .stExpander > div > div > div > div > div > p {
            color: white !important;
        }
    }
</style>
""", unsafe_allow_html=True)

# é¡¯ç¤ºæ¨™é¡Œ
st.markdown('<h1 class="main-header">æœŸåˆŠæ¯”å°ç³»çµ±</h1>', unsafe_allow_html=True)
st.markdown('æœ¬ç³»çµ±ç”¨æ–¼æ¯”å°åŸå§‹Wordæ–‡ä»¶èˆ‡ç¾ç·¨å¾ŒPDFæ–‡ä»¶çš„å…§å®¹å·®ç•°ï¼Œå¹«åŠ©æ ¡å°äººå“¡å¿«é€Ÿæ‰¾å‡ºä¸ä¸€è‡´ä¹‹è™•ã€‚')

# --- SessionState é è¨­å€¼èˆ‡åˆå§‹åŒ– ------------------------------------
default_states = {
    "comparison_mode": "hybrid",
    "similarity_threshold": 0.6,
    "use_ocr": False,
    "ocr_engine": "qwen_builtin",
    "ocr_api_key": "",
    "use_ai": False,
    "ai_provider": "deepseek_builtin",
    "ai_api_key": "",
    "ignore_whitespace": True,
    "ignore_punctuation": True,
    "ignore_case": True,
    "ignore_linebreaks": True,
}
for k, v in default_states.items():
    if k not in st.session_state:
        st.session_state[k] = v

# Sidebar è¨­å®šï¼ˆç¹é«”ä¸­æ–‡ä»‹é¢ï¼‰
with st.sidebar:
    st.header("âš™ï¸ æ¯”å°è¨­å®š")

    mode_labels = {
        "ç²¾ç¢ºæ¯”å°ï¼ˆExactï¼‰": "exact",
        "èªæ„æ¯”å°ï¼ˆSemanticï¼‰": "semantic",
        "æ··åˆæ¯”å°ï¼ˆHybridï¼‰": "hybrid",
        "AI æ¯”å°": "ai",
    }
    # å–å¾—ç•¶å‰æ¨¡å¼å°æ‡‰çš„ä¸­æ–‡æ¨™ç±¤
    current_mode_label = next(k for k,v in mode_labels.items() if v == st.session_state.comparison_mode)
    selected_label = st.selectbox("æ¯”å°æ¨¡å¼", list(mode_labels.keys()),
                                  index=list(mode_labels.keys()).index(current_mode_label))
    st.session_state.comparison_mode = mode_labels[selected_label]

    st.session_state.similarity_threshold = st.slider(
        "ç›¸ä¼¼åº¦é–¾å€¼",
        0.0, 1.0,
        st.session_state.similarity_threshold,
        0.05
    )

    st.divider()
    st.subheader("ğŸ” OCR è¨­å®š")

    st.session_state.use_ocr = st.checkbox("å•Ÿç”¨ OCR", value=st.session_state.use_ocr)

    if st.session_state.use_ocr:
        ocr_choice = st.radio(
            "OCR å¼•æ“",
            ["Qwenï¼ˆå…§å»ºï¼‰", "EasyOCR", "Tesseract", "è‡ªå®šç¾© OCR API"],
            index=["qwen_builtin", "easyocr", "tesseract", "ocr_custom"]
            .index(st.session_state.ocr_engine)
        )
        st.session_state.ocr_engine = {
            "Qwenï¼ˆå…§å»ºï¼‰": "qwen_builtin",
            "EasyOCR": "easyocr",
            "Tesseract": "tesseract",
            "è‡ªå®šç¾© OCR API": "ocr_custom",
        }[ocr_choice]

        if st.session_state.ocr_engine == "ocr_custom":
            st.session_state.ocr_api_key = st.text_input(
                "ğŸ”‘ è«‹è¼¸å…¥ OCR API é‡‘é‘°",
                type="password",
                value=st.session_state.ocr_api_key
            )

    st.divider()
    st.subheader("ğŸ¤– ç”Ÿæˆå¼ AI è¨­å®š")

    st.session_state.use_ai = st.checkbox("ä½¿ç”¨ç”Ÿæˆå¼ AI", value=st.session_state.use_ai)

    if st.session_state.use_ai:
        ai_choice = st.selectbox(
            "AI ä¾†æº / æ¨¡å‹",
            ["DeepSeekï¼ˆå…§å»ºï¼‰", "Qwen2.5ï¼ˆå…§å»ºï¼‰", "Mistralâ€‘7Bï¼ˆå…§å»ºï¼‰",
             "OpenAI", "Anthropic", "Qwen (API)", "è‡ªå®šç¾© AI API"],
            index=[
                "deepseek_builtin","qwen_builtin","mistral_builtin",
                "openai","anthropic","qwen_api","ai_custom"
            ].index(st.session_state.ai_provider)
        )
        st.session_state.ai_provider = {
            "DeepSeekï¼ˆå…§å»ºï¼‰": "deepseek_builtin",
            "Qwen2.5ï¼ˆå…§å»ºï¼‰": "qwen_builtin",
            "Mistralâ€‘7Bï¼ˆå…§å»ºï¼‰": "mistral_builtin",
            "OpenAI": "openai",
            "Anthropic": "anthropic",
            "Qwen (API)": "qwen_api",
            "è‡ªå®šç¾© AI API": "ai_custom",
        }[ai_choice]

        if st.session_state.ai_provider in {"openai","anthropic","qwen_api","ai_custom"}:
            st.session_state.ai_api_key = st.text_input(
                "ğŸ”‘ ç”Ÿæˆå¼ AI API é‡‘é‘°",
                type="password",
                value=st.session_state.ai_api_key
            )

    st.divider()
    st.subheader("ğŸ§¹ å¿½ç•¥è¦å‰‡")
    st.session_state.ignore_whitespace = st.checkbox("å¿½ç•¥ç©ºæ ¼", value=st.session_state.ignore_whitespace)
    st.session_state.ignore_punctuation = st.checkbox("å¿½ç•¥æ¨™é»ç¬¦è™Ÿ", value=st.session_state.ignore_punctuation)
    st.session_state.ignore_case = st.checkbox("å¿½ç•¥å¤§å°å¯«", value=st.session_state.ignore_case)
    st.session_state.ignore_linebreaks = st.checkbox("å¿½ç•¥æ–·è¡Œ", value=st.session_state.ignore_linebreaks)

    st.divider()
    st.subheader("â„¹ï¸ ç³»çµ±è³‡è¨Š")
    st.info("æœ¬ç³»çµ±ç”¨æ–¼æ¯”å°åŸå§‹ Word èˆ‡ PDF å…§å®¹å·®ç•°ï¼Œå”åŠ©æ ¡å°ã€‚")

# æ–‡ä»¶ä¸Šå‚³å€åŸŸ
st.header("ğŸ“ æ–‡ä»¶ä¸Šå‚³")

col1, col2 = st.columns(2)
with col1:
    st.subheader("åŸå§‹Wordæ–‡ä»¶")
    word_file = st.file_uploader("ä¸Šå‚³åŸå§‹ Word æ–‡ç¨¿", type=["docx"])
    
    if word_file:
        st.success(f"å·²ä¸Šå‚³: {word_file.name}")
        
with col2:
    st.subheader("ç¾ç·¨å¾ŒPDFæ–‡ä»¶")
    pdf_file = st.file_uploader("ä¸Šå‚³ç¾ç·¨å¾Œ PDF æ–‡ä»¶", type=["pdf"])
    
    if pdf_file:
        st.success(f"å·²ä¸Šå‚³: {pdf_file.name}")

# ä½¿ç”¨ç¤ºä¾‹æ–‡ä»¶é¸é …
use_example_files = st.checkbox("ä½¿ç”¨ç¤ºä¾‹æ–‡ä»¶é€²è¡Œæ¼”ç¤º", value=False)

# æ–‡æœ¬æå–å’Œè™•ç†å‡½æ•¸
def extract_text_from_word(word_file):
    """å¾Wordæ–‡ä»¶ä¸­æå–æ–‡æœ¬"""
    doc = docx.Document(word_file)
    
    paragraphs = []
    tables = []
    
    # æå–æ®µè½
    for i, para in enumerate(doc.paragraphs):
        text = para.text.strip()
        if text:
            paragraphs.append({
                "index": i,
                "content": text,
                "type": "paragraph"
            })
    
    # æå–è¡¨æ ¼
    for i, table in enumerate(doc.tables):
        table_data = []
        for row in table.rows:
            row_data = []
            for cell in row.cells:
                row_data.append(cell.text.strip())
            table_data.append(row_data)
        
        if any(any(cell for cell in row) for row in table_data):
            tables.append({
                "index": i,
                "content": table_data,
                "type": "table"
            })
    
    return {
        "paragraphs": paragraphs,
        "tables": tables
    }

def extract_text_from_pdf(pdf_file):
    """å¾PDFæ–‡ä»¶ä¸­æå–æ–‡æœ¬ï¼ˆä½¿ç”¨PyMuPDFï¼‰"""
    doc = fitz.open(pdf_file)
    
    paragraphs = []
    page_texts = []
    
    for page_num in range(len(doc)):
        page = doc[page_num]
        text = page.get_text()
        page_texts.append(text)
        
        # ç°¡å–®åœ°æŒ‰è¡Œåˆ†å‰²æ–‡æœ¬
        lines = text.split('\n')
        for i, line in enumerate(lines):
            line = line.strip()
            if line:
                paragraphs.append({
                    "index": len(paragraphs),
                    "content": line,
                    "type": "paragraph",
                    "page": page_num + 1
                })
    
    return {
        "paragraphs": paragraphs,
        "tables": [],  # ç°¡åŒ–ç‰ˆæœ¬ä¸æå–è¡¨æ ¼
        "page_texts": page_texts
    }

def enhanced_pdf_extraction(word_path, pdf_path):
    """å¢å¼·ç‰ˆçš„æ–‡æª”æå–å‡½æ•¸"""
    # æå–Wordæ–‡æª”å…§å®¹
    if word_path.endswith('.docx'):
        word_data = extract_text_from_word(word_path)
    else:
        # å¦‚æœä¸æ˜¯.docxæ–‡ä»¶ï¼Œå˜—è©¦ä½œç‚ºæ–‡æœ¬æ–‡ä»¶è®€å–
        try:
            with open(word_path, 'r', encoding='utf-8') as f:
                content = f.read()
                paragraphs = []
                for i, para in enumerate(content.split('\n\n')):
                    para = para.strip()
                    if para:
                        paragraphs.append({
                            "index": i,
                            "content": para,
                            "type": "paragraph"
                        })
                word_data = {
                    "paragraphs": paragraphs,
                    "tables": []
                }
        except Exception as e:
            st.error(f"ç„¡æ³•è®€å–Wordæ–‡ä»¶: {e}")
            return None, None
    
    # æå–PDFæ–‡æª”å…§å®¹
    if pdf_path.endswith('.pdf'):
        pdf_data = extract_text_from_pdf(pdf_path)
    else:
        # å¦‚æœä¸æ˜¯.pdfæ–‡ä»¶ï¼Œå˜—è©¦ä½œç‚ºæ–‡æœ¬æ–‡ä»¶è®€å–
        try:
            with open(pdf_path, 'r', encoding='utf-8') as f:
                content = f.read()
                paragraphs = []
                for i, para in enumerate(content.split('\n\n')):
                    para = para.strip()
                    if para:
                        paragraphs.append({
                            "index": i,
                            "content": para,
                            "type": "paragraph",
                            "page": 1  # å‡è¨­åªæœ‰ä¸€é 
                        })
                pdf_data = {
                    "paragraphs": paragraphs,
                    "tables": [],
                    "page_texts": [content]
                }
        except Exception as e:
            st.error(f"ç„¡æ³•è®€å–PDFæ–‡ä»¶: {e}")
            return None, None
    
    return word_data, pdf_data

def improved_matching_algorithm(word_data, pdf_data, similarity_threshold=0.6):
    """æ”¹é€²çš„åŒ¹é…ç®—æ³•"""
    matches = []
    
    # å°æ¯å€‹Wordæ®µè½ï¼Œæ‰¾åˆ°æœ€ç›¸ä¼¼çš„PDFæ®µè½
    for word_para in word_data["paragraphs"]:
        best_match = None
        best_similarity = 0
        
        for pdf_para in pdf_data["paragraphs"]:
            # ä½¿ç”¨difflibè¨ˆç®—ç›¸ä¼¼åº¦
            similarity = difflib.SequenceMatcher(None, word_para["content"], pdf_para["content"]).ratio()
            
            if similarity > best_similarity:
                best_similarity = similarity
                best_match = pdf_para
        
        # å¦‚æœæ‰¾åˆ°è¶³å¤ ç›¸ä¼¼çš„åŒ¹é…
        if best_match and best_similarity >= similarity_threshold:
            matches.append({
                "doc1_index": word_para["index"],
                "doc1_text": word_para["content"],
                "doc2_index": best_match["index"],
                "doc2_text": best_match["content"],
                "similarity": best_similarity,
                "page_number": best_match.get("page", None)
            })
    
    return matches

# æ¯”å°ç®—æ³•
def compare_documents(doc1_data, doc2_data, ignore_options=None, comparison_mode="hybrid", similarity_threshold=0.6, ai_instance=None):
    """æ¯”å°å…©å€‹æ–‡æª”çš„å…§å®¹"""
    if ignore_options is None:
        ignore_options = {
            "ignore_whitespace": True,
            "ignore_punctuation": True,
            "ignore_case": True,
            "ignore_linebreaks": True
        }
    
    # é è™•ç†æ–‡æœ¬
    def preprocess_text(text):
        if ignore_options.get("ignore_whitespace", False):
            text = re.sub(r'\s+', ' ', text)
        if ignore_options.get("ignore_punctuation", False):
            text = re.sub(r'[^\w\s]', '', text)
        if ignore_options.get("ignore_case", False):
            text = text.lower()
        if ignore_options.get("ignore_linebreaks", False):
            text = text.replace('\n', ' ')
        return text.strip()
    
    # é è™•ç†æ‰€æœ‰æ®µè½
    for para in doc1_data["paragraphs"]:
        para["processed_content"] = preprocess_text(para["content"])
    
    for para in doc2_data["paragraphs"]:
        para["processed_content"] = preprocess_text(para["content"])
    
    # æ ¹æ“šæ¯”å°æ¨¡å¼é¸æ“‡ä¸åŒçš„ç®—æ³•
    if comparison_mode == "exact":
        # ç²¾ç¢ºæ¯”å°
        matches = []
        for doc1_para in doc1_data["paragraphs"]:
            for doc2_para in doc2_data["paragraphs"]:
                if doc1_para["processed_content"] == doc2_para["processed_content"]:
                    matches.append({
                        "doc1_index": doc1_para["index"],
                        "doc1_text": doc1_para["content"],
                        "doc2_index": doc2_para["index"],
                        "doc2_text": doc2_para["content"],
                        "similarity": 1.0,
                        "page_number": doc2_para.get("page", None)
                    })
                    break
        
        # å°æ–¼æ²’æœ‰ç²¾ç¢ºåŒ¹é…çš„æ®µè½ï¼Œä½¿ç”¨æ¨¡ç³ŠåŒ¹é…
        for doc1_para in doc1_data["paragraphs"]:
            if not any(match["doc1_index"] == doc1_para["index"] for match in matches):
                best_match = None
                best_similarity = 0
                
                for doc2_para in doc2_data["paragraphs"]:
                    similarity = difflib.SequenceMatcher(None, doc1_para["processed_content"], doc2_para["processed_content"]).ratio()
                    
                    if similarity > best_similarity and similarity >= similarity_threshold:
                        best_similarity = similarity
                        best_match = doc2_para
                
                if best_match:
                    matches.append({
                        "doc1_index": doc1_para["index"],
                        "doc1_text": doc1_para["content"],
                        "doc2_index": best_match["index"],
                        "doc2_text": best_match["content"],
                        "similarity": best_similarity,
                        "page_number": best_match.get("page", None)
                    })
    
    elif comparison_mode == "semantic":
        # èªæ„æ¯”å°
        if SENTENCE_TRANSFORMERS_AVAILABLE and ai_instance and ai_instance.is_available():
            # ä½¿ç”¨AIé€²è¡Œèªæ„æ¯”å°
            matches = ai_instance.match_paragraphs(doc1_data["paragraphs"], doc2_data["paragraphs"])
        else:
            # å¦‚æœAIä¸å¯ç”¨ï¼Œé€€å›åˆ°æ¨¡ç³Šæ¯”å°
            matches = improved_matching_algorithm(doc1_data, doc2_data, similarity_threshold)
    
    elif comparison_mode == "hybrid" or comparison_mode == "ai":
        # æ··åˆæ¯”å°æˆ–AIæ¯”å°
        matches = improved_matching_algorithm(doc1_data, doc2_data, similarity_threshold)
    
    else:
        # é»˜èªä½¿ç”¨æ¨¡ç³Šæ¯”å°
        matches = improved_matching_algorithm(doc1_data, doc2_data, similarity_threshold)
    
    # ç‚ºæ¯å€‹åŒ¹é…ç”Ÿæˆå·®ç•°æ¨™è¨˜
    for match in matches:
        # å­—ç¬¦ç´šåˆ¥å·®ç•°
        d = difflib.Differ()
        diff = list(d.compare(match["doc1_text"], match["doc2_text"]))
        
        # ç”ŸæˆHTMLå·®ç•°é¡¯ç¤º
        html_diff = []
        for i, s in enumerate(diff):
            if s.startswith('  '):  # ç›¸åŒ
                html_diff.append(s[2:])
            elif s.startswith('- '):  # åˆªé™¤
                if i+1 < len(diff) and diff[i+1].startswith('? '):
                    # æœ‰æ¨™è¨˜ï¼Œä½¿ç”¨å­—ç¬¦ç´šåˆ¥å·®ç•°
                    markers = diff[i+1][2:]
                    s = s[2:]
                    html_s = ""
                    for j, c in enumerate(s):
                        if j < len(markers) and markers[j] in '-^':
                            html_s += f'<span class="diff-char-removed">{c}</span>'
                        else:
                            html_s += c
                    html_diff.append(f'<span class="diff-removed">{html_s}</span>')
                else:
                    # æ²’æœ‰æ¨™è¨˜ï¼Œä½¿ç”¨è¡Œç´šåˆ¥å·®ç•°
                    html_diff.append(f'<span class="diff-removed">{s[2:]}</span>')
            elif s.startswith('+ '):  # æ·»åŠ 
                if i+1 < len(diff) and diff[i+1].startswith('? '):
                    # æœ‰æ¨™è¨˜ï¼Œä½¿ç”¨å­—ç¬¦ç´šåˆ¥å·®ç•°
                    markers = diff[i+1][2:]
                    s = s[2:]
                    html_s = ""
                    for j, c in enumerate(s):
                        if j < len(markers) and markers[j] in '+^':
                            html_s += f'<span class="diff-char-added">{c}</span>'
                        else:
                            html_s += c
                    html_diff.append(f'<span class="diff-added">{html_s}</span>')
                else:
                    # æ²’æœ‰æ¨™è¨˜ï¼Œä½¿ç”¨è¡Œç´šåˆ¥å·®ç•°
                    html_diff.append(f'<span class="diff-added">{s[2:]}</span>')
            elif s.startswith('? '):  # æ¨™è¨˜è¡Œï¼Œå·²åœ¨ä¸Šé¢è™•ç†
                continue
        
        match["diff_html"] = ''.join(html_diff)
    
    return matches

# è‡ªå®šç¾©AIé¡
class CustomAI:
    def __init__(self, api_key=None, model_name=None):
        self.api_key = api_key
        self.model_name = model_name
    
    def is_available(self):
        """æª¢æŸ¥APIæ˜¯å¦å¯ç”¨"""
        return self.api_key is not None and len(self.api_key) > 0
    
    def match_paragraphs(self, source_paragraphs, target_paragraphs):
        """ä½¿ç”¨AIåŒ¹é…æ®µè½"""
        # ç°¡åŒ–ç‰ˆæœ¬ï¼Œå¯¦éš›ä¸Šåªæ˜¯ä½¿ç”¨æ”¹é€²çš„åŒ¹é…ç®—æ³•
        matches = []
        
        for source_para in source_paragraphs:
            best_match = None
            best_similarity = 0
            
            for target_para in target_paragraphs:
                # ä½¿ç”¨difflibè¨ˆç®—ç›¸ä¼¼åº¦
                similarity = difflib.SequenceMatcher(None, source_para["content"], target_para["content"]).ratio()
                
                if similarity > best_similarity:
                    best_similarity = similarity
                    best_match = target_para
            
            # å¦‚æœæ‰¾åˆ°è¶³å¤ ç›¸ä¼¼çš„åŒ¹é…
            if best_match and best_similarity >= 0.6:
                matches.append({
                    "doc1_index": source_para["index"],
                    "doc1_text": source_para["content"],
                    "doc2_index": best_match["index"],
                    "doc2_text": best_match["content"],
                    "similarity": best_similarity,
                    "page_number": best_match.get("page", None)
                })
        
        return matches

# å‰µå»ºä¸€å€‹ç°¡å–®çš„QwenOCRé¡ï¼Œå¦‚æœåŸå§‹æ¨¡çµ„ä¸å¯ç”¨
if not QWEN_OCR_AVAILABLE:
    class QwenOCR:
        def __init__(self, api_key=None, api_url=None):
            self.api_key = api_key
            self.api_url = api_url
        
        def is_available(self):
            return self.api_key is not None and len(self.api_key) > 0
        
        def extract_text_from_image(self, image_path):
            return "OCRåŠŸèƒ½éœ€è¦å®‰è£qwen_ocræ¨¡çµ„"

if st.button("é–‹å§‹æ¯”å°"):
    if (word_file is None or pdf_file is None) and not use_example_files:
        st.warning("è«‹å…ˆä¸Šå‚³ Word èˆ‡ PDF æª”æ¡ˆï¼Œæˆ–é¸æ“‡ä½¿ç”¨ç¤ºä¾‹æ–‡ä»¶")
    else:
        st.info("ğŸ§  é–‹å§‹æ¯”å°ä¸­...")

        # 1. ä¿å­˜ä¸Šå‚³æª”æ¡ˆè‡³æš«å­˜
        if use_example_files:
            # ä½¿ç”¨ç¤ºä¾‹æ–‡ä»¶
            word_path = "æ¯”å°ç´ æ-åŸç¨¿.docx"  # å‡è¨­ç¤ºä¾‹æ–‡ä»¶å­˜åœ¨æ–¼ç•¶å‰ç›®éŒ„
            pdf_path = "æ¯”å°ç´ æ-ç¾ç·¨å¾Œå®Œç¨¿.pdf"  # å‡è¨­ç¤ºä¾‹æ–‡ä»¶å­˜åœ¨æ–¼ç•¶å‰ç›®éŒ„
            
            if not os.path.exists(word_path) or not os.path.exists(pdf_path):
                st.error("ç¤ºä¾‹æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè«‹ä¸Šå‚³è‡ªå·±çš„æ–‡ä»¶")
                st.stop()
        else:
            # ä½¿ç”¨ä¸Šå‚³çš„æ–‡ä»¶
            with tempfile.NamedTemporaryFile(delete=False, suffix=".docx") as tmp_word:
                tmp_word.write(word_file.read())
                word_path = tmp_word.name
            with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_pdf:
                tmp_pdf.write(pdf_file.read())
                pdf_path = tmp_pdf.name

        # 2. é€²è¡Œæ–‡å­—æŠ½å–
        word_data, pdf_data = enhanced_pdf_extraction(word_path, pdf_path)

        
# 3. å»ºç«‹ AI / OCR ç‰©ä»¶
ai_instance = None
prov = st.session_state.ai_provider
if st.session_state.use_ai:
    if prov.endswith("_builtin"):
        ai_instance = CustomAI(api_key=None, model_name=prov.replace("_builtin",""))
    else:
        ai_instance = CustomAI(api_key=st.session_state.ai_api_key,
                               model_name=prov)

# OCR instanceï¼ˆç›®å‰åƒ…å‚³éçµ¦é€²éšæŠ½å–å‡½å¼ï¼Œå¯è¦–éœ€æ±‚æ“´å……ï¼‰
ocr_instance = None
if st.session_state.use_ocr:
    eng = st.session_state.ocr_engine
    if eng == "qwen_builtin":
        ocr_instance = QwenOCR(api_key=None)
    elif eng == "ocr_custom":
        ocr_instance = QwenOCR(api_key=st.session_state.ocr_api_key)
    elif eng == "easyocr" and EASYOCR_AVAILABLE:
        import easyocr
        ocr_instance = easyocr.Reader(['ch_sim','en'])
    elif eng == "tesseract" and PYTESSERACT_AVAILABLE:
        import pytesseract
        ocr_instance = pytesseract
# 4. åŸ·è¡Œæ¯”å°æ¼”ç®—æ³•
        ignore_options = {
            "ignore_whitespace": st.session_state.ignore_whitespace,
            "ignore_punctuation": st.session_state.ignore_punctuation,
            "ignore_case": st.session_state.ignore_case,
            "ignore_linebreaks": st.session_state.ignore_linebreaks,
        }

        result = compare_documents(
            word_data,
            pdf_data,
            ignore_options=ignore_options,
            comparison_mode=st.session_state.comparison_mode,
            similarity_threshold=st.session_state.similarity_threshold,
            ai_instance=ai_instance
        )

        # 5. é¡¯ç¤ºçµæœ
        if result:
            st.success(f"æ¯”å°å®Œæˆï¼Œå…±è™•ç† {len(result)} çµ„æ®µè½ï¼")
            
            # å‰µå»ºä¸€å€‹æ‘˜è¦è¡¨æ ¼
            summary_data = {
                "ç¸½æ®µè½æ•¸": len(word_data["paragraphs"]),
                "PDFæ®µè½æ•¸": len(pdf_data["paragraphs"]),
                "åŒ¹é…æ®µè½æ•¸": len(result),
                "å·®ç•°æ®µè½æ•¸": sum(1 for item in result if item["similarity"] < 1.0)
            }
            
            st.subheader("æ¯”å°çµæœæ‘˜è¦")
            col1, col2, col3, col4 = st.columns(4)
            col1.metric("ç¸½æ®µè½æ•¸", summary_data["ç¸½æ®µè½æ•¸"])
            col2.metric("PDFæ®µè½æ•¸", summary_data["PDFæ®µè½æ•¸"])
            col3.metric("åŒ¹é…æ®µè½æ•¸", summary_data["åŒ¹é…æ®µè½æ•¸"])
            col4.metric("å·®ç•°æ®µè½æ•¸", summary_data["å·®ç•°æ®µè½æ•¸"])
            
            st.subheader("æ®µè½æ¯”å°çµæœ")
            
            # æŒ‰ç›¸ä¼¼åº¦æ’åºçµæœï¼ˆå¾ä½åˆ°é«˜ï¼‰
            sorted_result = sorted(result, key=lambda x: x["similarity"])
            
            for i, item in enumerate(sorted_result):
                similarity_class = ""
                if item["similarity"] >= 0.9:
                    similarity_class = "similarity-high"
                elif item["similarity"] >= 0.7:
                    similarity_class = "similarity-medium"
                else:
                    similarity_class = "similarity-low"
                
                with st.expander(f"æ®µè½ {i+1}: {item['doc1_text'][:50]}... (ç›¸ä¼¼åº¦: {item['similarity']:.2f})"):
                    st.markdown(f"**åŸå§‹æ–‡æœ¬:**")
                    st.markdown(f"{item['doc1_text']}")
                    
                    st.markdown(f"**ç¾ç·¨å¾Œæ–‡æœ¬:**")
                    if "page_number" in item and item["page_number"]:
                        st.markdown(f"é ç¢¼: {item['page_number']}")
                    st.markdown(f"{item['doc2_text']}")
                    
                    st.markdown(f"**ç›¸ä¼¼åº¦:** <span class='{similarity_class}'>{item['similarity']:.2f}</span>", unsafe_allow_html=True)
                    
                    if "diff_html" in item and item["diff_html"]:
                        st.markdown("**å·®ç•°é¡¯ç¤º:**")
                        st.markdown(item["diff_html"], unsafe_allow_html=True)
                    
                    # é¡¯ç¤ºPDFé é¢é è¦½
                    if "page_number" in item and item["page_number"] and os.path.exists(pdf_path):
                        st.markdown("**PDFé é¢é è¦½:**")
                        try:
                            doc = fitz.open(pdf_path)
                            page_num = item["page_number"] - 1  # é ç¢¼å¾1é–‹å§‹ï¼Œä½†PyMuPDFå¾0é–‹å§‹
                            if 0 <= page_num < len(doc):
                                page = doc[page_num]
                                pix = page.get_pixmap(matrix=fitz.Matrix(300/72, 300/72))
                                img_bytes = pix.tobytes("png")
                                st.image(img_bytes, caption=f"é ç¢¼: {item['page_number']}")
                            else:
                                st.warning(f"é ç¢¼ {item['page_number']} è¶…å‡ºç¯„åœ")
                        except Exception as e:
                            st.error(f"ç„¡æ³•é¡¯ç¤ºPDFé é¢: {e}")
        else:
            st.warning("æœªæ¯”å°åˆ°æœ‰æ•ˆæ®µè½ï¼Œè«‹æª¢æŸ¥æ–‡ä»¶å…§å®¹æ˜¯å¦æ­£ç¢ºã€‚")
