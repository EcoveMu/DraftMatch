import streamlit as st
import os
import tempfile
import docx
import re
from comparison_algorithm import semantic_matching
import difflib
import pandas as pd
import numpy as np
import base64
from io import BytesIO
import json
import shutil
import sys
from pathlib import Path
import fitz  # PyMuPDF

# æª¢æŸ¥sentence-transformersæ˜¯å¦å¯ç”¨
try:
    from sentence_transformers import SentenceTransformer
    SENTENCE_TRANSFORMERS_AVAILABLE = True
except ImportError:
    SENTENCE_TRANSFORMERS_AVAILABLE = False

# æª¢æŸ¥easyocræ˜¯å¦å¯ç”¨
try:
    import easyocr
    EASYOCR_AVAILABLE = True
except ImportError:
    EASYOCR_AVAILABLE = False

# æª¢æŸ¥tabulaæ˜¯å¦å¯ç”¨
try:
    import tabula
    TABULA_AVAILABLE = True
except ImportError:
    TABULA_AVAILABLE = False

# æª¢æŸ¥pdfplumberæ˜¯å¦å¯ç”¨
try:
    import pdfplumber
    PDFPLUMBER_AVAILABLE = True
except ImportError:
    PDFPLUMBER_AVAILABLE = False

# æª¢æŸ¥pytesseractæ˜¯å¦å¯ç”¨
try:
    import pytesseract
    PYTESSERACT_AVAILABLE = True
except ImportError:
    PYTESSERACT_AVAILABLE = False

# æª¢æŸ¥qwen_ocræ¨¡çµ„æ˜¯å¦å¯ç”¨
try:
    from qwen_ocr import QwenOCR
    QWEN_OCR_AVAILABLE = True
except ImportError:
    QWEN_OCR_AVAILABLE = False

# è¨­ç½®é é¢é…ç½®
st.set_page_config(
    page_title="æœŸåˆŠæ¯”å°ç³»çµ±",
    page_icon="ğŸ“Š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# è‡ªå®šç¾©CSSæ¨£å¼
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        color: #1E88E5;
        text-align: center;
        margin-bottom: 1rem;
    }
    .sub-header {
        font-size: 1.5rem;
        color: #0D47A1;
        margin-top: 2rem;
        margin-bottom: 1rem;
    }
    .highlight-diff {
        background-color: #FFECB3;
        padding: 2px;
        border-radius: 3px;
    }
    .diff-added {
        color: #000000;
        background-color: #C8E6C9;
        padding: 2px;
        border-radius: 3px;
    }
    .diff-removed {
        color: #000000;
        background-color: #FFCDD2;
        padding: 2px;
        border-radius: 3px;
    }
    .result-container {
        border: 1px solid #E0E0E0;
        border-radius: 5px;
        padding: 15px;
        margin-bottom: 15px;
    }
    .similarity-high {
        color: #2E7D32;
        font-weight: bold;
    }
    .similarity-medium {
        color: #F57F17;
        font-weight: bold;
    }
    .similarity-low {
        color: #C62828;
        font-weight: bold;
    }
    .table-warning {
        background-color: #FFF3E0;
        padding: 10px;
        border-left: 4px solid #FF9800;
        margin-bottom: 10px;
    }
    .file-uploader-container {
        border: 1px dashed #BDBDBD;
        border-radius: 5px;
        padding: 10px;
        margin-bottom: 10px;
    }
    .multi-file-uploader {
        margin-bottom: 20px;
    }
    .chapter-selector {
        margin-top: 10px;
        margin-bottom: 20px;
    }
    .diff-char-removed {
        color: #000000;
        background-color: #FFCDD2;
        font-weight: bold;
        padding: 1px;
        border-radius: 2px;
    }
    .diff-char-added {
        color: #000000;
        background-color: #C8E6C9;
        font-weight: bold;
        padding: 1px;
        border-radius: 2px;
    }
    .diff-section {
        margin-top: 10px;
        margin-bottom: 10px;
        padding: 10px;
        border: 1px solid #E0E0E0;
        border-radius: 5px;
    }
    .diff-navigation {
        margin-top: 10px;
        margin-bottom: 10px;
        text-align: center;
    }
    .diff-count {
        font-weight: bold;
        margin-left: 10px;
        margin-right: 10px;
    }
    .api-key-input {
        margin-top: 10px;
        margin-bottom: 10px;
    }
    .success-message {
        color: #2E7D32;
        background-color: #E8F5E9;
        padding: 10px;
        border-radius: 5px;
        margin-top: 10px;
        margin-bottom: 10px;
    }
    .error-message {
        color: #C62828;
        background-color: #FFEBEE;
        padding: 10px;
        border-radius: 5px;
        margin-top: 10px;
        margin-bottom: 10px;
    }
    .ai-model-section {
        background-color: #E3F2FD;
        padding: 10px;
        border-radius: 5px;
        margin-top: 10px;
        margin-bottom: 10px;
    }
    /* é©æ‡‰æ·±è‰²ä¸»é¡Œçš„æ¨£å¼ */
    @media (prefers-color-scheme: dark) {
        .stMarkdown p, .stMarkdown h1, .stMarkdown h2, .stMarkdown h3, .stMarkdown h4, .stMarkdown h5, .stMarkdown h6, .stMarkdown li {
            color: white !important;
        }
        .stText {
            color: white !important;
        }
        .stTextInput > div > div > input {
            color: white !important;
        }
        .stSelectbox > div > div > div {
            color: white !important;
        }
        .stSlider > div > div > div {
            color: white !important;
        }
        .stCheckbox > div > div > label {
            color: white !important;
        }
        .stExpander > div > div > div > div > p {
            color: white !important;
        }
        .stExpander > div > div > div > div > div > p {
            color: white !important;
        }
    }
</style>
""", unsafe_allow_html=True)

# é¡¯ç¤ºæ¨™é¡Œ
st.markdown('<h1 class="main-header">æœŸåˆŠæ¯”å°ç³»çµ±</h1>', unsafe_allow_html=True)
st.markdown('æœ¬ç³»çµ±ç”¨æ–¼æ¯”å°åŸå§‹Wordæ–‡ä»¶èˆ‡ç¾ç·¨å¾ŒPDFæ–‡ä»¶çš„å…§å®¹å·®ç•°ï¼Œå¹«åŠ©æ ¡å°äººå“¡å¿«é€Ÿæ‰¾å‡ºä¸ä¸€è‡´ä¹‹è™•ã€‚')

# --- SessionState é è¨­å€¼èˆ‡åˆå§‹åŒ– ------------------------------------
default_states = {
    "comparison_mode": "hybrid",
    "similarity_threshold": 0.6,
    "use_ocr": False,
    "ocr_engine": "qwen_builtin",
    "ocr_api_key": "",
    "use_ai": False,
    "ai_provider": "deepseek_builtin",
    "ai_api_key": "",
    "ignore_whitespace": True,
    "ignore_punctuation": True,
    "ignore_case": True,
    "ignore_linebreaks": True,
}
for k, v in default_states.items():
    if k not in st.session_state:
        st.session_state[k] = v

# Sidebar è¨­å®šï¼ˆç¹é«”ä¸­æ–‡ä»‹é¢ï¼‰
with st.sidebar:
    st.header("âš™ï¸ æ¯”å°è¨­å®š")

    mode_labels = {
        "ç²¾ç¢ºæ¯”å°ï¼ˆExactï¼‰": "exact",
        "èªæ„æ¯”å°ï¼ˆSemanticï¼‰": "semantic",
        "æ··åˆæ¯”å°ï¼ˆHybridï¼‰": "hybrid",
        "AI æ¯”å°": "ai",
    }
    # å–å¾—ç•¶å‰æ¨¡å¼å°æ‡‰çš„ä¸­æ–‡æ¨™ç±¤
    current_mode_label = next(k for k,v in mode_labels.items() if v == st.session_state.comparison_mode)
    selected_label = st.selectbox("æ¯”å°æ¨¡å¼", list(mode_labels.keys()),
                                  index=list(mode_labels.keys()).index(current_mode_label))
    st.session_state.comparison_mode = mode_labels[selected_label]

    st.session_state.similarity_threshold = st.slider(
        "ç›¸ä¼¼åº¦é–¾å€¼",
        0.0, 1.0,
        st.session_state.similarity_threshold,
        0.05
    )

    st.divider()
    st.subheader("ğŸ” OCR è¨­å®š")

    st.session_state.use_ocr = st.checkbox("å•Ÿç”¨ OCR", value=st.session_state.use_ocr)

    if st.session_state.use_ocr:
        ocr_choice = st.radio(
            "OCR å¼•æ“",
            ["Qwenï¼ˆå…§å»ºï¼‰", "EasyOCR", "Tesseract", "è‡ªå®šç¾© OCR API"],
            index=["qwen_builtin", "easyocr", "tesseract", "ocr_custom"]
            .index(st.session_state.ocr_engine)
        )
        st.session_state.ocr_engine = {
            "Qwenï¼ˆå…§å»ºï¼‰": "qwen_builtin",
            "EasyOCR": "easyocr",
            "Tesseract": "tesseract",
            "è‡ªå®šç¾© OCR API": "ocr_custom",
        }[ocr_choice]

        if st.session_state.ocr_engine == "ocr_custom":
            st.session_state.ocr_api_key = st.text_input(
                "ğŸ”‘ è«‹è¼¸å…¥ OCR API é‡‘é‘°",
                type="password",
                value=st.session_state.ocr_api_key
            )

    st.divider()
    st.subheader("ğŸ¤– ç”Ÿæˆå¼ AI è¨­å®š")

    st.session_state.use_ai = st.checkbox("ä½¿ç”¨ç”Ÿæˆå¼ AI", value=st.session_state.use_ai)

    if st.session_state.use_ai:
        ai_choice = st.selectbox(
            "AI ä¾†æº / æ¨¡å‹",
            ["DeepSeekï¼ˆå…§å»ºï¼‰", "Qwen2.5ï¼ˆå…§å»ºï¼‰", "Mistralâ€‘7Bï¼ˆå…§å»ºï¼‰",
             "OpenAI", "Anthropic", "Qwen (API)", "è‡ªå®šç¾© AI API"],
            index=[
                "deepseek_builtin","qwen_builtin","mistral_builtin",
                "openai","anthropic","qwen_api","ai_custom"
            ].index(st.session_state.ai_provider)
        )
        st.session_state.ai_provider = {
            "DeepSeekï¼ˆå…§å»ºï¼‰": "deepseek_builtin",
            "Qwen2.5ï¼ˆå…§å»ºï¼‰": "qwen_builtin",
            "Mistralâ€‘7Bï¼ˆå…§å»ºï¼‰": "mistral_builtin",
            "OpenAI": "openai",
            "Anthropic": "anthropic",
            "Qwen (API)": "qwen_api",
            "è‡ªå®šç¾© AI API": "ai_custom",
        }[ai_choice]

        if st.session_state.ai_provider in {"openai","anthropic","qwen_api","ai_custom"}:
            st.session_state.ai_api_key = st.text_input(
                "ğŸ”‘ ç”Ÿæˆå¼ AI API é‡‘é‘°",
                type="password",
                value=st.session_state.ai_api_key
            )

    st.divider()
    st.subheader("ğŸ§¹ å¿½ç•¥è¦å‰‡")
    st.session_state.ignore_whitespace = st.checkbox("å¿½ç•¥ç©ºæ ¼", value=st.session_state.ignore_whitespace)
    st.session_state.ignore_punctuation = st.checkbox("å¿½ç•¥æ¨™é»ç¬¦è™Ÿ", value=st.session_state.ignore_punctuation)
    st.session_state.ignore_case = st.checkbox("å¿½ç•¥å¤§å°å¯«", value=st.session_state.ignore_case)
    st.session_state.ignore_linebreaks = st.checkbox("å¿½ç•¥æ–·è¡Œ", value=st.session_state.ignore_linebreaks)

    st.divider()
    st.subheader("â„¹ï¸ ç³»çµ±è³‡è¨Š")
    st.info("æœ¬ç³»çµ±ç”¨æ–¼æ¯”å°åŸå§‹ Word èˆ‡ PDF å…§å®¹å·®ç•°ï¼Œå”åŠ©æ ¡å°ã€‚")

# æ–‡ä»¶ä¸Šå‚³å€åŸŸ
st.header("ğŸ“ æ–‡ä»¶ä¸Šå‚³")

col1, col2 = st.columns(2)
with col1:
    st.subheader("åŸå§‹Wordæ–‡ä»¶")
    word_file = st.file_uploader("ä¸Šå‚³åŸå§‹ Word æ–‡ç¨¿", type=["docx"])
    
    if word_file:
        st.success(f"å·²ä¸Šå‚³: {word_file.name}")
        
with col2:
    st.subheader("ç¾ç·¨å¾ŒPDFæ–‡ä»¶")
    pdf_file = st.file_uploader("ä¸Šå‚³ç¾ç·¨å¾Œ PDF æ–‡ä»¶", type=["pdf"])
    
    if pdf_file:
        st.success(f"å·²ä¸Šå‚³: {pdf_file.name}")

# ä½¿ç”¨ç¤ºä¾‹æ–‡ä»¶é¸é …
use_example_files = st.checkbox("ä½¿ç”¨ç¤ºä¾‹æ–‡ä»¶é€²è¡Œæ¼”ç¤º", value=False)

# æ–‡æœ¬æå–å’Œè™•ç†å‡½æ•¸
def extract_text_from_word(word_file):
    """å¾Wordæ–‡ä»¶ä¸­æå–æ–‡æœ¬"""
    doc = docx.Document(word_file)
    
    paragraphs = []
    tables = []
    
    # æå–æ®µè½
    for i, para in enumerate(doc.paragraphs):
        text = para.text.strip()
        if text:
            paragraphs.append({
                "index": i,
                "content": text,
                "type": "paragraph"
            })
    
    # æå–è¡¨æ ¼
    for i, table in enumerate(doc.tables):
        table_data = []
        for row in table.rows:
            row_data = []
            for cell in row.cells:
                row_data.append(cell.text.strip())
            table_data.append(row_data)
        
        if any(any(cell for cell in row) for row in table_data):
            tables.append({
                "index": i,
                "content": table_data,
                "type": "table"
            })
    
    return {
        "paragraphs": paragraphs,
        "tables": tables
    }

def extract_text_from_pdf(pdf_file):
    """å¾PDFæ–‡ä»¶ä¸­æå–æ–‡æœ¬ï¼ˆä½¿ç”¨PyMuPDFï¼‰"""
    doc = fitz.open(pdf_file)
    
    paragraphs = []
    page_texts = []
    
    for page_num in range(len(doc)):
        page = doc[page_num]
        text = page.get_text()
        page_texts.append(text)
        
        # ç°¡å–®åœ°æŒ‰è¡Œåˆ†å‰²æ–‡æœ¬
        lines = text.split('\n')
        for i, line in enumerate(lines):
            line = line.strip()
            if line:
                paragraphs.append({
                    "index": len(paragraphs),
                    "content": line,
                    "type": "paragraph",
                    "page": page_num + 1
                })
    
    return {
        "paragraphs": paragraphs,
        "tables": [],  # ç°¡åŒ–ç‰ˆæœ¬ä¸æå–è¡¨æ ¼
        "page_texts": page_texts
    }

def enhanced_pdf_extraction(word_path, pdf_path):
    """å¢å¼·ç‰ˆçš„æ–‡æª”æå–å‡½æ•¸"""
    # æå–Wordæ–‡æª”å…§å®¹
    if word_path.endswith('.docx'):
        word_data = extract_text_from_word(word_path)
    else:
        # å¦‚æœä¸æ˜¯.docxæ–‡ä»¶ï¼Œå˜—è©¦ä½œç‚ºæ–‡æœ¬æ–‡ä»¶è®€å–
        try:
            with open(word_path, 'r', encoding='utf-8') as f:
                content = f.read()
                paragraphs = []
                for i, para in enumerate(content.split('\n\n')):
                    para = para.strip()
                    if para:
                        paragraphs.append({
                            "index": i,
                            "content": para,
                            "type": "paragraph"
                        })
                word_data = {
                    "paragraphs": paragraphs,
                    "tables": []
                }
        except Exception as e:
            st.error(f"ç„¡æ³•è®€å–Wordæ–‡ä»¶: {e}")
            return None, None
    
    # æå–PDFæ–‡æª”å…§å®¹
    if pdf_path.endswith('.pdf'):
        pdf_data = extract_text_from_pdf(pdf_path)
    else:
        # å¦‚æœä¸æ˜¯.pdfæ–‡ä»¶ï¼Œå˜—è©¦ä½œç‚ºæ–‡æœ¬æ–‡ä»¶è®€å–
        try:
            with open(pdf_path, 'r', encoding='utf-8') as f:
                content = f.read()
                paragraphs = []
                for i, para in enumerate(content.split('\n\n')):
                    para = para.strip()
                    if para:
                        paragraphs.append({
                            "index": i,
                            "content": para,
                            "type": "paragraph",
                            "page": 1  # å‡è¨­åªæœ‰ä¸€é 
                        })
                pdf_data = {
                    "paragraphs": paragraphs,
                    "tables": [],
                    "page_texts": [content]
                }
        except Exception as e:
            st.error(f"ç„¡æ³•è®€å–PDFæ–‡ä»¶: {e}")
            return None, None
    
    return word_data, pdf_data

def improved_matching_algorithm(word_data, pdf_data, similarity_threshold=0.6):
    """æ”¹é€²çš„åŒ¹é…ç®—æ³•"""
    matches = []
    
    # å°æ¯å€‹Wordæ®µè½ï¼Œæ‰¾åˆ°æœ€ç›¸ä¼¼çš„PDFæ®µè½
    for word_para in word_data["paragraphs"]:
        best_match = None
        best_similarity = 0
        
        for pdf_para in pdf_data["paragraphs"]:
            # ä½¿ç”¨difflibè¨ˆç®—ç›¸ä¼¼åº¦
            similarity = difflib.SequenceMatcher(None, word_para["content"], pdf_para["content"]).ratio()
            
            if similarity > best_similarity:
                best_similarity = similarity
                best_match = pdf_para
        
        # å¦‚æœæ‰¾åˆ°è¶³å¤ ç›¸ä¼¼çš„åŒ¹é…
        if best_match and best_similarity >= similarity_threshold:
            matches.append({
                "doc1_index": word_para["index"],
                "doc1_text": word_para["content"],
                "doc2_index": best_match["index"],
                "doc2_text": best_match["content"],
                "similarity": best_similarity,
                "page_number": best_match.get("page", None)
            })
    
    return matches

# æ¯”å°ç®—æ³•
def compare_documents(doc1_data, doc2_data, ignore_options=None, comparison_mode="hybrid",
                      similarity_threshold=0.6, ai_instance=None):
    """æ¯”å°å…©å€‹æ–‡æª”çš„å…§å®¹ï¼Œæ”¯æ´ç²¾ç¢º / èªæ„ / æ··åˆ / AI æ¨¡å¼ï¼Œä¸¦å…§å»ºé€²åº¦æ¢é¡¯ç¤º"""
    if ignore_options is None:
        ignore_options = {
            "ignore_whitespace": True,
            "ignore_punctuation": True,
            "ignore_case": True,
            "ignore_linebreaks": True
        }

    # é è™•ç†å‡½å¼
    def preprocess_text(text):
        if ignore_options.get("ignore_whitespace", False):
            text = re.sub(r'\s+', ' ', text)
        if ignore_options.get("ignore_punctuation", False):
            text = re.sub(r'[^\w\s]', '', text)
        if ignore_options.get("ignore_case", False):
            text = text.lower()
        if ignore_options.get("ignore_linebreaks", False):
            text = text.replace('\n', ' ')
        return text.strip()

    # æ•´ç†æ®µè½
    for para in doc1_data["paragraphs"]:
        para["processed"] = preprocess_text(para["content"])
    for para in doc2_data["paragraphs"]:
        para["processed"] = preprocess_text(para["content"])

    total = len(doc1_data["paragraphs"])
    progress_bar = st.progress(0.0)

    matches = []
    for idx, src in enumerate(doc1_data["paragraphs"]):
        best_match = None
        best_sim = 0.0

        for tgt in doc2_data["paragraphs"]:
            if comparison_mode == "exact":
                sim = difflib.SequenceMatcher(None, src["processed"], tgt["processed"]).ratio()
            elif comparison_mode == "semantic":
                sim = semantic_matching(src["processed"], tgt["processed"])
            elif comparison_mode == "hybrid":
                exact_sim = difflib.SequenceMatcher(None, src["processed"], tgt["processed"]).ratio()
                if exact_sim >= similarity_threshold:
                    sim = exact_sim
                else:
                    sem_sim = semantic_matching(src["processed"], tgt["processed"])
                    sim = max(exact_sim, sem_sim)
            elif comparison_mode == "ai" and ai_instance:
                sim, _ = ai_instance.semantic_comparison(src["content"], tgt["content"])
            else:
                sim = 0.0

            if sim > best_sim:
                best_sim = sim
                best_match = tgt

        if best_match and best_sim >= similarity_threshold:
            # ç”Ÿæˆå·®ç•° html
            d = difflib.Differ()
            diff = list(d.compare(src["content"], best_match["content"]))
            diff_html = []
            for i, s in enumerate(diff):
                if s.startswith('  '):
                    diff_html.append(s[2:])
                elif s.startswith('- '):
                    diff_html.append(f'<span class="diff-removed">{s[2:]}</span>')
                elif s.startswith('+ '):
                    diff_html.append(f'<span class="diff-added">{s[2:]}</span>')
            matches.append({
                "doc1_index": src["index"],
                "doc1_text": src["content"],
                "doc2_index": best_match["index"],
                "doc2_text": best_match["content"],
                "similarity": best_sim,
                "page_number": best_match.get("page", None),
                "diff_html": ''.join(diff_html)
            })

        progress_bar.progress((idx + 1) / total)

    return matches


# -------------------- é–‹å§‹æ¯”å°æŒ‰éˆ•èˆ‡çµæœå±•ç¤º --------------------
st.markdown("---")
if st.button("ğŸš€ é–‹å§‹æ¯”å°", use_container_width=True):
    if not word_file and not use_example_files:
        st.error("è«‹å…ˆä¸Šå‚³ Word æ–‡ä»¶ï¼Œæˆ–å‹¾é¸ä½¿ç”¨ç¤ºä¾‹æ–‡ä»¶ã€‚")
    elif not pdf_file and not use_example_files:
        st.error("è«‹å…ˆä¸Šå‚³ PDF æ–‡ä»¶ï¼Œæˆ–å‹¾é¸ä½¿ç”¨ç¤ºä¾‹æ–‡ä»¶ã€‚")
    else:
        if use_example_files:
            # å¾ example è³‡æ–™å¤¾è®€å…¥å…§å»ºç¯„ä¾‹
            sample_dir = Path(__file__).parent / "examples"
            word_path = sample_dir / "sample.docx"
            pdf_path = sample_dir / "sample.pdf"
            if not word_path.exists() or not pdf_path.exists():
                st.error("æ‰¾ä¸åˆ°ç¤ºä¾‹æ–‡ä»¶ï¼Œè«‹ç¢ºèª examples ç›®éŒ„å­˜åœ¨ sample.docx / sample.pdf")
                st.stop()
            word_data = extract_text_from_word(str(word_path))
            pdf_data = extract_text_from_pdf(str(pdf_path))
        else:
            word_data = extract_text_from_word(word_file)
            pdf_data = extract_text_from_pdf(pdf_file)

        st.info("æ­£åœ¨åŸ·è¡Œæ¯”å°ï¼Œè«‹ç¨å€™...")
        matches = compare_documents(
            word_data, pdf_data,
            ignore_options={
                "ignore_whitespace": st.session_state.ignore_whitespace,
                "ignore_punctuation": st.session_state.ignore_punctuation,
                "ignore_case": st.session_state.ignore_case,
                "ignore_linebreaks": st.session_state.ignore_linebreaks,
            },
            comparison_mode=st.session_state.comparison_mode,
            similarity_threshold=st.session_state.similarity_threshold,
            ai_instance=None
        )
        if not matches:
            st.warning("æœªæ‰¾åˆ°ä»»ä½•é«˜æ–¼ç›¸ä¼¼åº¦é–¾å€¼çš„åŒ¹é…æ®µè½ã€‚")
        else:
            st.success(f"æ¯”å°å®Œæˆï¼Œå…±æ‰¾åˆ° {len(matches)} çµ„åŒ¹é…ï¼")

            # çµ±è¨ˆèˆ‡æ‘˜è¦
            total_paragraphs = len(word_data["paragraphs"])
            matched = len(matches)
            unmatched = total_paragraphs - matched
            match_rate = matched / total_paragraphs * 100 if total_paragraphs else 0.0

            st.subheader("ğŸ“Š æ¯”å°çµæœæ‘˜è¦")
            col_a, col_b, col_c = st.columns(3)
            col_a.metric("Wordæ®µè½ç¸½æ•¸", total_paragraphs)
            col_b.metric("åŒ¹é…æ®µè½æ•¸", matched)
            col_c.metric("åŒ¹é…ç‡", f"{match_rate:.1f}%")

            # è©³ç´°è¡¨æ ¼
            st.subheader("ğŸ“„ è©³ç´°åŒ¹é…çµæœ")
            import pandas as pd
            df = pd.DataFrame(matches)
            df_display = df[["doc1_index","doc2_index","similarity","page_number"]]
            st.dataframe(df_display, use_container_width=True)

            # å¯å±•é–‹æŸ¥çœ‹å·®ç•°
            for m in matches:
                with st.expander(f"æ®µè½ {m['doc1_index']} â†” PDFæ®µè½ {m['doc2_index']} (ç›¸ä¼¼åº¦ {m['similarity']:.2f})"):
                    st.markdown(f"**Wordï¼š** {m['doc1_text']}", unsafe_allow_html=True)
                    st.markdown(f"**PDFï¼š** {m['doc2_text']}", unsafe_allow_html=True)
                    st.markdown("**å·®ç•°ï¼š**", unsafe_allow_html=True)
                    st.markdown(m["diff_html"], unsafe_allow_html=True)
