import streamlit as st
import os
import tempfile
import docx
import re
import difflib
import pandas as pd
import numpy as np
import base64
from io import BytesIO
import json
import shutil
import sys
from pathlib import Path
from enhanced_extraction import enhanced_pdf_extraction, improved_matching_algorithm
from qwen_api import QwenOCR

# æª¢æŸ¥sentence-transformersæ˜¯å¦å¯ç”¨
try:
    from sentence_transformers import SentenceTransformer
    SENTENCE_TRANSFORMERS_AVAILABLE = True
except ImportError:
    SENTENCE_TRANSFORMERS_AVAILABLE = False

# è¨­ç½®é é¢é…ç½®
st.set_page_config(
    page_title="æœŸåˆŠæ¯”å°ç³»çµ±",
    page_icon="ğŸ“Š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# è‡ªå®šç¾©CSSæ¨£å¼
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        color: #1E88E5;
        text-align: center;
        margin-bottom: 1rem;
    }
    .sub-header {
        font-size: 1.5rem;
        color: #0D47A1;
        margin-top: 2rem;
        margin-bottom: 1rem;
    }
    .highlight-diff {
        background-color: #FFECB3;
        padding: 2px;
        border-radius: 3px;
    }
    .diff-added {
        color: #000000;
        background-color: #C8E6C9;
        padding: 2px;
        border-radius: 3px;
    }
    .diff-removed {
        color: #000000;
        background-color: #FFCDD2;
        padding: 2px;
        border-radius: 3px;
    }
    .result-container {
        border: 1px solid #E0E0E0;
        border-radius: 5px;
        padding: 15px;
        margin-bottom: 15px;
    }
    .similarity-high {
        color: #2E7D32;
        font-weight: bold;
    }
    .similarity-medium {
        color: #F57F17;
        font-weight: bold;
    }
    .similarity-low {
        color: #C62828;
        font-weight: bold;
    }
    .table-warning {
        background-color: #FFF3E0;
        padding: 10px;
        border-left: 4px solid #FF9800;
        margin-bottom: 10px;
    }
    .file-uploader-container {
        border: 1px dashed #BDBDBD;
        border-radius: 5px;
        padding: 10px;
        margin-bottom: 10px;
    }
    .multi-file-uploader {
        margin-bottom: 20px;
    }
    .chapter-selector {
        margin-top: 10px;
        margin-bottom: 20px;
    }
    .diff-char-removed {
        color: #000000;
        background-color: #FFCDD2;
        font-weight: bold;
        padding: 1px;
        border-radius: 2px;
    }
    .diff-char-added {
        color: #000000;
        background-color: #C8E6C9;
        font-weight: bold;
        padding: 1px;
        border-radius: 2px;
    }
    .diff-section {
        margin-top: 10px;
        margin-bottom: 10px;
        padding: 10px;
        border: 1px solid #E0E0E0;
        border-radius: 5px;
    }
    .diff-navigation {
        margin-top: 10px;
        margin-bottom: 10px;
        text-align: center;
    }
    .diff-count {
        font-weight: bold;
        margin-left: 10px;
        margin-right: 10px;
    }
    .api-key-input {
        margin-top: 10px;
        margin-bottom: 10px;
    }
    .success-message {
        color: #2E7D32;
        background-color: #E8F5E9;
        padding: 10px;
        border-radius: 5px;
        margin-top: 10px;
        margin-bottom: 10px;
    }
    .error-message {
        color: #C62828;
        background-color: #FFEBEE;
        padding: 10px;
        border-radius: 5px;
        margin-top: 10px;
        margin-bottom: 10px;
    }
</style>
""", unsafe_allow_html=True)

# é¡¯ç¤ºæ¨™é¡Œ
st.markdown('<h1 class="main-header">æœŸåˆŠæ¯”å°ç³»çµ±</h1>', unsafe_allow_html=True)
st.markdown('æœ¬ç³»çµ±ç”¨æ–¼æ¯”å°åŸå§‹Wordæ–‡ä»¶èˆ‡ç¾ç·¨å¾ŒPDFæ–‡ä»¶çš„å…§å®¹å·®ç•°ï¼Œå¹«åŠ©æ ¡å°äººå“¡å¿«é€Ÿæ‰¾å‡ºä¸ä¸€è‡´ä¹‹è™•ã€‚')

# æª¢æŸ¥Javaæ˜¯å¦å®‰è£
def is_java_installed():
    try:
        result = os.system("java -version > /dev/null 2>&1")
        return result == 0
    except:
        return False

# æª¢æŸ¥EasyOCRæ˜¯å¦å¯ç”¨
def is_easyocr_available():
    try:
        import easyocr
        return True
    except ImportError:
        return False

# æª¢æŸ¥tabula-pyæ˜¯å¦å¯ç”¨
def is_tabula_available():
    if not is_java_installed():
        return False
    try:
        import tabula
        return True
    except ImportError:
        return False

# æª¢æŸ¥sentence-transformersæ˜¯å¦å¯ç”¨
def is_sentence_transformers_available():
    return SENTENCE_TRANSFORMERS_AVAILABLE

# åŠ è¼‰èªç¾©æ¨¡å‹
@st.cache_resource
def load_semantic_model():
    if is_sentence_transformers_available():
        try:
            model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
            return model
        except Exception as e:
            st.error(f"åŠ è¼‰èªç¾©æ¨¡å‹å¤±æ•—: {e}")
            return None
    return None

# é¡¯ç¤ºç³»çµ±ç‹€æ…‹
with st.expander("ç³»çµ±ç‹€æ…‹", expanded=True):
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        if is_java_installed():
            st.success("Javaå·²å®‰è£")
        else:
            st.error("Javaæœªå®‰è£ï¼Œè¡¨æ ¼æå–åŠŸèƒ½å¯èƒ½å—é™")
    
    with col2:
        if is_easyocr_available():
            st.success("OpenOCR (EasyOCR) å·²å®‰è£")
        else:
            st.warning("OpenOCRæœªå®‰è£ï¼Œå°‡ä½¿ç”¨Tesseractä½œç‚ºæ›¿ä»£")
    
    with col3:
        if is_tabula_available():
            st.success("è¡¨æ ¼æå–å·¥å…·å·²å®‰è£")
        else:
            st.error("è¡¨æ ¼æå–å·¥å…·æœªå®‰è£æˆ–ç„¡æ³•ä½¿ç”¨")
    
    with col4:
        if is_sentence_transformers_available():
            st.success("èªç¾©æ¨¡å‹å·²å®‰è£")
        else:
            st.warning("èªç¾©æ¨¡å‹æœªå®‰è£ï¼Œèªç¾©æ¯”å°åŠŸèƒ½å°‡ä½¿ç”¨ç°¡åŒ–ç‰ˆ")
    
    if not is_java_installed():
        st.info("å®‰è£Java: è«‹åŸ·è¡Œ 'sudo apt-get install -y default-jre' æˆ–å®‰è£é©åˆæ‚¨ç³»çµ±çš„Javaé‹è¡Œç’°å¢ƒ")
    
    if not is_easyocr_available():
        st.info("å®‰è£OpenOCR: è«‹åŸ·è¡Œ 'pip install easyocr'")
    
    if not is_sentence_transformers_available():
        st.info("å®‰è£èªç¾©æ¨¡å‹: è«‹åŸ·è¡Œ 'pip install sentence-transformers'")
        st.warning("æ³¨æ„ï¼šèªç¾©æ¨¡å‹æœªå®‰è£ï¼Œç³»çµ±å°‡ä½¿ç”¨ç°¡åŒ–ç‰ˆèªç¾©æ¯”å°ï¼Œç²¾åº¦å¯èƒ½è¼ƒä½")

# å´é‚Šæ¬„ - æ¯”å°è¨­ç½®
with st.sidebar:
    st.header("æ¯”å°è¨­ç½®")
    
    # æ¯”å°æ¨¡å¼
    comparison_mode = st.radio(
        "æ¯”å°æ¨¡å¼",
        ["ç²¾ç¢ºæ¯”å°", "èªæ„æ¯”å°", "æ··åˆæ¯”å°"],
        index=2
    )
    
    # ç›¸ä¼¼åº¦é–¾å€¼
    similarity_threshold = st.slider(
        "ç›¸ä¼¼åº¦é–¾å€¼",
        min_value=0.0,
        max_value=1.0,
        value=0.8,
        step=0.05,
        help="ç›¸ä¼¼åº¦ä½æ–¼æ­¤é–¾å€¼çš„æ®µè½å°‡è¢«æ¨™è¨˜ç‚ºä¸ä¸€è‡´"
    )
    
    # å¿½ç•¥é¸é …
    st.subheader("å¿½ç•¥é¸é …")
    ignore_space = st.checkbox("å¿½ç•¥ç©ºæ ¼", value=True)
    ignore_punctuation = st.checkbox("å¿½ç•¥æ¨™é»ç¬¦è™Ÿ", value=True)
    ignore_case = st.checkbox("å¿½ç•¥å¤§å°å¯«", value=True)
    ignore_newline = st.checkbox("å¿½ç•¥æ›è¡Œ", value=True)
    
    # OCRè¨­ç½®
    st.subheader("OCRè¨­ç½®")
    ocr_engine = st.radio(
        "OCRå¼•æ“",
        ["è‡ªå‹•é¸æ“‡", "Tesseract", "OpenOCR (EasyOCR)", "Qwen API"],
        index=0,
        help="é¸æ“‡ç”¨æ–¼æå–PDFæ–‡æœ¬çš„OCRå¼•æ“"
    )
    
    # å¦‚æœé¸æ“‡Qwen APIï¼Œé¡¯ç¤ºAPI Keyè¼¸å…¥æ¡†
    if ocr_engine == "Qwen API":
        qwen_api_key = st.text_input("Qwen API Key", type="password", help="è¼¸å…¥æ‚¨çš„Qwen API Key")
        st.info("Qwen APIæä¾›é«˜ç²¾åº¦OCRå’Œè¡¨æ ¼è­˜åˆ¥ï¼Œç‰¹åˆ¥é©åˆè¤‡é›œæ’ç‰ˆçš„PDF")
    else:
        use_ocr = st.checkbox("ä½¿ç”¨OCRæå–", value=True, help="å•Ÿç”¨OCRå¯ä»¥æé«˜æ–‡æœ¬æå–è³ªé‡ï¼Œä½†æœƒå¢åŠ è™•ç†æ™‚é–“")
    
    # è¡¨æ ¼è™•ç†è¨­ç½®
    st.subheader("è¡¨æ ¼è™•ç†")
    table_handling = st.radio(
        "è¡¨æ ¼è™•ç†æ–¹å¼",
        ["è¼”åŠ©äººå·¥æ¯”å°", "å˜—è©¦è‡ªå‹•æ¯”å°", "åƒ…æ¨™è¨˜è¡¨æ ¼ä½ç½®"],
        index=0,
        help="é¸æ“‡ç³»çµ±å¦‚ä½•è™•ç†è¡¨æ ¼æ¯”å°"
    )
    
    # é«˜ç´šè¨­ç½®
    with st.expander("é«˜ç´šè¨­ç½®"):
        segment_type = st.radio(
            "åˆ†æ®µé¡å‹",
            ["æ®µè½", "å¥å­"],
            index=0
        )
        
        show_all_content = st.checkbox(
            "é¡¯ç¤ºæ‰€æœ‰å…§å®¹",
            value=False,
            help="å‹¾é¸å¾Œé¡¯ç¤ºæ‰€æœ‰æ®µè½ï¼Œå¦å‰‡åªé¡¯ç¤ºä¸ä¸€è‡´çš„æ®µè½"
        )
        
        # å·®ç•°é¡¯ç¤ºè¨­ç½®
        st.subheader("å·®ç•°é¡¯ç¤ºè¨­ç½®")
        diff_display_mode = st.radio(
            "å·®ç•°é¡¯ç¤ºæ¨¡å¼",
            ["å­—ç¬¦ç´šåˆ¥", "è©èªç´šåˆ¥", "è¡Œç´šåˆ¥"],
            index=0,
            help="é¸æ“‡å¦‚ä½•é¡¯ç¤ºå·®ç•°"
        )
        
        # é¡è‰²è¨­ç½®
        st.subheader("é¡è‰²è¨­ç½®")
        diff_removed_color = st.color_picker("åˆªé™¤å…§å®¹é¡è‰²", "#FFCDD2")
        diff_added_color = st.color_picker("æ·»åŠ å…§å®¹é¡è‰²", "#C8E6C9")

# ä¸»è¦å…§å®¹å€åŸŸ - æ–‡ä»¶ä¸Šå‚³
st.markdown('<h2 class="sub-header">æ–‡ä»¶ä¸Šå‚³</h2>', unsafe_allow_html=True)

# é¸æ“‡å–®æ–‡ä»¶æˆ–å¤šæ–‡ä»¶æ¨¡å¼
file_mode = st.radio(
    "æ–‡ä»¶ä¸Šå‚³æ¨¡å¼",
    ["å–®ä¸€Wordæ–‡ä»¶", "å¤šå€‹Wordæ–‡ä»¶ï¼ˆç« ç¯€ï¼‰"],
    index=0,
    help="é¸æ“‡ä¸Šå‚³å–®ä¸€Wordæ–‡ä»¶æˆ–å¤šå€‹Wordæ–‡ä»¶ï¼ˆä½œç‚ºä¸åŒç« ç¯€ï¼‰"
)

if file_mode == "å–®ä¸€Wordæ–‡ä»¶":
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("åŸå§‹Wordæ–‡ä»¶")
        word_files = [st.file_uploader("ä¸Šå‚³åŸå§‹Wordæ–‡ä»¶", type=["docx"])]
    
    with col2:
        st.subheader("ç¾ç·¨å¾ŒPDFæ–‡ä»¶")
        pdf_file = st.file_uploader("ä¸Šå‚³ç¾ç·¨å¾ŒPDFæ–‡ä»¶", type=["pdf"])
else:
    st.subheader("åŸå§‹Wordæ–‡ä»¶ï¼ˆå¤šç« ç¯€ï¼‰")
    
    # å‰µå»ºä¸€å€‹å®¹å™¨ä¾†å­˜æ”¾å¤šå€‹æ–‡ä»¶ä¸Šå‚³å™¨
    uploaded_files = st.file_uploader("ä¸Šå‚³å¤šå€‹Wordæ–‡ä»¶", type=["docx"], accept_multiple_files=True)
    
    if uploaded_files:
        st.success(f"å·²ä¸Šå‚³ {len(uploaded_files)} å€‹Wordæ–‡ä»¶")
        
        # é¡¯ç¤ºä¸Šå‚³çš„æ–‡ä»¶
        for i, file in enumerate(uploaded_files):
            st.text(f"ç« ç¯€ {i+1}: {file.name}")
    
    word_files = uploaded_files if uploaded_files else []
    
    st.subheader("ç¾ç·¨å¾ŒPDFæ–‡ä»¶")
    pdf_file = st.file_uploader("ä¸Šå‚³ç¾ç·¨å¾ŒPDFæ–‡ä»¶", type=["pdf"])

# æ–‡æœ¬æå–å‡½æ•¸
def extract_docx_text(file):
    """å¾Wordæ–‡ä»¶æå–æ–‡æœ¬å’Œè¡¨æ ¼"""
    temp_dir = tempfile.mkdtemp()
    temp_path = os.path.join(temp_dir, "temp.docx")
    
    with open(temp_path, "wb") as f:
        f.write(file.getvalue())
    
    doc = docx.Document(temp_path)
    
    # æå–æ®µè½å’Œè¡¨æ ¼ï¼Œè¨˜éŒ„å®ƒå€‘åœ¨æ–‡æª”ä¸­çš„é †åº
    content_items = []
    
    for item in doc.element.body:
        if item.tag.endswith('p'):  # æ®µè½
            paragraph = docx.text.paragraph.Paragraph(item, doc)
            if paragraph.text.strip():
                content_items.append({
                    'type': 'paragraph',
                    'content': paragraph.text,
                    'context': {
                        'previous_text': '',  # å°‡åœ¨å¾Œè™•ç†ä¸­å¡«å……
                        'next_text': ''       # å°‡åœ¨å¾Œè™•ç†ä¸­å¡«å……
                    }
                })
        elif item.tag.endswith('tbl'):  # è¡¨æ ¼
            table = docx.table.Table(item, doc)
            table_data = []
            for row in table.rows:
                row_data = []
                for cell in row.cells:
                    row_data.append(cell.text)
                table_data.append(row_data)
            
            # æå–è¡¨æ ¼æ¨™é¡Œï¼ˆå‡è¨­è¡¨æ ¼å‰çš„æ®µè½å¯èƒ½æ˜¯æ¨™é¡Œï¼‰
            table_title = ""
            if len(content_items) > 0 and content_items[-1]['type'] == 'paragraph':
                table_title = content_items[-1]['content']
            
            content_items.append({
                'type': 'table',
                'content': table_data,
                'title': table_title,
                'context': {
                    'previous_text': '',  # å°‡åœ¨å¾Œè™•ç†ä¸­å¡«å……
                    'next_text': ''       # å°‡åœ¨å¾Œè™•ç†ä¸­å¡«å……
                }
            })
    
    # å¡«å……ä¸Šä¸‹æ–‡ä¿¡æ¯
    for i in range(len(content_items)):
        # å‰æ–‡
        if i > 0:
            content_items[i]['context']['previous_text'] = (
                content_items[i-1]['content'] if content_items[i-1]['type'] == 'paragraph'
                else "è¡¨æ ¼: " + content_items[i-1]['title']
            )
        
        # å¾Œæ–‡
        if i < len(content_items) - 1:
            content_items[i]['context']['next_text'] = (
                content_items[i+1]['content'] if content_items[i+1]['type'] == 'paragraph'
                else "è¡¨æ ¼: " + content_items[i+1]['title']
            )
    
    # åˆ†é›¢æ®µè½å’Œè¡¨æ ¼
    paragraphs = [item for item in content_items if item['type'] == 'paragraph']
    tables = [item for item in content_items if item['type'] == 'table']
    
    # æ¸…ç†è‡¨æ™‚ç›®éŒ„
    try:
        shutil.rmtree(temp_dir)
    except:
        pass
    
    return {
        "content_items": content_items,
        "paragraphs": paragraphs,
        "tables": tables,
        "filename": file.name
    }

# å¢å¼·çš„PDFæå–å‡½æ•¸ï¼Œæ”¯æŒå¤šç¨®OCRå¼•æ“
def advanced_pdf_extraction(pdf_file, ocr_engine="è‡ªå‹•é¸æ“‡", qwen_api_key=None, use_ocr=True):
    """
    å¢å¼·çš„PDFæ–‡æœ¬æå–å‡½æ•¸ï¼Œæ”¯æŒå¤šç¨®OCRå¼•æ“
    """
    # å‰µå»ºè‡¨æ™‚ç›®éŒ„
    temp_dir = tempfile.mkdtemp()
    temp_path = os.path.join(temp_dir, "temp.pdf")
    
    # ä¿å­˜ä¸Šå‚³çš„æ–‡ä»¶
    with open(temp_path, "wb") as f:
        f.write(pdf_file.getvalue())
    
    # å¦‚æœé¸æ“‡Qwen API
    if ocr_engine == "Qwen API" and qwen_api_key:
        try:
            st.info("æ­£åœ¨ä½¿ç”¨Qwen APIæå–æ–‡æœ¬ï¼Œé€™å¯èƒ½éœ€è¦ä¸€äº›æ™‚é–“...")
            
            # åˆå§‹åŒ–Qwen OCR
            qwen_ocr = QwenOCR(qwen_api_key)
            
            # ä½¿ç”¨PyMuPDFç²å–PDFé æ•¸
            import fitz
            doc = fitz.open(temp_path)
            num_pages = len(doc)
            
            # å­˜å„²æ‰€æœ‰æå–çš„æ®µè½å’Œè¡¨æ ¼
            all_paragraphs = []
            all_tables = []
            
            # è™•ç†æ¯ä¸€é 
            for page_num in range(num_pages):
                st.text(f"æ­£åœ¨è™•ç†ç¬¬ {page_num+1}/{num_pages} é ...")
                
                # å°‡PDFé é¢è½‰æ›ç‚ºåœ–åƒ
                page = doc[page_num]
                pix = page.get_pixmap(matrix=fitz.Matrix(300/72, 300/72))  # 300 DPI
                img_path = os.path.join(temp_dir, f"page_{page_num+1}.png")
                pix.save(img_path)
                
                # ä½¿ç”¨Qwen APIæå–æ–‡æœ¬
                extracted_text = qwen_ocr.extract_text_from_image(img_path)
                
                # åˆ†å‰²æ®µè½
                paragraphs = [p.strip() for p in extracted_text.split('\n\n') if p.strip()]
                for para in paragraphs:
                    all_paragraphs.append({
                        'content': para,
                        'page': page_num + 1,
                        'method': 'qwen_api',
                        'confidence': 0.95
                    })
                
                # æå–è¡¨æ ¼
                tables = qwen_ocr.extract_tables_from_image(img_path)
                if isinstance(tables, list) and tables:
                    for table in tables:
                        all_tables.append({
                            'content': table,
                            'page': page_num + 1,
                            'method': 'qwen_api',
                            'context': {
                                'previous_text': '',
                                'next_text': ''
                            },
                            'title': ''
                        })
            
            # æ¸…ç†è‡¨æ™‚æ–‡ä»¶
            try:
                import shutil
                shutil.rmtree(temp_dir)
            except:
                pass
            
            return {
                "paragraphs": all_paragraphs,
                "tables": all_tables
            }
            
        except Exception as e:
            st.error(f"Qwen APIæå–å¤±æ•—: {e}")
            st.warning("å°‡ä½¿ç”¨å‚™ç”¨æ–¹æ³•æå–æ–‡æœ¬...")
    
    # ä½¿ç”¨å¢å¼·çš„PDFæå–å‡½æ•¸
    return enhanced_pdf_extraction(pdf_file, use_ocr=use_ocr)

# æ–‡æœ¬é è™•ç†å‡½æ•¸
def preprocess_text(text, ignore_space=True, ignore_punctuation=True, ignore_case=True, ignore_newline=True):
    """æ–‡æœ¬é è™•ç†"""
    if ignore_space:
        text = re.sub(r'\s+', ' ', text)
    
    if ignore_punctuation:
        text = re.sub(r'[.,;:!?ï¼Œã€‚ï¼›ï¼šï¼ï¼Ÿ]', '', text)
    
    if ignore_case:
        text = text.lower()
    
    if ignore_newline:
        text = text.replace('\n', ' ')
    
    return text.strip()

# æ¯”å°å‡½æ•¸
def exact_matching(text1, text2, ignore_space=True, ignore_punctuation=True, ignore_case=True, ignore_newline=True):
    """ç²¾ç¢ºæ¯”å°"""
    # æ–‡æœ¬é è™•ç†
    processed_text1 = preprocess_text(text1, ignore_space, ignore_punctuation, ignore_case, ignore_newline)
    processed_text2 = preprocess_text(text2, ignore_space, ignore_punctuation, ignore_case, ignore_newline)
    
    # è¨ˆç®—ç›¸ä¼¼åº¦
    matcher = difflib.SequenceMatcher(None, processed_text1, processed_text2)
    similarity = matcher.ratio()
    
    # ç”Ÿæˆå·®ç•°
    diff = list(difflib.ndiff(text1.splitlines(), text2.splitlines()))
    
    return similarity, diff

def semantic_matching(text1, text2, model=None):
    """èªæ„æ¯”å°ï¼ˆä½¿ç”¨Sentence-BERTæˆ–ç°¡åŒ–ç‰ˆï¼‰"""
    if model is not None and is_sentence_transformers_available():
        try:
            # ä½¿ç”¨Sentence-BERTè¨ˆç®—èªç¾©ç›¸ä¼¼åº¦
            embedding1 = model.encode(text1)
            embedding2 = model.encode(text2)
            
            # è¨ˆç®—é¤˜å¼¦ç›¸ä¼¼åº¦
            similarity = np.dot(embedding1, embedding2) / (np.linalg.norm(embedding1) * np.linalg.norm(embedding2))
            return float(similarity)
        except Exception as e:
            st.warning(f"èªç¾©æ¨¡å‹è¨ˆç®—å¤±æ•—: {e}ï¼Œä½¿ç”¨ç°¡åŒ–ç‰ˆèªç¾©æ¯”å°")
    
    # ç°¡åŒ–ç‰ˆèªç¾©æ¯”å°ï¼ˆè©è¢‹æ¨¡å‹ï¼‰
    words1 = set(preprocess_text(text1, True, True,
(Content truncated due to size limit. Use line ranges to read in chunks)