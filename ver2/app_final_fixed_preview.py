import streamlit as st
import os
import tempfile
import docx
import re
import difflib
import pandas as pd
import numpy as np
import base64
from io import BytesIO
import json
import shutil
import sys
from pathlib import Path
from enhanced_extraction import enhanced_pdf_extraction, improved_matching_algorithm
from qwen_api import QwenOCR

# æª¢æŸ¥sentence-transformersæ˜¯å¦å¯ç”¨
try:
    from sentence_transformers import SentenceTransformer
    SENTENCE_TRANSFORMERS_AVAILABLE = True
except ImportError:
    SENTENCE_TRANSFORMERS_AVAILABLE = False

# è¨­ç½®é é¢é…ç½®
st.set_page_config(
    page_title="æœŸåˆŠæ¯”å°ç³»çµ±",
    page_icon="ğŸ“Š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# è‡ªå®šç¾©CSSæ¨£å¼
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        color: #1E88E5;
        text-align: center;
        margin-bottom: 1rem;
    }
    .sub-header {
        font-size: 1.5rem;
        color: #0D47A1;
        margin-top: 2rem;
        margin-bottom: 1rem;
    }
    .highlight-diff {
        background-color: #FFECB3;
        padding: 2px;
        border-radius: 3px;
    }
    .diff-added {
        color: #000000;
        background-color: #C8E6C9;
        padding: 2px;
        border-radius: 3px;
    }
    .diff-removed {
        color: #000000;
        background-color: #FFCDD2;
        padding: 2px;
        border-radius: 3px;
    }
    .result-container {
        border: 1px solid #E0E0E0;
        border-radius: 5px;
        padding: 15px;
        margin-bottom: 15px;
    }
    .similarity-high {
        color: #2E7D32;
        font-weight: bold;
    }
    .similarity-medium {
        color: #F57F17;
        font-weight: bold;
    }
    .similarity-low {
        color: #C62828;
        font-weight: bold;
    }
    .table-warning {
        background-color: #FFF3E0;
        padding: 10px;
        border-left: 4px solid #FF9800;
        margin-bottom: 10px;
    }
    .file-uploader-container {
        border: 1px dashed #BDBDBD;
        border-radius: 5px;
        padding: 10px;
        margin-bottom: 10px;
    }
    .multi-file-uploader {
        margin-bottom: 20px;
    }
    .chapter-selector {
        margin-top: 10px;
        margin-bottom: 20px;
    }
    .diff-char-removed {
        color: #000000;
        background-color: #FFCDD2;
        font-weight: bold;
        padding: 1px;
        border-radius: 2px;
    }
    .diff-char-added {
        color: #000000;
        background-color: #C8E6C9;
        font-weight: bold;
        padding: 1px;
        border-radius: 2px;
    }
    .diff-section {
        margin-top: 10px;
        margin-bottom: 10px;
        padding: 10px;
        border: 1px solid #E0E0E0;
        border-radius: 5px;
    }
    .diff-navigation {
        margin-top: 10px;
        margin-bottom: 10px;
        text-align: center;
    }
    .diff-count {
        font-weight: bold;
        margin-left: 10px;
        margin-right: 10px;
    }
    .api-key-input {
        margin-top: 10px;
        margin-bottom: 10px;
    }
    .success-message {
        color: #2E7D32;
        background-color: #E8F5E9;
        padding: 10px;
        border-radius: 5px;
        margin-top: 10px;
        margin-bottom: 10px;
    }
    .error-message {
        color: #C62828;
        background-color: #FFEBEE;
        padding: 10px;
        border-radius: 5px;
        margin-top: 10px;
        margin-bottom: 10px;
    }
    .ai-model-section {
        background-color: #E3F2FD;
        padding: 10px;
        border-radius: 5px;
        margin-top: 10px;
        margin-bottom: 10px;
    }
</style>
""", unsafe_allow_html=True)

# é¡¯ç¤ºæ¨™é¡Œ
st.markdown('<h1 class="main-header">æœŸåˆŠæ¯”å°ç³»çµ±</h1>', unsafe_allow_html=True)
st.markdown('æœ¬ç³»çµ±ç”¨æ–¼æ¯”å°åŸå§‹Wordæ–‡ä»¶èˆ‡ç¾ç·¨å¾ŒPDFæ–‡ä»¶çš„å…§å®¹å·®ç•°ï¼Œå¹«åŠ©æ ¡å°äººå“¡å¿«é€Ÿæ‰¾å‡ºä¸ä¸€è‡´ä¹‹è™•ã€‚')

from enhanced_extraction import enhanced_pdf_extraction
from comparison_algorithm_example import compare_documents
from custom_ai import CustomAI

st.header("ğŸ“ æ–‡ä»¶ä¸Šå‚³")

col1, col2 = st.columns(2)
with col1:
    word_file = st.file_uploader("ä¸Šå‚³åŸå§‹ Word æ–‡ç¨¿", type=["docx"])
with col2:
    pdf_file = st.file_uploader("ä¸Šå‚³ç¾ç·¨å¾Œ PDF æ–‡ä»¶", type=["pdf"])

similarity_threshold = st.slider("ç›¸ä¼¼åº¦é–¾å€¼", 0.0, 1.0, 0.6, 0.05)
use_ai = st.checkbox("ä½¿ç”¨ç”Ÿæˆå¼ AI é€²è¡Œèªæ„æ¯”å°", value=False)
ai_key = st.text_input("ğŸ”‘ è«‹è¼¸å…¥ä½ çš„ AI API é‡‘é‘°", type="password") if use_ai else None

if st.button("é–‹å§‹æ¯”å°"):
    if word_file is None or pdf_file is None:
        st.warning("è«‹å…ˆä¸Šå‚³ Word èˆ‡ PDF æª”æ¡ˆ")
    else:
        st.info("ğŸ§  é–‹å§‹æ¯”å°ä¸­...")

        # 1. ä¿å­˜ä¸Šå‚³æª”æ¡ˆè‡³æš«å­˜
        with tempfile.NamedTemporaryFile(delete=False, suffix=".docx") as tmp_word:
            tmp_word.write(word_file.read())
            word_path = tmp_word.name
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_pdf:
            tmp_pdf.write(pdf_file.read())
            pdf_path = tmp_pdf.name

        # 2. é€²è¡Œæ–‡å­—æŠ½å–
        word_data, pdf_data = enhanced_pdf_extraction(word_path, pdf_path)

        # 3. å»ºç«‹ AI æ¨¡å‹ï¼ˆå¦‚å•Ÿç”¨ï¼‰
        ai_instance = None
        if use_ai and ai_key:
            ai_instance = CustomAI(api_key=ai_key, model_name="Qwen")

        # 4. åŸ·è¡Œæ¯”å°æ¼”ç®—æ³•
        ignore_options = {
            "ignore_whitespace": True,
            "ignore_punctuation": True,
            "ignore_case": True,
            "ignore_linebreaks": True,
        }

        result = compare_documents(
            word_data,
            pdf_data,
            ignore_options=ignore_options,
            comparison_mode="hybrid" if use_ai else "exact",
            similarity_threshold=similarity_threshold,
            ai_instance=ai_instance
        )

        # 5. é¡¯ç¤ºçµæœ
        if result:
            st.success(f"æ¯”å°å®Œæˆï¼Œå…±è™•ç† {len(result)} çµ„æ®µè½ï¼")
            for item in result:
                st.markdown("### ğŸ“Œ å·®ç•°æ®µè½")
                st.markdown(f"- **åŸå§‹ï¼š** {item['doc1_text']}")
                st.markdown(f"- **PDFï¼š** {item['doc2_text']}")
                st.markdown(f"- **ç›¸ä¼¼åº¦ï¼š** {item['similarity']:.2f}")
                st.markdown("---")
        else:
            st.warning("æœªæ¯”å°åˆ°æœ‰æ•ˆæ®µè½ï¼Œè«‹æª¢æŸ¥æ–‡ä»¶å…§å®¹æ˜¯å¦æ­£ç¢ºã€‚")


# æª¢æŸ¥Javaæ˜¯å¦å®‰è£
def is_java_installed():
    try:
        result = os.system("java -version > /dev/null 2>&1")
        return result == 0
    except:
        return False

# æª¢æŸ¥EasyOCRæ˜¯å¦å¯ç”¨
def is_easyocr_available():
    try:
        import easyocr
        return True
    except ImportError:
        return False

# æª¢æŸ¥tabula-pyæ˜¯å¦å¯ç”¨
def is_tabula_available():
    if not is_java_installed():
        return False
    try:
        import tabula
        return True
    except ImportError:
        return False

# æª¢æŸ¥sentence-transformersæ˜¯å¦å¯ç”¨
def is_sentence_transformers_available():
    return SENTENCE_TRANSFORMERS_AVAILABLE

# åŠ è¼‰èªç¾©æ¨¡å‹
@st.cache_resource
def load_semantic_model():
    if is_sentence_transformers_available():
        try:
            model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
            return model
        except Exception as e:
            st.error(f"åŠ è¼‰èªç¾©æ¨¡å‹å¤±æ•—: {e}")
            return None
    return None

# ç”Ÿæˆå¼AIæ¨¡å‹é¡
class GenerativeAI:
    def __init__(self, model_name, api_key=None):
        self.model_name = model_name
        self.api_key = api_key
        self.is_available = self._check_availability()
    
    def _check_availability(self):
        """æª¢æŸ¥æ¨¡å‹æ˜¯å¦å¯ç”¨"""
        if self.model_name in ["BERTå¤šèªè¨€æ¨¡å‹", "MPNetä¸­æ–‡æ¨¡å‹", "RoBERTaä¸­æ–‡æ¨¡å‹"]:
            # æª¢æŸ¥æœ¬åœ°æ¨¡å‹
            try:
                from transformers import AutoModel, AutoTokenizer
                return True
            except ImportError:
                return False
        elif self.model_name in ["OpenAI API", "Anthropic API", "Gemini API", "Qwen API"]:
            # æª¢æŸ¥APIæ¨¡å‹
            return self.api_key is not None and len(self.api_key) > 0
        return False
    
    def match_paragraphs(self, source_paragraphs, target_paragraphs):
        """ä½¿ç”¨ç”Ÿæˆå¼AIåŒ¹é…æ®µè½"""
        if not self.is_available:
            return None
        
        if self.model_name == "BERTå¤šèªè¨€æ¨¡å‹":
            return self._match_with_bert(source_paragraphs, target_paragraphs)
        elif self.model_name == "MPNetä¸­æ–‡æ¨¡å‹":
            return self._match_with_mpnet(source_paragraphs, target_paragraphs)
        elif self.model_name == "RoBERTaä¸­æ–‡æ¨¡å‹":
            return self._match_with_roberta(source_paragraphs, target_paragraphs)
        elif self.model_name == "OpenAI API":
            return self._match_with_openai(source_paragraphs, target_paragraphs)
        elif self.model_name == "Anthropic API":
            return self._match_with_anthropic(source_paragraphs, target_paragraphs)
        elif self.model_name == "Gemini API":
            return self._match_with_gemini(source_paragraphs, target_paragraphs)
        elif self.model_name == "Qwen API":
            return self._match_with_qwen(source_paragraphs, target_paragraphs)
        
        return None
    
    def _match_with_bert(self, source_paragraphs, target_paragraphs):
        """ä½¿ç”¨BERTå¤šèªè¨€æ¨¡å‹åŒ¹é…æ®µè½"""
        try:
            from transformers import BertModel, BertTokenizer
            import torch
            
            # åŠ è¼‰æ¨¡å‹
            tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')
            model = BertModel.from_pretrained('bert-base-multilingual-cased')
            
            # è¨ˆç®—åµŒå…¥
            source_embeddings = []
            for para in source_paragraphs:
                inputs = tokenizer(para['content'], return_tensors="pt", truncation=True, max_length=512)
                with torch.no_grad():
                    outputs = model(**inputs)
                embeddings = outputs.last_hidden_state.mean(dim=1)
                source_embeddings.append(embeddings.squeeze().numpy())
            
            target_embeddings = []
            for para in target_paragraphs:
                inputs = tokenizer(para['content'], return_tensors="pt", truncation=True, max_length=512)
                with torch.no_grad():
                    outputs = model(**inputs)
                embeddings = outputs.last_hidden_state.mean(dim=1)
                target_embeddings.append(embeddings.squeeze().numpy())
            
            # è¨ˆç®—ç›¸ä¼¼åº¦çŸ©é™£
            similarity_matrix = np.zeros((len(source_paragraphs), len(target_paragraphs)))
            for i, source_emb in enumerate(source_embeddings):
                for j, target_emb in enumerate(target_embeddings):
                    similarity = np.dot(source_emb, target_emb) / (np.linalg.norm(source_emb) * np.linalg.norm(target_emb))
                    similarity_matrix[i, j] = similarity
            
            # ä½¿ç”¨åŒˆç‰™åˆ©ç®—æ³•æ‰¾åˆ°æœ€ä½³åŒ¹é…
            from scipy.optimize import linear_sum_assignment
            row_ind, col_ind = linear_sum_assignment(-similarity_matrix)
            
            # æ§‹å»ºåŒ¹é…çµæœ
            matches = []
            for i, j in zip(row_ind, col_ind):
                matches.append({
                    "doc1_index": i,
                    "doc2_index": j,
                    "similarity": similarity_matrix[i, j]
                })
            
            return matches
        
        except Exception as e:
            st.error(f"BERTåŒ¹é…å¤±æ•—: {e}")
            return None
    
    def _match_with_mpnet(self, source_paragraphs, target_paragraphs):
        """ä½¿ç”¨MPNetä¸­æ–‡æ¨¡å‹åŒ¹é…æ®µè½"""
        try:
            from transformers import AutoModel, AutoTokenizer
            import torch
            
            # åŠ è¼‰æ¨¡å‹
            model_name = "shibing624/text2vec-base-chinese"
            tokenizer = AutoTokenizer.from_pretrained(model_name)
            model = AutoModel.from_pretrained(model_name)
            
            # è¨ˆç®—åµŒå…¥
            source_embeddings = []
            for para in source_paragraphs:
                inputs = tokenizer(para['content'], return_tensors="pt", truncation=True, max_length=512)
                with torch.no_grad():
                    outputs = model(**inputs)
                embeddings = outputs.last_hidden_state.mean(dim=1)
                source_embeddings.append(embeddings.squeeze().numpy())
            
            target_embeddings = []
            for para in target_paragraphs:
                inputs = tokenizer(para['content'], return_tensors="pt", truncation=True, max_length=512)
                with torch.no_grad():
                    outputs = model(**inputs)
                embeddings = outputs.last_hidden_state.mean(dim=1)
                target_embeddings.append(embeddings.squeeze().numpy())
            
            # è¨ˆç®—ç›¸ä¼¼åº¦çŸ©é™£
            similarity_matrix = np.zeros((len(source_paragraphs), len(target_paragraphs)))
            for i, source_emb in enumerate(source_embeddings):
                for j, target_emb in enumerate(target_embeddings):
                    similarity = np.dot(source_emb, target_emb) / (np.linalg.norm(source_emb) * np.linalg.norm(target_emb))
                    similarity_matrix[i, j] = similarity
            
            # ä½¿ç”¨åŒˆç‰™åˆ©ç®—æ³•æ‰¾åˆ°æœ€ä½³åŒ¹é…
            from scipy.optimize import linear_sum_assignment
            row_ind, col_ind = linear_sum_assignment(-similarity_matrix)
            
            # æ§‹å»ºåŒ¹é…çµæœ
            matches = []
            for i, j in zip(row_ind, col_ind):
                matches.append({
                    "doc1_index": i,
                    "doc2_index": j,
                    "similarity": similarity_matrix[i, j]
                })
            
            return matches
        
        except Exception as e:
            st.error(f"MPNetåŒ¹é…å¤±æ•—: {e}")
            return None
    
    def _match_with_roberta(self, source_paragraphs, target_paragraphs):
        """ä½¿ç”¨RoBERTaä¸­æ–‡æ¨¡å‹åŒ¹é…æ®µè½"""
        try:
            from transformers import AutoModel, AutoTokenizer
            import torch
            
            # åŠ è¼‰æ¨¡å‹
            model_name = "hfl/chinese-roberta-wwm-ext"
            tokenizer = AutoTokenizer.from_pretrained(model_name)
            model = AutoModel.from_pretrained(model_name)
            
            # è¨ˆç®—åµŒå…¥
            source_embeddings = []
            for para in source_paragraphs:
                inputs = tokenizer(para['content'], return_tensors="pt", truncation=True, max_length=512)
                with torch.no_grad():
                    outputs = model(**inputs)
                embeddings = outputs.last_hidden_state.mean(dim=1)
                source_embeddings.append(embeddings.squeeze().numpy())
            
            target_embeddings = []
            for para in target_paragraphs:
                inputs = tokenizer(para['content'], return_tensors="pt", truncation=True, max_length=512)
                with torch.no_grad():
                    outputs = model(**inputs)
                embeddings = outputs.last_hidden_state.mean(dim=1)
                target_embeddings.append(embeddings.squeeze().numpy())
            
            # è¨ˆç®—ç›¸ä¼¼åº¦çŸ©é™£
            similarity_matrix = np.zeros((len(source_paragraphs), len(target_paragraphs)))
            for i, source_emb in enumerate(source_embeddings):
                for j, target_emb in enumerate(target_embeddings):
                    similarity = np.dot(source_emb, target_emb) / (np.linalg.norm(source_emb) * np.linalg.norm(target_emb))
                    similarity_matrix[i, j] = similarity
            
            # ä½¿ç”¨åŒˆç‰™åˆ©ç®—æ³•æ‰¾åˆ°æœ€ä½³åŒ¹é…
            from scipy.optimize import linear_sum_assignment
            row_ind, col_ind = linear_sum_assignment(-similarity_matrix)
            
            # æ§‹å»ºåŒ¹é…çµæœ
            matches = []
            for i, j in zip(row_ind, col_ind):
                matches.append({
                    "doc1_index": i,
                    "doc2_index": j,
                    "similarity": similarity_matrix[i, j]
                })
            
            return matches
        
        except Exception as e:
            st.error(f"RoBERTaåŒ¹é…å¤±æ•—: {e}")
            return None
    
    def _match_with_openai(self, source_paragraphs, target_paragraphs):
        """ä½¿ç”¨OpenAI APIåŒ¹é…æ®µè½"""
        try:
            import openai
            
            # è¨­ç½®APIå¯†é‘°
            openai.api_key = self.api_key
            
            # æº–å‚™æ•¸æ“š
            source_texts = [p['content'] for p in source_paragraphs]
            target_texts = [p['content'] for p in target_paragraphs]
            
            # æ§‹å»ºæç¤º
            prompt = f"""
            æˆ‘æœ‰å…©çµ„æ–‡æœ¬æ®µè½ï¼Œéœ€è¦æ‰¾å‡ºå®ƒå€‘ä¹‹é–“çš„æœ€ä½³åŒ¹é…é—œä¿‚ã€‚
            
            ç¬¬ä¸€çµ„æ®µè½ï¼ˆåŸå§‹æ–‡æœ¬ï¼‰ï¼š
            {json.dumps(source_texts, ensure_ascii=False)}
            
            ç¬¬äºŒçµ„æ®µè½ï¼ˆç›®æ¨™æ–‡æœ¬ï¼‰ï¼š
            {json.dumps(target_texts, ensure_ascii=False)}
            
            è«‹åˆ†æé€™äº›æ®µè½ï¼Œæ‰¾å‡ºæ¯å€‹åŸå§‹æ®µè½åœ¨ç›®æ¨™æ®µè½ä¸­çš„æœ€ä½³åŒ¹é…ã€‚è¿”å›ä¸€å€‹JSONæ•¸çµ„ï¼Œæ¯å€‹å…ƒç´ åŒ…å«ï¼š
            1. "source_index": åŸå§‹æ®µè½çš„ç´¢å¼•ï¼ˆå¾0é–‹å§‹ï¼‰
            2. "target_index": åŒ¹é…çš„ç›®æ¨™æ®µè½çš„ç´¢å¼•ï¼ˆå¾0é–‹å§‹ï¼‰
            3. "similarity": ç›¸ä¼¼åº¦è©•åˆ†ï¼ˆ0åˆ°1ä¹‹é–“ï¼‰
            
            åªè¿”å›JSONæ•¸çµ„ï¼Œä¸è¦æœ‰å…¶ä»–è§£é‡‹ã€‚
            """
            
            # èª¿ç”¨API
            response = openai.ChatCompletion.create(
                model="gpt-4",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„æ–‡æœ¬åŒ¹é…åŠ©æ‰‹ï¼Œæ“…é•·åˆ†ææ–‡æœ¬ç›¸ä¼¼åº¦å’Œæ‰¾å‡ºæœ€ä½³åŒ¹é…ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.2
            )
            
            # è§£æçµæœ
            result_text = response.choices[0].message.content
            
            # æå–JSON
            import re
            json_match = re.search(r'\[.*\]', result_text, re.DOTALL)
            if json_match:
                json_str = json_match.group(0)
                matches_data = json.loads(json_str)
                
                # è½‰æ›ç‚ºæ‰€éœ€æ ¼å¼
                matches = []
                for item in matches_data:
                    matches.append({
                        "doc1_index": item["source_index"],
                        "doc2_index": item["target_index"],
                        "similarity": item["similarity"]
                    })
                
                return matches
            
            return None
        
        except Exception as e:
            st.error(f"OpenAI APIåŒ¹é…å¤±æ•—: {e}")
            return None
    
def _match_with_anthropic(self, source_paragraphs, target_paragraphs):
    """ä½¿ç”¨Anthropic APIåŒ¹é…æ®µè½"""
    try:
        import anthropic
        client = anthropic.Anthropic(api_key=self.api_key)

        # æº–å‚™è³‡æ–™
        source_texts = [p['content'] for p in source_paragraphs]
        target_texts = [p['content'] for p in target_paragraphs]

        # TODO: åŠ å…¥Anthropic APIèª¿ç”¨é‚è¼¯
        return []  # æš«æ™‚è¿”å›ç©ºçµæœä½œç‚ºå ä½
    except Exception as e:
        st.error(f"Anthropic API åŒ¹é…å¤±æ•—ï¼š{e}")
        return None

